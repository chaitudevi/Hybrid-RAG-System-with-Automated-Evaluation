question_id,mode,mrr,recall_at_5,precision_at_5,rouge_l,generated_answer,ground_truth_answer
q_000,dense,0.0,0.0,0.0,0.025316455696202528," a b c      : cs1 maint : numeric names : authors list ( link ) retrieved from "" https : / / en. wikipedia. org / w / index. php? title = let _ txapote _ vote _ for _ you & oldid = 1311469944","introduction of a multi - head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence. its parallelizability was an important factor to its widespread use in large neural networks. ai boom era as early as spring 2017, even before the "" attention is all you need "" preprint was published, one of the co - authors applied the "" decoder - only "" variation of the architecture to generate fictitious wikipedia articles. transformer architecture is now used alongside many generative models that contribute to the ongoing ai boom. in language modelling, elmo ( 2018 ) was a bi - directional lstm that produces contextualized word embeddings, improving upon the line of research from bag of words and word2vec. it was followed by bert ( 2018 ), an encoder - only transformer model. in october 2019, google started using bert to process search queries. in 2020, google translate replaced the previous rnn - encoder – rnn - decoder model by a transformer - encoder – rnn - decoder model. starting in 2018, the openai gpt series of decoder - only transformers became state of the art in natural language generation. in 2022, a chatbot based on gpt - 3, chatgpt, became unexpectedly popular, triggering a boom around large language models. since 2020, transformers have been applied in modalities beyond text, including the"
q_001,dense,0.0,0.0,0.0,0.014084507042253523,liberals take longer to decide their stance on the information,"cs. cl ]. ^ press, ofir ; smith, noah a. ; lewis, mike ( 2021 - 08 - 01 ). "" train short, test long : attention with linear biases enables input length extrapolation "". arxiv : 2108. 12409 [ cs. cl ]. ^ shaw, peter ; uszkoreit, jakob ; vaswani, ashish ( 2018 ). "" self - attention with relative position representations "". arxiv : 1803. 02155 [ cs. cl ]. ^ ke, guolin ; he, di ; liu, tie - yan ( 2021 - 03 - 15 ), rethinking positional encoding in language pre - training, arxiv : 2006. 15595 ^ kwon, woosuk ; li, zhuohan ; zhuang, siyuan ; sheng, ying ; zheng, lianmin ; yu, cody hao ; gonzalez, joseph ; zhang, hao ; stoica, ion ( 2023 - 10 - 23 ). "" efficient memory management for large language model serving with pagedattention "". proceedings of the 29th symposium on operating systems principles. sosp ' 23. new york, ny, usa : association for computing machinery. pp. 611 – 626. arxiv : 2309. 06180. doi : 10. 1145 / 3600006. 3613165. isbn 979 - 8 -"
q_002,dense,0.0,0.0,0.0,0.0,"braingate - home "". braingate2. org.","- term memory based deep recurrent neural networks for large vocabulary speech recognition "". arxiv : 1410. 4281 [ cs. cl ]. ^ dupond, samuel ( 2019 ). "" a thorough review on the current advance of neural network structures "". annual reviews in control. 14 : 200 – 230. ^ abiodun, oludare isaac ; jantan, aman ; omolara, abiodun esther ; dada, kemi victoria ; mohamed, nachaat abdelatif ; arshad, humaira ( 2018 - 11 - 01 ). "" state - of - the - art in artificial neural network applications : a survey "". heliyon. 4 ( 11 ) e00938. bibcode : 2018heliy... 400938a. doi : 10. 1016 / j. heliyon. 2018. e00938. issn 2405 - 8440. pmc 6260436. pmid 30519653. ^ espinosa - sanchez, juan manuel ; gomez - marin, alex ; de castro, fernando ( 2023 - 07 - 05 ). "" the importance of cajal ' s and lorente de no ' s neuroscience to the birth of cybernetics "". the neuroscientist. 31 ( 1 ) : 14 – 30. doi : 10. 1177 / 10738584231179932. hdl : 1026"
q_003,dense,0.0,0.0,0.0,0.11055276381909548," a b c luba, aminath ( 2 may 2022 ). "" eid gift to the audience "". avas ( in divehi ). archived from the original on 7 may 2022. retrieved 7 april 2022.  luba, aminath ( 8 august 2023 ). "" eupe ' s "" jokaru "" to release on","on 14 february 2019. retrieved 20 january 2024. ^ "" chatgpt a year on : 3 ways the ai chatbot has completely changed the world in 12 months "". euronews. november 30, 2023. archived from the original on january 14, 2024. retrieved january 20, 2024. ^ heaven, will ( march 14, 2023 ). "" gpt - 4 is bigger and better than chatgpt — but openai won ' t say why "". mit technology review. archived from the original on march 17, 2023. retrieved january 20, 2024. ^ metz, cade ( september 12, 2024 ). "" openai unveils new chatgpt that can reason through math and science "". the new york times. retrieved september 12, 2024. ^ "" parameters in notable artificial intelligence systems "". ourworldindata. org. november 30, 2023. retrieved january 20, 2024. ^ sharma, shubham ( 2025 - 01 - 20 ). "" open - source deepseek - r1 uses pure reinforcement learning to match openai o1 — at 95 % less cost "". venturebeat. retrieved 2025 - 01 - 26. ^ "" llama - mesh "". research. nvidia. com. 2024. retrieved 2025 - 10 - 30. ^ zia, dr tehseen ( 2024 - 01 - 08 ). "" unveiling of large multimo"
q_004,dense,0.0,0.0,0.0,0.01515151515151515,What is the author's last name?,"deep learning for human action recognition "". in salah, albert ali ; lepri, bruno ( eds. ). human behavior unterstanding. lecture notes in computer science. vol. 7065. amsterdam, netherlands : springer. pp. 29 – 39. doi : 10. 1007 / 978 - 3 - 642 - 25446 - 8 _ 4. isbn 978 - 3 - 642 - 25445 - 1. ^ hochreiter, sepp ; heusel, martin ; obermayer, klaus ( 2007 ). "" fast model - based protein homology detection without alignment "". bioinformatics. 23 ( 14 ) : 1728 – 1736. doi : 10. 1093 / bioinformatics / btm247. pmid 17488755. ^ thireou, trias ; reczko, martin ( july 2007 ). "" bidirectional long short - term memory networks for predicting the subcellular localization of eukaryotic proteins "". ieee / acm transactions on computational biology and bioinformatics. 4 ( 3 ) : 441 – 446. bibcode : 2007itcbb... 4.. 441t. doi : 10. 1109 / tcbb. 2007. 1015. pmid 17666763. s2cid 11787259. ^ tax, niek ; verenich, ilya ; la rosa, marcello ; dumas,"
q_005,dense,0.0,0.0,0.0,0.0,"july 20, 2009","transformers for longer sequences with sparse attention methods "". google ai blog. 25 march 2021. archived from the original on 2021 - 09 - 18. retrieved 2021 - 05 - 28. ^ zhai, shuangfei ; talbott, walter ; srivastava, nitish ; huang, chen ; goh, hanlin ; zhang, ruixiang ; susskind, josh ( 2021 - 09 - 21 ). "" an attention free transformer "". arxiv : 2105. 14103 [ cs. lg ]. ^ peng, hao ; pappas, nikolaos ; yogatama, dani ; schwartz, roy ; smith, noah a. ; kong, lingpeng ( 2021 - 03 - 19 ). "" random feature attention "". arxiv : 2103. 02143 [ cs. cl ]. ^ choromanski, krzysztof ; likhosherstov, valerii ; dohan, david ; song, xingyou ; gane, andreea ; sarlos, tamas ; hawkins, peter ; davis, jared ; belanger, david ; colwell, lucy ; weller, adrian ( 2020 - 09 - 30 ). "" masked language modeling for proteins via linearly scalable long - context transformers "". arxiv : 2006. 03555 [ cs. lg ]. ^ lu, kevin ; grover, aditya ; abbeel, pieter ;"
q_006,dense,0.0,0.0,0.0,0.0,"/ index. php? title = lo _ carmen & oldid = 1335910933 "" ##gento.",", fabio lorenzo ; di ventra, massimiliano ( 2017 ). "" the complex dynamics of memristive circuits : analytical results and universal slow relaxation "". physical review e. 95 ( 2 ) 022140. arxiv : 1608. 08651. bibcode : 2017phrve.. 95b2140c. doi : 10. 1103 / physreve. 95. 022140. pmid 28297937. s2cid 6758362. ^ harvey, inman ; husbands, phil ; cliff, dave ( 1994 ), "" seeing the light : artificial evolution, real vision "", 3rd international conference on simulation of adaptive behavior : from animals to animats 3, pp. 392 – 401 ^ quinn, matt ( 2001 ). "" evolving communication without dedicated communication channels "". advances in artificial life : 6th european conference, ecal 2001. pp. 357 – 366. doi : 10. 1007 / 3 - 540 - 44811 - x _ 38. isbn 978 - 3 - 540 - 42567 - 0. ^ beer, randall d. ( 1997 ). "" the dynamics of adaptive behavior : a research program "". robotics and autonomous systems. 20 ( 2 – 4 ) : 257 – 289. doi : 10. 1016 / s0921 - 8890 ( 96 ) 00063 - 2. ^ sherstinsky, alex ( 2018 - 12 -"
q_007,dense,0.0,0.0,0.0,0.043478260869565216,"  cite book   : cs1 maint : location missing publisher ( link )  febvre, lucien ( 2010 ). the coming of the book : the impact of printing, 1450 – 1800. henri - jean martin, geoffrey nowell - smith, david wootton. london. isbn ","; mackworth, alan ( 2023 ). artificial intelligence, foundations of computational agents ( 3rd ed. ). cambridge university press. doi : 10. 1017 / 9781009258227. isbn 978 - 1 - 0092 - 5819 - 7. ^ russell, stuart ; norvig, peter ( 2020 ). artificial intelligence : a modern approach ( 4th ed. ). pearson. isbn 978 - 0 - 1346 - 1099 - 3. ^ "" why agents are the next frontier of generative ai "". mckinsey digital. 24 july 2024. archived from the original on 3 october 2024. retrieved 10 august 2024. ^ "" introducing copilot search in bing "". blogs. bing. com. 4 april 2025. ^ peters, jay ( 14 march 2023 ). "" the bing ai bot has been secretly running gpt - 4 "". the verge. retrieved 31 august 2025. ^ "" security for microsoft 365 copilot "". learn. microsoft. com. ^ o ' flaherty, kate ( 21 may 2025 ). "" google ai overviews — everything you need to know "". forbes. ^ "" generative ai in search : let google do the searching for you "". google. 14 may 2024. ^ figueiredo, mayara costa ; ankrah, elizabeth ; powell, jacquelyn e. ; epstein, daniel a. ; chen, yunan"
q_008,dense,0.0,0.0,0.0,0.03508771929824561," a b c d e "" ship news "". the standard. no. 7608. london. 1 january 1849.  a b c d e "" ship news "". liverpool mercury etc. no. 2052 ( second ed. ). liverpool. 29 december 1848.  a b c d e "" ship news "". the","able to recognize the wide variety of input as being identical to each other in terms of its textual equivalent. speech segmentation given a sound clip of a person or people speaking, separate it into words. a subtask of speech recognition and typically grouped with it. text - to - speech given a text, transform those units and produce a spoken representation. text - to - speech can be used to aid the visually impaired. word segmentation ( tokenization ) tokenization is a text - processing technique that divides text into individual words or word fragments. this technique results in two key components : a word index and tokenized text. the word index is a list that maps unique words to specific numerical identifiers, and the tokenized text replaces each word with its corresponding numerical token. these numerical tokens are then used in various deep learning methods. for a language like english, this is fairly trivial, since words are usually separated by spaces. however, some written languages like chinese, japanese and thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language. sometimes this process is also used in cases like bag of words ( bow ) creation in data mining. morphological analysis lemmatization of basque words lemmatization the task of removing inflectional endings only and to return the base dictionary form of a word which is also known as a lemma. lemmatization"
q_009,dense,0.0,0.0,0.0,0.06369426751592357,"quantum computing ' .  a b "" alibaba is plowing $ 15 billion into quantum computing ' .  a b "" alibaba is plowing $ 15 billion into quantum computing ' .  a b "" alibaba is plowing $ 15 billion into quantum computing ' .  a b "" alibaba is plowing $ 15 billion into quantum computing '","; azulay, osher ; sintov, avishai ( february 2023 ). "" learning to throw with a handful of samples using decision transformers "". ieee robotics and automation letters. 8 ( 2 ) : 576 – 583. bibcode : 2023iral.... 8.. 576m. doi : 10. 1109 / lra. 2022. 3229266. issn 2377 - 3766. ^ a b ruoss, anian ; deletang, gregoire ; medapati, sourabh ; grau - moya, jordi ; wenliang, li ; catt, elliot ; reid, john ; genewein, tim ( 2024 - 02 - 07 ). "" grandmaster - level chess without search "". arxiv : 2402. 04494v1 [ cs. lg ]. ^ a b wolf, thomas ; debut, lysandre ; sanh, victor ; chaumond, julien ; delangue, clement ; moi, anthony ; cistac, pierric ; rault, tim ; louf, remi ; funtowicz, morgan ; davison, joe ; shleifer, sam ; von platen, patrick ; ma, clara ; jernite, yacine ; plu, julien ; xu, canwen ; le scao, teven ; gugger, sylvain ; drame, maria"
q_010,dense,0.0,0.0,0.0,0.017543859649122806,the term is problematic,"the resulting models were reverse - engineered, and it turned out they used discrete fourier transform. the training of the model also highlighted a phenomenon called grokking, in which the model initially memorizes the training set ( overfitting ), and later suddenly learns to actually perform the calculation. understanding and intelligence see also : philosophy of artificial intelligence and artificial consciousness nlp researchers were evenly split when asked, in a 2022 survey, whether ( untuned ) llms "" could ( ever ) understand natural language in some nontrivial sense "". proponents of "" llm understanding "" believe that some llm abilities, such as mathematical reasoning, imply an ability to "" understand "" certain concepts. a microsoft team argued in 2023 that gpt - 4 "" can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more "" and that gpt - 4 "" could reasonably be viewed as an early ( yet still incomplete ) version of an artificial general intelligence system "" : "" can one reasonably say that a system that passes exams for software engineering candidates is not really intelligent? "" ilya sutskever argues that predicting the next word sometimes involves reasoning and deep insights, for example if the llm has to predict the name of the criminal in an unknown detective novel after processing the entire story leading up to the revelation. some researchers characterize llms as "" alien intelligence "". for example, conjecture ceo connor leahy considers untuned llms to be"
q_011,dense,0.0,0.0,0.0,0.03571428571428571," dickinson , boonsri ( 5 april 2012 ). "" meet the 7 stars of the new silicon valley reality show "". sfgate. retrieved 15 september 2013.  david murray. "" how remote and hybrid work broke performance reviews "".  david murray. "" david murray - forbes technology council "".  brink magazine. retrieved on 6 november 2016.","googlers cracked an sf rival ' s tech model with a single word "". sfgate. archived from the original on 16 december 2023. ^ "" prepare for truly useful large language models "". nature biomedical engineering. 7 ( 2 ) : 85 – 86. 7 march 2023. doi : 10. 1038 / s41551 - 023 - 01012 - 6. pmid 36882584. s2cid 257403466. ^ brinkmann, levin ; baumann, fabian ; bonnefon, jean - francois ; derex, maxime ; muller, thomas f. ; nussberger, anne - marie ; czaplicka, agnieszka ; acerbi, alberto ; griffiths, thomas l. ; henrich, joseph ; leibo, joel z. ; mcelreath, richard ; oudeyer, pierre - yves ; stray, jonathan ; rahwan, iyad ( 2023 - 11 - 20 ). "" machine culture "". nature human behaviour. 7 ( 11 ) : 1855 – 1868. arxiv : 2311. 11388. doi : 10. 1038 / s41562 - 023 - 01742 - 2. issn 2397 - 3374. pmid 37985914. ^ niederhoffer, kate ; kellerman, gabriella rosen ; lee, angela ; liebscher, alex ; rapuano, kristina"
q_012,dense,0.0,0.0,0.0,0.0,renaissance,"the area of computational linguistics maintained strong ties with cognitive studies. as an example, george lakoff offers a methodology to build natural language processing ( nlp ) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, with two defining aspects : apply the theory of conceptual metaphor, explained by lakoff as "" the understanding of one idea, in terms of another "" which provides an idea of the intent of the author. for example, consider the english word big. when used in a comparison ( "" that is a big tree "" ), the author ' s intent is to imply that the tree is physically large relative to other trees or the authors experience. when used metaphorically ( "" tomorrow is a big day "" ), the author ' s intent to imply importance. the intent behind other usages, like in "" she is a big person "", will remain somewhat ambiguous to a person and a cognitive nlp algorithm alike without additional information. assign relative measures of meaning to a word, phrase, sentence or piece of text based on the information presented before and after the piece of text being analyzed, e. g., by means of a probabilistic context - free grammar ( pcfg ). the mathematical equation for such algorithms is presented in us patent 9269353 : r m m ( t o k e n n ) = p m m ( t o k e n n ) × 1 2 d ( [UNK] i = − d d ( ( p"
q_013,dense,0.0,0.0,0.0,0.0,                                                 ,"^ zhang, aston ; lipton, zachary ; li, mu ; smola, alexander j. ( 2024 ). "" 10. modern recurrent neural networks "". dive into deep learning. cambridge new york port melbourne new delhi singapore : cambridge university press. isbn 978 - 1 - 009 - 38943 - 3. ^ rumelhart, david e. ; hinton, geoffrey e. ; williams, ronald j. ( october 1986 ). "" learning representations by back - propagating errors "". nature. 323 ( 6088 ) : 533 – 536. bibcode : 1986natur. 323.. 533r. doi : 10. 1038 / 323533a0. issn 1476 - 4687. ^ a b schmidhuber, jurgen ( 1993 ). habilitation thesis : system modeling and optimization ( pdf ). page 150 ff demonstrates credit assignment across the equivalent of 1, 200 layers in an unfolded rnn. ^ sepp hochreiter ; jurgen schmidhuber ( 21 august 1995 ), long short term memory, wikidata q98967430 ^ a b hochreiter, sepp ; schmidhuber, jurgen ( 1997 - 11 - 01 ). "" long short - term memory "". neural computation. 9 ( 8 ) : 1735 – 1780. doi : 10. 1162 / neco. 1997. 9. 8. 1735. pmid"
q_014,dense,0.0,0.0,0.0,0.01680672268907563,tyr ' ahnee is a figurehead ruler,"explicit symbolic knowledge. although his arguments had been ridiculed and ignored when they were first presented, eventually, ai research came to agree with him. the issue is not resolved : sub - symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. critics such as noam chomsky argue continuing research into symbolic ai will still be necessary to attain general intelligence, in part because sub - symbolic ai is a move away from explainable ai : it can be difficult or impossible to understand why a modern statistical ai program made a particular decision. the emerging field of neuro - symbolic artificial intelligence attempts to bridge the two approaches. neat vs. scruffy main article : neats and scruffies "" neats "" hope that intelligent behavior is described using simple, elegant principles ( such as logic, optimization, or neural networks ). "" scruffies "" expect that it necessarily requires solving a large number of unrelated problems. neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. this issue was actively discussed in the 1970s and 1980s, but eventually was seen as irrelevant. modern ai has elements of both. soft vs. hard computing main article : soft computing finding a provably correct or optimal solution is intractable for many important problems. soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision,"
q_015,dense,0.0,0.0,0.0,0.0,", eds. , eds. , eds. , eds. , eds. , eds. , eds. , eds. , eds. , eds. , eds. , eds. , eds. , eds. ,","from paralegals to fast food cooks, while job demand is likely to increase for care - related professions ranging from personal healthcare to the clergy. in july 2025, ford ceo jim farley predicted that "" artificial intelligence is going to replace literally half of all white - collar workers in the u. s. "" from the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by joseph weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value - based judgement. existential risk main article : existential risk from artificial intelligence recent public debates in artificial intelligence have increasingly focused on its broader societal and ethical implications. it has been argued ai will become so powerful that humanity may irreversibly lose control of it. this could, as physicist stephen hawking stated, "" spell the end of the human race "". this scenario has been common in science fiction, when a computer or robot suddenly develops a human - like "" self - awareness "" ( or "" sentience "" or "" consciousness "" ) and becomes a malevolent character. these sci - fi scenarios are misleading in several ways. first, ai does not require human - like sentience to be an existential risk. modern ai programs are given specific goals and use learning and intelligence to achieve them. philosopher nick bostrom argued that if one gives almost any goal"
q_016,dense,0.0,0.0,0.0,0.05076142131979696,"the trinity is rejected in islam. even though traditional islamic teaching does not formally prohibit using the term "" father "" in reference to god, it does not propagate or encourage it.","information : word embedding each integer token identifier is converted into an embedding vector via a lookup table. equivalently stated, it multiplies a one - hot representation of the token identifier by an embedding matrix m { \ displaystyle m }. for example, if the input token ' s identifier is 3 { \ displaystyle 3 }, then the one - hot representation is [ 0, 0, 0, 1, 0, 0, … ] { \ displaystyle [ 0, 0, 0, 1, 0, 0, \ dots ] }, and its embedding vector is e m b e d ( 3 ) = [ 0, 0, 0, 1, 0, 0, … ] m { \ displaystyle \ mathrm { embed } ( 3 ) = [ 0, 0, 0, 1, 0, 0, \ dots ] m } the token embedding vectors are added to their respective positional encoding vectors ( see below ), producing the sequence of input vectors. the dimension of an embedding vector is called hidden size or embedding size and written as d emb { \ displaystyle d _ { \ text { emb } } }. this size is written as d model { \ displaystyle d _ { \ text { model } } } in the original transformer paper. un - embedding an un - embedding layer is almost the reverse of"
q_017,dense,0.0,0.0,0.0,0.023076923076923075," lanci, michelangelo ( 1845 ). paralipomeni alla illustrazione della sagra scrittura ( in italian ) ( facsimile of the first ed. ). dondey - dupre. pp. 100 – 113.","training data for llms. this produces large volumes of traffic which has led to denial of service issues with many websites. the situation has been described as "" a ddos on the entire internet "" and in some cases scrapers make up the majority of traffic to a site. ai web crawlers may bypass the methods that are usually used to block web scrapers, such as robots. txt files, blocking user - agents and filtering suspicious traffic. website operators have resorted to novel methods such as ai tarpits, but some fear that tarpits will only worsen the burden on servers. mental health clinical and mental health contexts present emerging applications alongside significant safety concerns. research and social media posts suggest that some individuals are using llms to seek therapy or mental health support. in early 2025, a survey by sentio university found that nearly half ( 48. 7 % ) of 499 u. s. adults with ongoing mental health conditions who had used llms reported turning to them for therapy or emotional support, including help with anxiety, depression, loneliness, and similar concerns. llms can produce hallucinations — plausible but incorrect statements — which may mislead users in sensitive mental health contexts. research also shows that llms may express stigma or inappropriate agreement with maladaptive thoughts, reflecting limitations in replicating the judgment and relational skills of human therapists. evaluations of crisis scenarios indicate that some llms lack effective safety protocols, such as assessing suicide risk or making appropriate"
q_018,dense,0.0,0.0,0.0,0.0,"greater than 20, 000 hz","d, x ⟩, sin ⟨ w d, x ⟩ ] t { \ displaystyle \ varphi ( x ) = { \ frac { 1 } { \ sqrt { d } } } [ \ cos \ langle w _ { 1 }, x \ rangle, \ sin \ langle w _ { 1 }, x \ rangle, \ cdots \ cos \ langle w _ { d }, x \ rangle, \ sin \ langle w _ { d }, x \ rangle ] ^ { t } } where w 1,..., w d { \ displaystyle w _ { 1 },..., w _ { d } } are independent samples from the normal distribution n ( 0, σ 2 i ) { \ displaystyle n ( 0, \ sigma ^ { 2 } i ) }. this choice of parameters satisfy e [ ⟨ φ ( x ), φ ( y ) ⟩ ] = e − ‖ x − y ‖ 2 2 σ 2 { \ displaystyle \ mathbb { e } [ \ langle \ varphi ( x ), \ varphi ( y ) \ rangle ] = e ^ { - { \ frac { \ | x - y \ | ^ { 2 } } { 2 \ sigma ^ { 2 } } } } }, or e ⟨ x, y ⟩ / σ 2 = e [ ⟨ e ‖ x ‖ 2 / 2 σ 2 φ"
q_019,dense,0.0,0.0,0.0,0.05741626794258374,"  cite book   : cs1 maint : location missing publisher ( link )  febvre, lucien ( 2010 ). the coming of the book : the impact of printing , 1450 – 1800. henri - jean martin , geoffrey nowell - smith , david wootton. london. is","commitments from companies "". reuters. 21 may 2024. retrieved 23 may 2024. ^ "" frontier ai safety commitments, ai seoul summit 2024 "". gov. uk. 21 may 2024. archived from the original on 23 may 2024. retrieved 23 may 2024. ^ a b buntz, brian ( 3 november 2024 ). "" quality vs. quantity : us and china chart different paths in global ai patent race in 2024 / geographical breakdown of ai patents in 2024 "". research & development world. r & d world. archived from the original on 9 december 2024. ^ a b russell & norvig 2021, p. 9. ^ a b c copeland, j., ed. ( 2004 ). the essential turing : the ideas that gave birth to the computer age. oxford, england : clarendon press. isbn 0 - 1982 - 5079 - 7. ^ "" google books ngram "". archived from the original on 5 october 2024. retrieved 5 october 2024. ^ ai ' s immediate precursors : mccorduck ( 2004, pp. 51 – 107 ), crevier ( 1993, pp. 27 – 32 ), russell & norvig ( 2021, pp. 8 – 17 ), moravec ( 1988, p. 3 ) ^ a b turing ' s original publication of the turing test in "" computing machinery and intelligence "" : turing ( 1950 ) historical influence and philosophical implications : haugeland"
q_020,dense,0.0,0.0,0.0,0.0,evelyn everett - green,"; kuttler, heinrich ; lewis, mike ; yih, wen - tau ; rocktaschel, tim ; riedel, sebastian ; kiela, douwe ( 2020 ). "" retrieval - augmented generation for knowledge - intensive nlp tasks "". advances in neural information processing systems. 33. curran associates, inc. : 9459 – 9474. arxiv : 2005. 11401. archived from the original on 2023 - 06 - 12. retrieved 2023 - 06 - 12. ^ dickson, ben ( 2025 - 04 - 02 ). "" the tool integration problem that ' s holding back enterprise ai ( and how cotools solves it ) "". venturebeat. retrieved 2025 - 05 - 26. ^ liang, yaobo ; wu, chenfei ; song, ting ; wu, wenshan ; xia, yan ; liu, yu ; ou, yang ; lu, shuai ; ji, lei ; mao, shaoguang ; wang, yun ; shou, linjun ; gong, ming ; duan, nan ( 2024 ). "" taskmatrix. ai : completing tasks by connecting foundation models with millions of apis "". science. 3 0063. doi : 10. 34133 / icomputing. 0063. ^ patil, shishir g. ; zhang, tianjun ; wang, xin ; gonzalez, joseph e. ( 2023 - 05 - 01"
q_021,dense,0.0,0.0,0.0,0.0,a320,"be easier to train, requiring no warm - up, leading to faster convergence. pseudocode the following is the pseudocode for a standard pre - ln encoder – decoder transformer, adapted from formal algorithms for transformers input : encoder input t _ e decoder input t _ d output : array of probability distributions, with shape ( decoder vocabulary size x length ( decoder output sequence ) ) / * encoder * / z _ e ← encoder. tokenizer ( t _ e ) for each t in 1 : length ( z _ e ) do z _ e [ t ] ← encoder. embedding ( z _ e [ t ] ) + encoder. positional _ embedding ( t ) for each l in 1 : length ( encoder. layers ) do layer ← encoder. layers [ l ] / * first sublayer * / z _ e _ copy ← copy ( z _ e ) for each t in 1 : length ( z _ e ) do z _ e [ t ] ← layer. layer _ norm ( z _ e [ t ] ) z _ e ← layer. multihead _ attention ( z _ e, z _ e, z _ e ) for each t in 1 : length ( z _ e ) do z _ e [ t ] ← z _ e [ t ] + z _ e _ copy [ t ] / * second sublayer * / z _ e"
q_022,dense,0.0,0.0,0.0,0.025862068965517238," lanci, michelangelo ( 1845 ). paralipomeni alla illustrazione della sagra scrittura ( in italian ) ( facsimile of the first ed. ). dondey - dupre. pp. 100 – 113.","predict the name of the criminal in an unknown detective novel after processing the entire story leading up to the revelation. some researchers characterize llms as "" alien intelligence "". for example, conjecture ceo connor leahy considers untuned llms to be like inscrutable alien "" shoggoths "", and believes that rlhf tuning creates a "" smiling facade "" obscuring the inner workings of the llm : "" if you don ' t push it too far, the smiley face stays on. but then you give it [ an unexpected ] prompt, and suddenly you see this massive underbelly of insanity, of weird thought processes and clearly non - human understanding. "" in contrast, some skeptics of llm understanding believe that existing llms are "" simply remixing and recombining existing writing "", a phenomenon known as stochastic parrot, or they point to the deficits existing llms continue to have in prediction skills, reasoning skills, agency, and explainability. for example, gpt - 4 has natural deficits in planning and in real - time learning. generative llms have been observed to confidently assert claims of fact which do not seem to be justified by their training data, a phenomenon which has been termed "" hallucination "". specifically, hallucinations in the context of llms correspond to the generation of text or responses that seem syntactically sound, fluent, and natural but are factually incorrect, nonsen"
q_023,dense,0.0,0.0,0.0,0.0,twilight zone,": 10. 1353 / pbm. 2000. 0001. issn 1529 - 8795. pmid 10804585. ^ renshaw, birdsey ( 1946 - 05 - 01 ). "" central effects of centripetal impulses in axons of spinal ventral roots "". journal of neurophysiology. 9 ( 3 ) : 191 – 204. doi : 10. 1152 / jn. 1946. 9. 3. 191. issn 0022 - 3077. pmid 21028162. ^ a b grossberg, stephen ( 2013 - 02 - 22 ). "" recurrent neural networks "". scholarpedia. 8 ( 2 ) : 1888. bibcode : 2013schpj... 8. 1888g. doi : 10. 4249 / scholarpedia. 1888. issn 1941 - 6016. ^ a b c rosenblatt, frank ( 1961 - 03 - 15 ). dtic ad0256582 : principles of neurodynamics. perceptrons and the theory of brain mechanisms. defense technical information center. ^ f. rosenblatt, "" perceptual generalization over transformation groups "", pp. 63 - - 100 in self - organizing systems : proceedings of an inter - disciplinary conference, 5 and 6 may 1959. edited by marshall c. yovitz and scott cameron. london, new york, [ etc. ], pergamon press, 1960"
q_024,dense,0.0,0.0,0.0,0.024242424242424242,What is the author's last name?,"time ) skepticism by most participants. until then, neural learning was basically rejected because of its lack of statistical interpretability. until 2015, deep learning had evolved into the major framework of nlp. [ link is broken, try http : / / web. stanford. edu / class / cs224n / ] ^ segev, elad ( 2022 ). semantic network analysis in social sciences. london : routledge. isbn 978 - 0 - 367 - 63652 - 4. archived from the original on 5 december 2021. retrieved 5 december 2021. ^ yi, chucai ; tian, yingli ( 2012 ), "" assistive text reading from complex background for blind persons "", camera - based document analysis and recognition, lecture notes in computer science, vol. 7139, springer berlin heidelberg, pp. 15 – 28, citeseerx 10. 1. 1. 668. 869, doi : 10. 1007 / 978 - 3 - 642 - 29364 - 1 _ 2, isbn 978 - 3 - 642 - 29363 - 4 { { citation } } : cs1 maint : work parameter with isbn ( link ) ^ a b "" natural language processing ( nlp ) - a complete guide "". www. deeplearning. ai. 2023 - 01 - 11. retrieved 2024 - 05 - 05. ^ "" geeksforgeeks. ( n. d. ). tokenization in natural language"
q_025,dense,0.0,0.0,0.0,0.02380952380952381,"php? title = ruth berman & oldid = 1330522436 "". archived from the original on 2022 - 06 - 18 . retrieved 2017 - 03 - 01.  "" brief biographies of the martyrs "". bi - weekly eleven ( in burmese ). weekly eleven publishing group . v t e retrieved from "" https : / / en.","##opomorphism foundation models list of large language models list of chatbots language model benchmark reinforcement learning small language model references ^ a b c bommasani, rishi ; hudson, drew a. ; adeli, ehsan ; altman, russ ; arora, simran ; von arx, matthew ; bernstein, michael s. ; bohg, jeannette ; bosselut, antoine ; brunskill, emma ( 2021 ). "" on the opportunities and risks of foundation models "". arxiv : 2108. 07258 [ cs. lg ]. ^ a b brown, tom b. ; mann, benjamin ; ryder, nick ; subbiah, melanie ; kaplan, jared ; dhariwal, prafulla ; neelakantan, arvind ; shyam, pranav ; sastry, girish ; askell, amanda ( 2020 ). "" language models are few - shot learners "". arxiv : 2005. 14165 [ cs. cl ]. ^ a b c brown, tom b. ; mann, benjamin ; ryder, nick ; subbiah, melanie ; kaplan, jared ; dhariwal, prafulla ; neelakantan, arvind ; shyam, pranav ; sastry, girish ; askell, amanda ; agarwal, sandhini ; herbert - voss, ariel ; krueger, gretchen ; henighan, tom ; child"
q_026,dense,0.0,0.0,0.0,0.03187250996015936,the president of space was voiced by tom kane.,"of research were continued, e. g., the development of chatterbots with racter and jabberwacky. an important development ( that eventually led to the statistical turn in the 1990s ) was the rising importance of quantitative evaluation in this period. statistical nlp ( 1990s – present ) up until the 1980s, most natural language processing systems were based on complex sets of hand - written rules. starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. this shift was influenced by increasing computational power ( see moore ' s law ) and a decline in the dominance of chomskyan linguistic theories... ( e. g. transformational grammar ), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine - learning approach to language processing. 1990s : many of the notable early successes in statistical methods in nlp occurred in the field of machine translation, due especially to work at ibm research, such as ibm alignment models. these systems were able to take advantage of existing multilingual textual corpora that had been produced by the parliament of canada and the european union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. however, many systems relied on corpora that were specifically developed for the tasks they were designed to perform. this reliance has been a major limitation to their broader effectiveness and continues to affect similar systems. consequently,"
q_027,dense,0.0,0.0,0.0,0.0145985401459854,'ve published a poem of mine,"; lovreglio, ruggiero ; kuligowski, erica ( 2024 ). "" ai for large - scale evacuation modeling : promises and challenges "". interpretable machine learning for the analysis, design, assessment, and informed decision making for civil infrastructure. pp. 185 – 204. doi : 10. 1016 / b978 - 0 - 12 - 824073 - 1. 00014 - 9. isbn 978 - 0 - 12 - 824073 - 1. ^ gomaa, islam ; adelzadeh, masoud ; gwynne, steven ; spencer, bruce ; ko, yoon ; benichou, noureddine ; ma, chunyun ; elsagan, nour ; duong, dana ; zalok, ehab ; kinateder, max ( 1 november 2021 ). "" a framework for intelligent fire detection and evacuation system "". fire technology. 57 ( 6 ) : 3179 – 3185. doi : 10. 1007 / s10694 - 021 - 01157 - 3. ^ zhao, xilei ; lovreglio, ruggiero ; nilsson, daniel ( 1 may 2020 ). "" modelling and interpreting pre - evacuation decision - making using machine learning "". automation in construction. 113 103140. doi : 10. 1016 / j. autcon. 2020. 103140. hdl : 10179 / 17315. ^ "" india ' s"
q_028,dense,0.0,0.0,0.0,0.03125," lanci, michelangelo ( 1845 ). paralipomeni alla illustrazione della sagra scrittura ( in italian ) ( facsimile of the first ed. ). dondey - dupre. pp. 100 – 113.","study. in the late 2010s and early 2020s, agi companies began to deliver programs that created enormous interest. in 2015, alphago, developed by deepmind, beat the world champion go player. the program taught only the game ' s rules and developed a strategy by itself. gpt - 3 is a large language model that was released in 2020 by openai and is capable of generating high - quality human - like text. chatgpt, launched on 30 november 2022, became the fastest - growing consumer software application in history, gaining over 100 million users in two months. it marked what is widely regarded as ai ' s breakout year, bringing it into the public consciousness. these programs, and others, inspired an aggressive ai boom, where large companies began investing billions of dollars in ai research. according to ai impacts, about us $ 50 billion annually was invested in "" ai "" around 2022 in the u. s. alone and about 20 % of the new u. s. computer science phd graduates have specialized in "" ai "". about 800, 000 "" ai "" - related u. s. job openings existed in 2022. according to pitchbook research, 22 % of newly funded startups in 2024 claimed to be ai companies. philosophy main article : philosophy of artificial intelligence philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. another major focus has been whether machines can be conscious, and the associated ethical implications. many other topics in"
q_029,dense,0.0,0.0,0.0,0.0,/ / en. wikipedia. org / w / index. php? title =                                      ,"##g. 2. 2. 30247. 50087. ^ iraqi, amjad ( 3 april 2024 ). "" ' lavender ' : the ai machine directing israel ' s bombing spree in gaza "". + 972 magazine. archived from the original on 10 october 2024. retrieved 6 april 2024. ^ davies, harry ; mckernan, bethan ; sabbagh, dan ( 1 december 2023 ). "" ' the gospel ' : how israel uses ai to select bombing targets in gaza "". the guardian. archived from the original on 6 december 2023. retrieved 4 december 2023. ^ marti, j werner ( 10 august 2024 ). "" drohnen haben den krieg in der ukraine revolutioniert, doch sie sind empfindlich auf storsender – deshalb sollen sie jetzt autonom operieren "". neue zurcher zeitung ( in german ). archived from the original on 10 august 2024. retrieved 10 august 2024. ^ banh, leonardo ; strobel, gero ( 2023 ). "" generative artificial intelligence "". electronic markets. 33 ( 1 ) 63. doi : 10. 1007 / s12525 - 023 - 00680 - 1. ^ pasick, adam ( 27 march 2023 ). "" artificial intelligence glossary : neural networks and other terms explained "". the new york times"
q_030,dense,0.0,0.0,0.0,0.008097165991902834,a normal trial,"in areas where there is hope that the future will be better than the past. it is descriptive rather than prescriptive. bias and unfairness may go undetected because the developers are overwhelmingly white and male : among ai engineers, about 4 % are black and 20 % are women. there are various conflicting definitions and mathematical models of fairness. these notions depend on ethical assumptions, and are influenced by beliefs about society. one broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. representational fairness tries to ensure that ai systems do not reinforce negative stereotypes or render certain groups invisible. procedural fairness focuses on the decision process rather than the outcome. the most relevant notions of fairness may depend on the context, notably the type of ai application and the stakeholders. the subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. having access to sensitive attributes such as race or gender is also considered by many ai ethicists to be necessary in order to compensate for biases, but it may conflict with anti - discrimination laws. at the 2022 acm conference on fairness, accountability, and transparency a paper reported that a clip ‑ based ( contrastive language - image pre - training ) robotic system reproduced harmful gender ‑ and race ‑ linked stereotypes in a simulated manipulation task. the authors recommended robot ‑ learning methods which physically manifest such harms be "" paused, reworked, or even wound down"
q_031,dense,0.0,0.0,0.0,0.0,                                                 ,"- 100 in self - organizing systems : proceedings of an inter - disciplinary conference, 5 and 6 may 1959. edited by marshall c. yovitz and scott cameron. london, new york, [ etc. ], pergamon press, 1960. ix, 322 p. ^ nakano, kaoru ( 1971 ). "" learning process in a model of associative memory "". pattern recognition and machine learning. pp. 172 – 186. doi : 10. 1007 / 978 - 1 - 4615 - 7566 - 5 _ 15. isbn 978 - 1 - 4615 - 7568 - 9. ^ nakano, kaoru ( 1972 ). "" associatron - a model of associative memory "". ieee transactions on systems, man, and cybernetics. smc - 2 ( 3 ) : 380 – 388. bibcode : 1972itsmc... 2.. 380n. doi : 10. 1109 / tsmc. 1972. 4309133. ^ amari, shun - ichi ( 1972 ). "" learning patterns and pattern sequences by self - organizing nets of threshold elements "". ieee transactions. c ( 21 ) : 1197 – 1206. ^ little, w. a. ( 1974 ). "" the existence of persistent states in the brain "". mathematical biosciences. 19 ( 1 – 2 ) : 101 – 120. doi : 10. 1016"
q_032,dense,0.0,0.0,0.0,0.03658536585365854,"michael gerson , post columnist and bush speechwriter on 9 / 11, dies at 58",". unice. fr. archived from the original on 2021 - 04 - 18. retrieved 2021 - 03 - 09. ^ "" nlp approaches to computational argumentation – acl 2016, berlin "". retrieved 2021 - 03 - 09. ^ administration. "" centre for language technology ( clt ) "". macquarie university. retrieved 2021 - 01 - 11. ^ "" shared task : grammatical error correction "". www. comp. nus. edu. sg. retrieved 2021 - 01 - 11. ^ "" shared task : grammatical error correction "". www. comp. nus. edu. sg. retrieved 2021 - 01 - 11. ^ duan, yucong ; cruz, christophe ( 2011 ). "" formalizing semantic of natural language through conceptualization from existence "". international journal of innovation, management and technology. 2 ( 1 ) : 37 – 42. archived from the original on 2011 - 10 - 09. ^ "" u b u w e b : : racter "". www. ubu. com. retrieved 2020 - 08 - 17. ^ writer, beta ( 2019 ). lithium - ion batteries. doi : 10. 1007 / 978 - 3 - 030 - 16800 - 1. isbn 978 - 3 - 030 - 16799 - 8. s2cid 155818532. ^ "" document understanding ai on google cloud ( cloud next ' 19 ) – youtube "". www. youtube. com. 11 april 2019"
q_033,dense,0.0,0.0,0.0,0.0,"""","##set. this basic construction can be applied with more sophistication to improve the model. the image encoder may be frozen to improve stability. this type of method, where embeddings from multiple modalities are fused and the predictor is trained on the combined embeddings, is called early fusion. another method, called intermediate fusion, involves each modality being first processed independently to obtain modality - specific representations ; then these intermediate representations are fused together. in general, cross - attention is used for integrating information from different modalities. as an example, the flamingo model uses cross - attention layers to inject visual information into its pre - trained language model. non - natural languages llms can handle programming languages similarly to how they handle natural languages. no special change in token handling is needed as code, like human language, is represented as plain text. llms can generate code based on problems or instructions written in natural language. they can also describe code in natural language or translate it into other programming languages. they were originally used as a code completion tool, but advances have moved them towards automatic programming. services such as github copilot offer llms specifically trained, fine - tuned, or prompted for programming. in computational biology, transformer - base architectures, such as dna llms, have also proven useful in analyzing biological sequences : protein, dna, and rna. with proteins they appear able to capture a degree of "" grammar """
q_034,dense,0.0,0.0,0.0,0.021897810218978103," lanci, michelangelo ( 1845 ). paralipomeni alla illustrazione della sagra scrittura ( in italian ) ( facsimile of the first ed. ). dondey - dupre. pp. 100 – 113.","be researching battlefield robots. ai tools make it easier for authoritarian governments to efficiently control their citizens in several ways. face and voice recognition allow widespread surveillance. machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. recommendation systems can precisely target propaganda and misinformation for maximum effect. deepfakes and generative ai aid in producing misinformation. advanced ai can make authoritarian centralized decision - making more competitive than liberal and decentralized systems such as markets. it lowers the cost and difficulty of digital warfare and advanced spyware. all these technologies have been available since 2020 or earlier — ai facial recognition systems are already being used for mass surveillance in china. there are many other ways in which ai is expected to help bad actors, some of which can not be foreseen. for example, machine - learning ai is able to design tens of thousands of toxic molecules in a matter of hours. technological unemployment main articles : workplace impact of artificial intelligence and technological unemployment economists have frequently highlighted the risks of redundancies from ai, and speculated about unemployment if there is no adequate social policy for full employment. in the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that "" we ' re in uncharted territory "" with ai. a survey of economists showed disagreement about whether the increasing use of robots and ai will cause a substantial increase in long - term unemployment, but they generally agree that it could be a net benefit if"
q_035,dense,0.0,0.0,0.0,0.03508771929824561," a b c d e f g h i "" ohioans win fellowships "". the daily reporter. dover, ohio, usa. 1964 - 03 - 30. p. 2. retrieved 2023 - 07 - 16.  a b c d e f g h i j k l m n",", accountability, and transparency. pp. 599 – 627. arxiv : 2504. 18412. doi : 10. 1145 / 3715275. 3732039. isbn 979 - 8 - 4007 - 1482 - 5. ^ grabb, declan ; lamparth, max ; vasan, nina ( 2024 - 08 - 14 ). "" risks from language models for automated mental healthcare : ethics and structure for implementation "". arxiv : 2406. 11852 [ cs. cy ]. ^ mcbain, ryan k. ; cantor, jonathan h. ; zhang, li ang ; baker, olesya ; zhang, fang ; halbisen, alyssa ; kofner, aaron ; breslau, joshua ; stein, bradley ; mehrotra, ateev ; yu, hao ( 2025 - 03 - 05 ). "" competency of large language models in evaluating appropriate responses to suicidal ideation : comparative study "". journal of medical internet research. 27 ( 1 ) e67891. doi : 10. 2196 / 67891. pmc 11928068. pmid 40053817. ^ li, fei - fei ; etchemendy, john ( 2024 - 05 - 22 ). "" no, today ' s ai isn ' t sentient. here ' s how we know "". time. retrieved 2024 - 05 - 22. ^"
q_036,dense,0.0,0.0,0.0,0.0,brightness,"} } f ( t ) = ( e i t / r k ) k = 0, 1, …, d 2 − 1 { \ displaystyle f ( t ) = \ left ( e ^ { it / r ^ { k } } \ right ) _ { k = 0, 1, \ ldots, { \ frac { d } { 2 } } - 1 } } where r = n 2 / d { \ displaystyle r = n ^ { 2 / d } }. the main reason for using this positional encoding function is that using it, shifts are linear transformations : f ( t + δ t ) = d i a g ( f ( δ t ) ) f ( t ) { \ displaystyle f ( t + \ delta t ) = \ mathrm { diag } ( f ( \ delta t ) ) f ( t ) } where δ t ∈ r { \ displaystyle \ delta t \ in \ mathbb { r } } is the distance one wishes to shift. this allows the transformer to take any encoded position, and find the encoding of the position n - steps - ahead or n - steps - behind, by a matrix multiplication. by taking a linear sum, any convolution can also be implemented as linear transformations : [UNK] j c j f ( t + δ t j ) = ( [UNK] j c j d i a g ( f ( δ t j ) ) ) f ( t ) { \ displaystyle"
q_037,dense,0.0,0.0,0.0,0.0," "" our history "". aopen.  "" vertical integration platform","of only twenty words, because that was all that would fit in a computer memory at the time. 1970s : during the 1970s, many programmers began to write "" conceptual ontologies "", which structured real - world information into computer - understandable data. examples are margie ( schank, 1975 ), sam ( cullingford, 1978 ), pam ( wilensky, 1978 ), talespin ( meehan, 1976 ), qualm ( lehnert, 1977 ), politics ( carbonell, 1979 ), and plot units ( lehnert 1981 ). during this time, the first chatterbots were written ( e. g., parry ). 1980s : the 1980s and early 1990s mark the heyday of symbolic methods in nlp. focus areas of the time included research on rule - based parsing ( e. g., the development of hpsg as a computational operationalization of generative grammar ), morphology ( e. g., two - level morphology ), semantics ( e. g., lesk algorithm ), reference ( e. g., within centering theory ) and other areas of natural language understanding ( e. g., in the rhetorical structure theory ). other lines of research were continued, e. g., the development of chatterbots with racter and jabberwacky. an important development ( that eventually led to the statistical turn in the 1990s ) was the rising importance of quantitative evaluation in this period."
q_038,dense,0.0,0.0,0.0,0.0,evelyn everett - green,". cv ]. ^ campolucci, paolo ; uncini, aurelio ; piazza, francesco ; rao, bhaskar d. ( 1999 ). "" on - line learning algorithms for locally recurrent neural networks "". ieee transactions on neural networks. 10 ( 2 ) : 253 – 271. bibcode : 1999itnn... 10.. 253c. citeseerx 10. 1. 1. 33. 7550. doi : 10. 1109 / 72. 750549. pmid 18252525. ^ wan, eric a. ; beaufays, francoise ( 1996 ). "" diagrammatic derivation of gradient algorithms for neural networks "". neural computation. 8 : 182 – 201. doi : 10. 1162 / neco. 1996. 8. 1. 182. s2cid 15512077. ^ a b campolucci, paolo ; uncini, aurelio ; piazza, francesco ( 2000 ). "" a signal - flow - graph approach to on - line gradient calculation "". neural computation. 12 ( 8 ) : 1901 – 1927. citeseerx 10. 1. 1. 212. 5406. doi : 10. 1162 / 089976600300015196. pmid 10953244. s2cid 15090951. ^ graves, alex ; fernandez, santiago ; gomez, faustino j. ( 2006 ). "" connectionist temporal classification : label"
q_039,dense,0.0,0.0,0.0,0.014084507042253521,"cs1 maint : multiple names : authors list ( link ) volume 6 kellogg , paul u., editor","- zheng ; lee, yee - chun ( 1992 ). "" learning and extracting finite state automata with second - order recurrent neural networks "" ( pdf ). neural computation. 4 ( 3 ) : 393 – 405. doi : 10. 1162 / neco. 1992. 4. 3. 393. s2cid 19666035. ^ omlin, christian w. ; giles, c. lee ( 1996 ). "" constructing deterministic finite - state automata in recurrent neural networks "". journal of the acm. 45 ( 6 ) : 937 – 972. citeseerx 10. 1. 1. 32. 2364. doi : 10. 1145 / 235809. 235811. s2cid 228941. ^ paine, rainer w. ; tani, jun ( 2005 - 09 - 01 ). "" how hierarchical control self - organizes in artificial adaptive systems "". adaptive behavior. 13 ( 3 ) : 211 – 225. doi : 10. 1177 / 105971230501300303. s2cid 9932565. ^ a b "" burns, benureau, tani ( 2018 ) a bergson - inspired adaptive time constant for the multiple timescales recurrent neural network model. jnns "". ^ barkan, oren ; benchimol, jonathan ; caspi, itamar ; cohen, eliya ;"
q_040,dense,0.0,0.0,0.0,0.05681818181818182,", there is a sloping roof with a sloping ridge and a sloping ridge.","^ { k } }, and the value weights w v { \ displaystyle w ^ { v } }. the module takes three sequences, a query sequence, a key sequence, and a value sequence. the query sequence is a sequence of length ℓ seq, query { \ displaystyle \ ell _ { \ text { seq, query } } }, and each entry is a vector of dimension d emb, query { \ displaystyle d _ { \ text { emb, query } } }. similarly for the key and value sequences. for each vector x i, query { \ displaystyle x _ { i, { \ text { query } } } } in the query sequence, it is multiplied by a matrix w q { \ displaystyle w ^ { q } } to produce a query vector q i = x i, query w q { \ displaystyle q _ { i } = x _ { i, { \ text { query } } } w ^ { q } }. the matrix of all query vectors is the query matrix : q = x query w q { \ displaystyle q = x _ { \ text { query } } w ^ { q } } similarly, we construct the key matrix k = x key w k { \ displaystyle k = x _ { \ text { key } } w ^ { k } } and the value matrix v = x value w v { \ displaystyle v = x _ { \ text {"
q_041,dense,0.0,0.0,0.0,0.02395209580838323,a year of adversity.,", in the smallest gpt - 2 model, there are only self - attention mechanisms. it has the following dimensions : d emb = 768, n head = 12, d head = 64 { \ displaystyle d _ { \ text { emb } } = 768, n _ { \ text { head } } = 12, d _ { \ text { head } } = 64 } since 12 × 64 = 768 { \ displaystyle 12 \ times 64 = 768 }, its output projection matrix w o ∈ r ( 12 × 64 ) × 768 { \ displaystyle w ^ { o } \ in \ mathbb { r } ^ { ( 12 \ times 64 ) \ times 768 } } is a square matrix. masked attention the transformer architecture is constructed to calculate output tokens iteratively. assuming t = 0 { \ displaystyle t = 0 } refers to the calculation of the first output token i = 0 { \ displaystyle i = 0 }, for step t > 0 { \ displaystyle t > 0 }, the output token i = 0 { \ displaystyle i = 0 } shall remain constant. this ensures properties of the model similar to autoregressive models. therefore, at every time step t { \ displaystyle t }, the calculation for all outputs i { \ displaystyle i } should not have access to tokens at position j { \ displaystyle j } for j > = i { \"
q_042,dense,0.0,0.0,0.0,0.0,expo 2010 - expo 2010 - expo 2010 - expo 2010 - expo 2010 - expo 2010 - expo 2010 - expo 2010 - expo 2010 - expo 2010 - expo 2010 - expo 2010 - expo 2010 - expo 2010 - expo 2010 - expo 2010 - expo 2010 - expo 2010 - expo 2010 - expo 2010 -,"acm conference on fairness, accountability, and transparency ( facct ' 22 ). seoul, south korea : association for computing machinery. doi : 10. 1145 / 3531146. 3533138. ^ for accessible summaries, see the georgia tech release and sciencedaily coverage of the study ' s findings. "" flawed ai makes robots racist, sexist "". georgia tech research news. 23 june 2022. ^ "" robots turn racist and sexist with flawed ai, study finds "". sciencedaily. 21 june 2022. ^ sample ( 2017 ). ^ "" black box ai "". 16 june 2023. archived from the original on 15 june 2024. retrieved 5 october 2024. ^ christian ( 2020 ), p. 110. ^ christian ( 2020 ), pp. 88 – 91. ^ christian ( 2020, p. 83 ) ; russell & norvig ( 2021, p. 997 ) ^ christian ( 2020 ), p. 91. ^ christian ( 2020 ), p. 83. ^ verma ( 2021 ). ^ rothman ( 2020 ). ^ christian ( 2020 ), pp. 105 – 108. ^ christian ( 2020 ), pp. 108 – 112. ^ ropek, lucas ( 21 may 2024 ). "" new anthropic research sheds light on ai ' s ' black box ' "". gizmodo. archived from the original on 5 october 2024. retrieved 23"
q_043,dense,0.0,0.0,0.0,0.0,biden married neilia hunter,"2, t + 1 ) { \ displaystyle f _ { \ theta _ { 2 } } : ( x _ { 1, t }, h _ { 2, t } ) \ mapsto ( x _ { 2, t }, h _ { 2, t + 1 } ) }.... layer n { \ displaystyle n } has hidden vector h n, t { \ displaystyle h _ { n, t } }, parameters θ n { \ displaystyle \ theta _ { n } }, and maps f θ n : ( x n − 1, t, h n, t ) ↦ ( x n, t, h n, t + 1 ) { \ displaystyle f _ { \ theta _ { n } } : ( x _ { n - 1, t }, h _ { n, t } ) \ mapsto ( x _ { n, t }, h _ { n, t + 1 } ) }. each layer operates as a stand - alone rnn, and each layer ' s output sequence is used as the input sequence to the layer above. there is no conceptual limit to the depth of stacked rnn. bidirectional main article : bidirectional recurrent neural networks bidirectional rnn a bidirectional rnn ( birnn ) is composed of two rnns, one processing the input sequence in one direction, and another in the opposite direction. abstractly,"
q_044,dense,0.0,0.0,0.0,0.009478672985781991,see it today.,"natural language generation. in 2022, a chatbot based on gpt - 3, chatgpt, became unexpectedly popular, triggering a boom around large language models. since 2020, transformers have been applied in modalities beyond text, including the vision transformer, speech recognition, robotics, and multimodal. the vision transformer, in turn, stimulated new developments in convolutional neural networks. image and video generators like dall - e ( 2021 ), stable diffusion 3 ( 2024 ), and sora ( 2024 ), use transformers to analyse input data ( like text prompts ) by breaking it down into "" tokens "" and then calculating the relevance between each token using self - attention, which helps the model understand the context and relationships within the data. training methods for stabilizing training the plain transformer architecture had difficulty in converging. in the original paper, the authors recommended using learning rate warmup. that is, the learning rate should linearly scale up from 0 to maximal value for the first part of the training ( usually recommended to be 2 % of the total number of training steps ), before decaying again. a 2020 paper found that using layer normalization before ( instead of after ) multihead attention and feedforward layers stabilizes training, not requiring learning rate warmup. this is the "" pre - ln transformer "" and is more commonly used, compared to the original "" post - ln transformer"
q_045,dense,0.0,0.0,0.0,0.0,1274016911.,"##3 - 12 - 29. ^ amodei, dario ; olah, chris ; steinhardt, jacob ; christiano, paul ; schulman, john ; mane, dan ( 2016 - 06 - 21 ). "" concrete problems in ai safety "". arxiv : 1606. 06565 [ cs. ai ]. ^ lyons, jessica ( 2025 - 09 - 26 ). "" prompt injection – and a $ 5 domain – trick salesforce agentforce into leaking sales "". the register. retrieved 2025 - 09 - 26. ^ carlini, nicholas ; tramer, florian ; wallace, eric ( 2021 - 08 - 11 ). "" extracting training data from large language models "" ( pdf ). usenix association. retrieved 2025 - 10 - 02. ^ zhao, yao ; zhang, yun ; sun, yong ( 2023 - 06 - 07 ). "" the debate over understanding in ai ' s large language models "". proceedings of the national academy of sciences. 120 ( 13 ) e2215907120. arxiv : 2306. 05499. bibcode : 2023pnas.. 12015907m. doi : 10. 1073 / pnas. 2215907120. pmc 10068812. pmid 36943882. ^ buolamwini, joy ; gebru, timnit ( 2018 - 01 - 01 )"
q_046,dense,0.0,0.0,0.0,0.0,brickwork,"} of type ( x t, h t ) ↦ ( y t, h t + 1 ) { \ displaystyle ( x _ { t }, h _ { t } ) \ mapsto ( y _ { t }, h _ { t + 1 } ) }, where x t { \ displaystyle x _ { t } } : input vector ; h t { \ displaystyle h _ { t } } : hidden vector ; y t { \ displaystyle y _ { t } } : output vector ; θ { \ displaystyle \ theta } : neural network parameters. in words, it is a neural network that maps an input x t { \ displaystyle x _ { t } } into an output y t { \ displaystyle y _ { t } }, with the hidden vector h t { \ displaystyle h _ { t } } playing the role of "" memory "", a partial record of all previous input - output pairs. at each step, it transforms input to an output, and modifies its "" memory "" to help it to better perform future processing. the illustration to the right may be misleading to many because practical neural network topologies are frequently organized in "" layers "" and the drawing gives that appearance. however, what appears to be layers are, in fact, different steps in time, "" unfolded "" to produce the appearance of layers. stacked rnn stacked rnn a stacked rnn, or deep rnn, is composed of multiple rnns"
q_047,dense,0.0,0.0,0.0,0.0,"reynolds , kimberley. girls only?.",", however, underestimated the difficulty of the problem. in 1974, both the u. s. and british governments cut off exploratory research in response to the criticism of sir james lighthill and ongoing pressure from the u. s. congress to fund more productive projects. minsky and papert ' s book perceptrons was understood as proving that artificial neural networks would never be useful for solving real - world tasks, thus discrediting the approach altogether. the "" ai winter "", a period when obtaining funding for ai projects was difficult, followed. in the early 1980s, ai research was revived by the commercial success of expert systems, a form of ai program that simulated the knowledge and analytical skills of human experts. by 1985, the market for ai had reached over a billion dollars. at the same time, japan ' s fifth generation computer project inspired the u. s. and british governments to restore funding for academic research. however, beginning with the collapse of the lisp machine market in 1987, ai once again fell into disrepute, and a second, longer - lasting winter began. up to this point, most of ai ' s funding had gone to projects that used high - level symbols to represent mental objects like plans, goals, beliefs, and known facts. in the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition, and began to look"
q_048,dense,0.0,0.0,0.0,0.0,"/ index. php? title = lo _ carmen & oldid = 1335910933 "" ##gento.","346238. pmid 6953413. ^ hopfield, j. j. ( 1984 ). "" neurons with graded response have collective computational properties like those of two - state neurons "". proceedings of the national academy of sciences. 81 ( 10 ) : 3088 – 3092. bibcode : 1984pnas... 81. 3088h. doi : 10. 1073 / pnas. 81. 10. 3088. pmc 345226. pmid 6587342. ^ engel, a. ; broeck, c. van den ( 2001 ). statistical mechanics of learning. cambridge, uk ; new york, ny : cambridge university press. isbn 978 - 0 - 521 - 77307 - 2. ^ seung, h. s. ; sompolinsky, h. ; tishby, n. ( 1992 - 04 - 01 ). "" statistical mechanics of learning from examples "". physical review a. 45 ( 8 ) : 6056 – 6091. bibcode : 1992phrva.. 45. 6056s. doi : 10. 1103 / physreva. 45. 6056. pmid 9907706. ^ zhang, aston ; lipton, zachary ; li, mu ; smola, alexander j. ( 2024 ). "" 10. modern recurrent neural networks "". dive into deep learning. cambridge new york port melbourne new delhi singapore : cambridge"
q_049,dense,0.0,0.0,0.0,0.05102040816326531,"  cite book   : isbn / date incompatibility ( help )  eckhart, meister ( 1994 ). "" on detachment and possessing god "". selected writings. oliver davies. london : penguin books. p. 9. isbn 0 - 14 - 043343 - 0. oclc 31240752.","from the original on 26 march 2023. retrieved 15 january 2023. ^ varshney, neeraj ; yao, wenlin ; zhang, hongming ; chen, jianshu ; yu, dong ( 2023 ). "" a stitch in time saves nine : detecting and mitigating hallucinations of llms by validating low - confidence generation "". arxiv : 2307. 03987 [ cs. cl ]. ^ lin, belle ( 2025 - 02 - 05 ). "" why amazon is betting on ' automated reasoning ' to reduce ai ' s hallucinations : the tech giant says an obscure field that combines ai and math can mitigate — but not completely eliminate — ai ' s propensity to provide wrong answers "". wall street journal. issn 0099 - 9660. ^ lakoff, george ( 1999 ). philosophy in the flesh : the embodied mind and its challenge to western philosophy ; appendix : the neural theory of language paradigm. new york basic books. pp. 569 – 583. isbn 978 - 0 - 465 - 05674 - 3. ^ evans, vyvyan. ( 2014 ). the language myth. cambridge university press. isbn 978 - 1 - 107 - 04396 - 1. ^ friston, karl j. ( 2022 ). active inference : the free energy principle in mind, brain, and behavior ; chapter 4 the generative models of active inference."
q_050,dense,0.0,0.0,0.0,0.03488372093023256," a b c d e f g h i "" ohioans win fellowships "". the daily reporter. dover, ohio, usa. 1964 - 03 - 30. p. 2. retrieved 2023 - 07 - 16.  a b c d e f g h i j k l m n","/ pnas. 2215907120. pmc 10068812. pmid 36943882. ^ buolamwini, joy ; gebru, timnit ( 2018 - 01 - 01 ). "" gender shades : intersectional accuracy disparities in commercial gender classification "" ( pdf ). proceedings of machine learning research ( fat * ). retrieved 2025 - 10 - 02. ^ yang, kaiqi ( 2024 - 11 - 01 ). "" unpacking political bias in large language models : a cross - model comparison on u. s. politics "". arxiv : 2412. 16746 [ cs. cy ]. ^ strubell, emma ; ganesh, ananya ; mccallum, andrew ( 2019 - 07 - 28 ). "" energy and policy considerations for deep learning in nlp "" ( pdf ). acl anthology. retrieved 2025 - 10 - 02. ^ he, yuhao ; yang, li ; qian, chunlian ; li, tong ; su, zhengyuan ; zhang, qiang ; hou, xiangqing ( 2023 - 04 - 28 ). "" conversational agent interventions for mental health problems : systematic review and meta - analysis of randomized controlled trials "". journal of medical internet research. 25 e43862. doi : 10. 2196 / 43862. pmc 10182468. pmid 37115595."
q_051,dense,0.0,0.0,0.0,0.024193548387096777," lanci, michelangelo ( 1845 ). paralipomeni alla illustrazione della sagra scrittura ( in italian ) ( facsimile of the first ed. ). dondey - dupre. pp. 100 – 113.","thousands of jobs in banking, financial planning, and pension advice in the process, but i ' m not sure it will unleash a new wave of [ e. g., sophisticated ] pension innovation. "" military main article : military applications of artificial intelligence various countries are deploying ai military applications. the main applications enhance command and control, communications, sensors, integration and interoperability. research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. ai technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed joint fires between networked combat vehicles, both human - operated and autonomous. ai has been used in military operations in iraq, syria, israel and ukraine. generative ai vincent van gogh in watercolour created by generative ai software these paragraphs are an excerpt from generative artificial intelligence. [ edit ] generative artificial intelligence, also known as generative ai or genai, is a subfield of artificial intelligence that uses generative models to generate text, images, videos, audio, software code or other forms of data. these models learn the underlying patterns and structures of their training data and use them to generate new data in response to input, which often takes the form of natural language prompts. the prevalence of generative ai tools has increased significantly since the ai boom in the 2020s. this boom was"
q_052,dense,0.0,0.0,0.0,0.0,first division,"fuzzy logic assigns a "" degree of truth "" between 0 and 1. it can therefore handle propositions that are vague and partially true. non - monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning. other specialized versions of logic have been developed to describe many complex domains. probabilistic methods for uncertain reasoning a simple bayesian network, with the associated conditional probability tables many problems in ai ( including reasoning, planning, learning, perception, and robotics ) require the agent to operate with incomplete or uncertain information. ai researchers have devised a number of tools to solve these problems using methods from probability theory and economics. precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis, and information value theory. these tools include models such as markov decision processes, dynamic decision networks, game theory and mechanism design. bayesian networks are a tool that can be used for reasoning ( using the bayesian inference algorithm ), learning ( using the expectation – maximization algorithm ), planning ( using decision networks ) and perception ( using dynamic bayesian networks ). probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time ( e. g., hidden markov models or kalman filters ). expectation – maximization clustering of old faithful eruption data starts from a random guess but then successfully converge"
q_053,dense,0.0,0.0,0.0,0.0,"chisholm , hugh , ed. ( 1911 ). "" orelli , hans konrad von "". encyclopdia britannica. vol. 20 ( 11th ed. ). cambridge university press. p. 251.",", paul ; chung, hyung won ; sutton, charles ; gehrmann, sebastian ; schuh, parker ; shi, kensen ; tsvyashchenko, sasha ; maynez, joshua ; rao, abhishek ( 2022 - 04 - 01 ). "" palm : scaling language modeling with pathways "". arxiv : 2204. 02311 [ cs. cl ]. ^ ainslie, joshua ; lee - thorp, james ; de jong, michiel ; zemlyanskiy, yury ; lebron, federico ; sanghai, sumit ( 2023 - 12 - 23 ), gqa : training generalized multi - query transformer models from multi - head checkpoints, arxiv : 2305. 13245 ^ a b deepseek - ai ; liu, aixin ; feng, bei ; wang, bin ; wang, bingxuan ; liu, bo ; zhao, chenggang ; dengr, chengqi ; ruan, chong ( 19 june 2024 ), deepseek - v2 : a strong, economical, and efficient mixture - of - experts language model, arxiv : 2405. 04434. ^ a b leviathan, yaniv ; kalman, matan ; matias, yossi ( 2023 - 05 - 18 ), fast inference from transformers via speculative decoding, arxiv : 2211. 17192 ^ fu, yao ( 202"
q_054,dense,0.0,0.0,0.0,0.0,autocoach,"( 1950 ). ^ solomonoff ( 1956 ). ^ unsupervised learning : russell & norvig ( 2021, pp. 653 ) ( definition ), russell & norvig ( 2021, pp. 738 – 740 ) ( cluster analysis ), russell & norvig ( 2021, pp. 846 – 860 ) ( word embedding ) ^ a b supervised learning : russell & norvig ( 2021, § 19. 2 ) ( definition ), russell & norvig ( 2021, chpt. 19 – 20 ) ( techniques ) ^ reinforcement learning : russell & norvig ( 2021, chpt. 22 ), luger & stubblefield ( 2004, pp. 442 – 449 ) ^ transfer learning : russell & norvig ( 2021, pp. 281 ), the economist ( 2016 ) ^ "" artificial intelligence ( ai ) : what is ai and how does it work? | built in "". builtin. com. retrieved 30 october 2023. ^ computational learning theory : russell & norvig ( 2021, pp. 672 – 674 ), jordan & mitchell ( 2015 ) ^ natural language processing ( nlp ) : russell & norvig ( 2021, chpt. 23 – 24 ), poole, mackworth & goebel ( 1998, pp. 91 – 104 ), luger & stubblefield ( 2004, pp. 591 – 632 ) ^ subproblems"
q_055,dense,0.0,0.0,0.0,0.00823045267489712,a century.,"the georgetown experiment in 1954 involved fully automatic translation of more than sixty russian sentences into english. the authors claimed that within three or five years, machine translation would be a solved problem. however, real progress was much slower, and after the alpac report in 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. little further research in machine translation was conducted in america ( though some research continued elsewhere, such as japan and europe ) until the late 1980s when the first statistical machine translation systems were developed. 1960s : some notably successful natural language processing systems developed in the 1960s were shrdlu, a natural language system working in restricted "" blocks worlds "" with restricted vocabularies, and eliza, a simulation of a rogerian psychotherapy, written by joseph weizenbaum between 1964 and 1966. despite using minimal information about human thought or emotion, eliza was able to produce interactions that appeared human - like. when the "" patient "" exceeded the very small knowledge base, eliza might provide a generic response, for example, responding to "" my head hurts "" with "" why do you say your head hurts? "". ross quillian ' s successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in a computer memory at the time. 1970s : during the 1970s, many programmers began to write "" conceptual ontologies "", which structured real - world information into computer - understandable data."
q_056,dense,0.0,0.0,0.0,0.017094017094017096," lanci, michelangelo ( 1845 ). paralipomeni alla illustrazione della sagra scrittura ( in italian ) ( facsimile of the first ed. ). dondey - dupre. pp. 100 – 113.","data. researchers target concrete failure modes, including memorization and copyright leakage, security exploits such as prompt injection, algorithmic bias manifesting as stereotyping, dataset selection effects, and political skew, methods for reducing high energy and carbon costs of large - scale training, and measurable cognitive and mental health impacts of conversational agents on users, while engaging empirical and ethical uncertainty about claims of machine sentience, and applying mitigation measures such as dataset curation, input sanitization, model auditing, scalable oversight, and governance frameworks. cbrn and content misuse ai labs treat cbrn defense ( chemical, biological, radiological, and nuclear defense ) and similar topics as high - consequence misuse attempt to apply various techniques to reduce potential harms. some commenters expressed concern over accidental or deliberate creation of misinformation, or other forms of misuse. for example, the availability of large language models could reduce the skill - level required to commit bioterrorism ; biosecurity researcher kevin esvelt has suggested that llm creators should exclude from their training data papers on creating or enhancing pathogens. content filtering llm applications accessible to the public, like chatgpt or claude, typically incorporate safety measures designed to filter out harmful content. however, implementing these controls effectively has proven challenging. for instance, a 2023 study proposed a method for circumventing llm safety systems. in 2025, the american sunlight"
q_057,dense,0.0,0.0,0.0,0.02247191011235955,What is the author's last name?,"both of which scales as o ( n 2 ) { \ displaystyle o ( n ^ { 2 } ) } where n { \ displaystyle n } is the number of tokens in a sequence. reformer ( 2020 ) reduces the computational load from o ( n 2 ) { \ displaystyle o ( n ^ { 2 } ) } to o ( n ln n ) { \ displaystyle o ( n \ ln n ) } by using locality - sensitive hashing and reversible layers. sparse attention uses attention graphs that grows slower than o ( n 2 ) { \ displaystyle o ( n ^ { 2 } ) }. for example, bigbird ( 2020 ) uses random small - world networks which grows as o ( n ) { \ displaystyle o ( n ) }. ordinary transformers require a memory size that is quadratic in the size of the context window. attention - free transformers reduce this to a linear dependence while still retaining the advantages of a transformer by linking the key to the value. random feature attention random feature attention ( 2021 ) uses fourier random features : φ ( x ) = 1 d [ cos ⟨ w 1, x ⟩, sin ⟨ w 1, x ⟩, [UNK] cos ⟨ w d, x ⟩, sin ⟨ w d, x ⟩ ] t { \ displaystyle \ varphi ( x ) = { \ frac { 1 } { \ sqrt { d } } } [ \ cos \ langle w _"
q_058,dense,0.0,0.0,0.0,0.015037593984962407," lanci, michelangelo ( 1845 ). paralipomeni alla illustrazione della sagra scrittura ( in italian ) ( facsimile of the first ed. ). dondey - dupre. pp. 100 – 113.","the ai boom. generative ai ' s ability to create and modify content has led to several unintended consequences and harms. ethical concerns have been raised about ai ' s long - term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology. goals the general problem of simulating ( or creating ) intelligence has been broken into subproblems. these consist of particular traits or capabilities that researchers expect an intelligent system to display. the traits described below have received the most attention and cover the scope of ai research. reasoning and problem - solving early researchers developed algorithms that imitated step - by - step reasoning that humans use when they solve puzzles or make logical deductions. by the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics. many of these algorithms are insufficient for solving large reasoning problems because they experience a "" combinatorial explosion "" : they become exponentially slower as the problems grow. even humans rarely use the step - by - step deduction that early ai research could model. they solve most of their problems using fast, intuitive judgments. accurate and efficient reasoning is an unsolved problem. knowledge representation an ontology represents knowledge as a set of concepts within a domain and the relationships between those concepts. knowledge representation and knowledge engineering allow ai programs to answer questions intelligently and make deductions about real - world facts. formal knowledge representations are used in content -"
q_059,dense,0.0,0.0,0.0,0.023529411764705882,"##anto - weltbundes. skonpres, bydgoszcz, 2006. 459 pages. isbn 978 - 83 - 89962 - 03 - 4. retrieved from "" https : / / en. wikipedia. org / w / index. php? title = soul _ flight & oldid = 13076812  zarat","##er & schmidhuber ( 2012 ). ^ russell & norvig ( 2021 ), p. 750. ^ a b c russell & norvig ( 2021 ), p. 17. ^ a b c d e f g russell & norvig ( 2021 ), p. 785. ^ a b schmidhuber ( 2022 ), sect. 5. ^ schmidhuber ( 2022 ), sect. 6. ^ a b c schmidhuber ( 2022 ), sect. 7. ^ schmidhuber ( 2022 ), sect. 8. ^ quoted in christian ( 2020, p. 22 ) ^ metz, cade ; weise, karen ( 5 may 2025 ). "" a. i. hallucinations are getting worse, even as new systems become more powerful "". the new york times. issn 0362 - 4331. retrieved 6 may 2025. ^ smith ( 2023 ). ^ "" explained : generative ai "". mit news | massachusetts institute of technology. 9 november 2023. ^ "" ai writing and content creation tools "". mit sloan teaching & learning technologies. archived from the original on 25 december 2023. retrieved 25 december 2023. ^ marmouyet ( 2023 ). ^ kobielus ( 2019 ). ^ thomason, james ( 21 may 2024 ). "" mojo rising : the resurgence of ai - first programming languages "". venturebeat."
q_060,dense,0.0,0.0,0.0,0.0136986301369863,dr. seuss and mr. geisel. random house. 1995. 178 - 9.,"##elbach, sven ( 2022 ). "" pre - trained language models "". foundation models for natural language processing. artificial intelligence : foundations, theory, and algorithms. pp. 19 – 78. doi : 10. 1007 / 978 - 3 - 031 - 23190 - 2 _ 2. isbn 978 - 3 - 031 - 23190 - 2. ^ dodge, jesse ; sap, maarten ; marasovic, ana ; agnew, william ; ilharco, gabriel ; groeneveld, dirk ; mitchell, margaret ; gardner, matt ( 2021 ). "" documenting large webtext corpora : a case study on the colossal clean crawled corpus "" ( pdf ). emnlp. arxiv : 2104. 08758. doi : 10. 1145 / 3571730. ^ lee, katherine ; ippolito, daphne ; nystrom, andrew ; zhang, chiyuan ; eck, douglas ; callison - burch, chris ; carlini, nicholas ( may 2022 ). "" deduplicating training data makes language models better "" ( pdf ). proceedings of the 60th annual meeting of the association for computational linguistics ( volume 1 : long papers ). pp. 8424 – 8445. doi : 10. 18653 / v1 / 2022. acl - long. 577. ^ li, yuanzhi ; bubeck, sebastien ; eldan, ronen ; del giorno"
q_061,dense,0.0,0.0,0.0,0.010471204188481674,"multiple names : authors list ( link ) volume 6 kellogg, paul u., editor ( 1914 ). wage - earning pittsburgh. new york : russell sage foundation publications. isbn none.","norvig : "" stong ai – the assertion that machines that do so are actually thinking ( as opposed to simulating thinking ). "" references ^ a b c russell & norvig ( 2021 ), pp. 1 – 4. ^ ai set to exceed human brain power archived 19 february 2008 at the wayback machine cnn. com ( 26 july 2006 ) ^ kaplan, andreas ; haenlein, michael ( 2019 ). "" siri, siri, in my hand : who ' s the fairest in the land? on the interpretations, illustrations, and implications of artificial intelligence "". business horizons. 62 : 15 – 25. doi : 10. 1016 / j. bushor. 2018. 08. 004. [ the question of the source is a pastiche of : snow white ] ^ russell & norvig ( 2021, § 1. 2 ). ^ "" tech companies want to build artificial general intelligence. but who decides when agi is attained? "". ap news. 4 april 2024. retrieved 20 may 2025. ^ a b dartmouth workshop : russell & norvig ( 2021, p. 18 ), mccorduck ( 2004, pp. 111 – 136 ), nrc ( 1999, pp. 200 – 201 ) the proposal : mccarthy et al. ( 1955 ) ^ a b successful programs of the 1960s : mccorduck ( 2004, pp. 243 – 252 ), crevier ( 1993, pp. 52 –"
q_062,dense,0.0,0.0,0.0,0.0,ingrid haas ' research,"ai companies. philosophy main article : philosophy of artificial intelligence philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. another major focus has been whether machines can be conscious, and the associated ethical implications. many other topics in philosophy are relevant to ai, such as epistemology and free will. for example, debates center on whether machines can genuinely understand meaning, whether they possess autonomous decision - making capabilities, and to what extent their actions can be considered intentional rather than merely the result of algorithmic processes. rapid advancements have intensified public discussions on the philosophy and ethics of ai. defining artificial intelligence see also : synthetic intelligence, intelligent agent, artificial mind, virtual intelligence, and dartmouth workshop alan turing wrote in 1950 "" i propose to consider the question ' can machines think '? "" he advised changing the question from whether a machine "" thinks "", to "" whether or not it is possible for machinery to show intelligent behaviour "". he devised the turing test, which measures the ability of a machine to simulate human conversation. since we can only observe the behavior of the machine, it does not matter if it is "" actually "" thinking or literally has a "" mind "". turing notes that we can not determine these things about other people but "" it is usual to have a polite convention that everyone thinks. "" the turing test can provide some evidence of intelligence, but it penalizes non - human intelligent behavior. russell and norvig agree with turing that intelligence must be defined in"
q_063,dense,0.0,0.0,0.0,0.012903225806451613,"  cite book   : cs1 maint : location missing publisher ( link )  febvre, lucien ( 2010 ). the coming of the book : the impact of printing , 1450 – 1800. henri - jean martin , geoffrey nowell - smith , david wootton. london. is","11 - 17. retrieved 2023 - 03 - 14. ^ fathallah, nadeen ; das, arunav ; de giorgis, stefano ; poltronieri, andrea ; haase, peter ; kovriguina, liubov ( 2024 - 05 - 26 ). neon - gpt : a large language model - powered pipeline for ontology learning ( pdf ). extended semantic web conference 2024. hersonissos, greece. ^ manning, christopher d. ( 2022 ). "" human language understanding & reasoning "". daedalus. 151 ( 2 ) : 127 – 138. doi : 10. 1162 / daed _ a _ 01905. s2cid 248377870. archived from the original on 2023 - 11 - 17. retrieved 2023 - 03 - 09. ^ kaplan, jared ; mccandlish, sam ; henighan, tom ; brown, tom b. ; chess, benjamin ; child, rewon ; gray, scott ; radford, alec ; wu, jeffrey ; amodei, dario ( 2020 ). "" scaling laws for neural language models "". arxiv : 2001. 08361 [ cs. lg ]. ^ vaswani, ashish ; shazeer, noam ; parmar, niki ; uszkoreit, jakob ; jones, llion ; gomez, aidan n ; kaiser, łukasz ; polosukhin"
q_064,dense,0.0,0.0,0.0,0.0234375," lanci, michelangelo ( 1845 ). paralipomeni alla illustrazione della sagra scrittura ( in italian ) ( facsimile of the first ed. ). dondey - dupre. pp. 100 – 113.","for artificial intelligence and cryptocurrency. the report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole japanese nation. prodigious power consumption by ai is responsible for the growth of fossil fuel use, and might delay closings of obsolete, carbon - emitting coal energy facilities. there is a feverish rise in the construction of data centers throughout the us, making large technology firms ( e. g., microsoft, meta, google, amazon ) into voracious consumers of electric power. projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. a chatgpt search involves the use of 10 times the electrical energy as a google search. the large firms are in haste to find power sources – from nuclear energy to geothermal to fusion. the tech firms argue that – in the long view – ai will be eventually kinder to the environment, but they need the energy now. ai makes the power grid more efficient and "" intelligent "", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms. a 2024 goldman sachs research paper, ai data centers and the coming us power demand surge, found "" us power demand ( is ) likely to experience growth not seen in a generation.... "" and forecasts that, by 2030, us data centers will consume 8 % of us power, as opposed to"
q_065,dense,0.0,0.0,0.0,0.0,pits,"} } \ right ) v \ end { aligned } } } the following matrix is commonly used in decoder self - attention modules, called "" causal masking "" : m causal = [ 0 − ∞ − ∞ … − ∞ 0 0 − ∞ … − ∞ 0 0 0 … − ∞ [UNK] [UNK] [UNK] [UNK] [UNK] 0 0 0 … 0 ] { \ displaystyle m _ { \ text { causal } } = { \ begin { bmatrix } 0 & - \ infty & - \ infty & \ dots & - \ infty \ \ 0 & 0 & - \ infty & \ dots & - \ infty \ \ 0 & 0 & 0 & \ dots & - \ infty \ \ \ vdots & \ vdots & \ vdots & \ ddots & \ vdots \ \ 0 & 0 & 0 & \ dots & 0 \ end { bmatrix } } } in words, it means that each token can pay attention to itself, and every token before it, but not any after it. a non - masked attention module can be thought of as a masked attention module where the mask has all entries zero. as an example of an uncommon use of mask matrix, the xlnet considers all masks of the form p m causal p − 1 { \ displaystyle pm _ { \ text { causal } } p ^ { - 1 } }, where p { \ displaystyle p }"
q_066,dense,0.0,0.0,0.0,0.025974025974025976,"  cite book   : isbn / date incompatibility ( help )  eckhart, meister ( 1994 ). "" on detachment and possessing god "". selected writings. oliver davies. london : penguin books. p. 9. isbn 0 - 14 - 043343 - 0. oclc 31240752.","' s neuroscience to the birth of cybernetics "". the neuroscientist. 31 ( 1 ) : 14 – 30. doi : 10. 1177 / 10738584231179932. hdl : 10261 / 348372. issn 1073 - 8584. pmid 37403768. ^ ramon y cajal, santiago ( 1909 ). histologie du systeme nerveux de l ' homme & des vertebres. vol. ii. foyle special collections library king ' s college london. paris : a. maloine. p. 149. ^ de no, r. lorente ( 1933 - 08 - 01 ). "" vestibulo - ocular reflex arc "". archives of neurology and psychiatry. 30 ( 2 ) : 245. doi : 10. 1001 / archneurpsyc. 1933. 02240140009001. issn 0096 - 6754. ^ larriva - sahd, jorge a. ( 2014 - 12 - 03 ). "" some predictions of rafael lorente de no 80 years later "". frontiers in neuroanatomy. 8 : 147. doi : 10. 3389 / fnana. 2014. 00147. issn 1662 - 5129. pmc 4253658. pmid 25520630. ^ "" reverberating circuit "". oxford reference. retrieved 2024 - 07 -"
q_067,dense,0.0,0.0,0.0,0.0,israeli language,"). "" robot rights violate human rights, experts warn eu "". euronews. archived from the original on 19 september 2024. retrieved 23 february 2024. ^ the intelligence explosion and technological singularity : russell & norvig ( 2021, pp. 1004 – 1005 ), omohundro ( 2008 ), kurzweil ( 2005 ) i. j. good ' s "" intelligence explosion "" : good ( 1965 ) vernor vinge ' s "" singularity "" : vinge ( 1993 ) ^ russell & norvig ( 2021 ), p. 1005. ^ transhumanism : moravec ( 1988 ), kurzweil ( 2005 ), russell & norvig ( 2021, p. 1005 ) ^ ai as evolution : edward fredkin is quoted in mccorduck ( 2004, p. 401 ), butler ( 1863 ), dyson ( 1998 ) ^ ai in myth : mccorduck ( 2004, pp. 4 – 5 ) ^ mccorduck ( 2004 ), pp. 340 – 400. ^ buttazzo ( 2001 ). ^ anderson ( 2008 ). ^ mccauley ( 2007 ). ^ galvan ( 1997 ). textbooks luger, george ; stubblefield, william ( 2004 ). artificial intelligence : structures and strategies for complex problem solving ( 5th ed. ). benjamin / cummings. isbn 978 - 0 - 8053 - 4780 - 7. archived from the original"
q_068,dense,0.0,0.0,0.0,0.05054151624548737,fcforum official website declaration and how to manual on sostenibility of creativity on the digital era. video : fcforum 2009 http : / / wiki. creativecommons. org / fcforum official website declaration and how to manual on sostenibility of creativity on the digital era.,"much of what people know is not represented as "" facts "" or "" statements "" that they could express verbally ). there is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for ai applications. planning and decision - making an "" agent "" is anything that perceives and takes actions in the world. a rational agent has goals or preferences and takes actions to make them happen. in automated planning, the agent has a specific goal. in automated decision - making, the agent has preferences — there are some situations it would prefer to be in, and some situations it is trying to avoid. the decision - making agent assigns a number to each situation ( called the "" utility "" ) that measures how much the agent prefers it. for each possible action, it can calculate the "" expected utility "" : the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. it can then choose the action with the maximum expected utility. in classical planning, the agent knows exactly what the effect of any action will be. in most real - world problems, however, the agent may not be certain about the situation they are in ( it is "" unknown "" or "" unobservable "" ) and it may not know for certain what will happen after each possible action ( it is not "" deterministic "" ). it must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked. in some problems"
q_069,dense,0.0,0.0,0.0,0.023076923076923075," lanci, michelangelo ( 1845 ). paralipomeni alla illustrazione della sagra scrittura ( in italian ) ( facsimile of the first ed. ). dondey - dupre. pp. 100 – 113.","such as llama 2, mistral or stable diffusion, have been made open - weight, meaning that their architecture and trained parameters ( the "" weights "" ) are publicly available. open - weight models can be freely fine - tuned, which allows companies to specialize them with their own data and for their own use - case. open - weight models are useful for research and innovation but can also be misused. since they can be fine - tuned, any built - in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. some researchers warn that future ai models may develop dangerous capabilities ( such as the potential to drastically facilitate bioterrorism ) and that once released on the internet, they cannot be deleted everywhere if needed. they recommend pre - release audits and cost - benefit analyses. frameworks artificial intelligence projects can be guided by ethical considerations during the design, development, and implementation of an ai system. an ai framework such as the care and act framework, developed by the alan turing institute and based on the sum values, outlines four main ethical dimensions, defined as follows : respect the dignity of individual people connect with other people sincerely, openly, and inclusively care for the wellbeing of everyone protect social values, justice, and the public interest other developments in ethical frameworks include those decided upon during the asilomar conference, the montreal declaration for responsible ai, and the ieee ' s ethics of autonomous systems initiative, among others ; however"
q_070,dense,0.0,0.0,0.0,0.07766990291262135,"retrieved 2015 - 11 - 01.  "" alibaba launches global research program for cutting - edge technology development "". www. businesscloudnews. com. retrieved 2015 - 11 - 01.  "" alibaba launches global research program for cutting - edge technology development "". www. businesscloudnews. com. retrieved 2015 - 11 - 01.  "" alibaba launches global research program for",". reuters. 2025 - 06 - 25. retrieved 2025 - 06 - 26. ^ "" meta scores victory in ai copyright case "". wired. 2025 - 06 - 25. retrieved 2025 - 06 - 26. ^ "" openai defeats news outlets ' copyright lawsuit over ai training for now "". reuters. 2024 - 11 - 07. retrieved 2024 - 11 - 08. ^ "" openai erases evidence in training data lawsuit "". the verge. 2024 - 11 - 21. retrieved 2024 - 11 - 22. ^ peng, zhencan ; wang, zhizhi ; deng, dong ( 13 june 2023 ). "" near - duplicate sequence search at scale for large language model memorization evaluation "" ( pdf ). proceedings of the acm on management of data. 1 ( 2 ) : 1 – 18. doi : 10. 1145 / 3589324. s2cid 259213212. archived ( pdf ) from the original on 2024 - 08 - 27. retrieved 2024 - 01 - 20. citing lee et al 2022. ^ peng, wang & deng 2023, p. 8. ^ stephen council ( 1 dec 2023 ). "" how googlers cracked an sf rival ' s tech model with a single word "". sfgate. archived from the original on 16 december 2023. ^ "" prepare for truly useful large language models "". nature biomedical engineering. 7 ( 2 ) : 85"
q_071,dense,0.0,0.0,0.0,0.0,argentina,"y } } _ { 2 }, \ dots, { \ hat { y } } _ { l } ) }. the problem is that if the model makes a mistake early on, say at y ^ 2 { \ displaystyle { \ hat { y } } _ { 2 } }, then subsequent tokens are likely to also be mistakes. this makes it inefficient for the model to obtain a learning signal, since the model would mostly learn to shift y ^ 2 { \ displaystyle { \ hat { y } } _ { 2 } } towards y 2 { \ displaystyle y _ { 2 } }, but not the others. teacher forcing makes it so that the decoder uses the correct output sequence for generating the next entry in the sequence. so for example, it would see ( y 1, …, y k ) { \ displaystyle ( y _ { 1 }, \ dots, y _ { k } ) } in order to generate y ^ k + 1 { \ displaystyle { \ hat { y } } _ { k + 1 } }. gradient descent main articles : gradient descent and vanishing gradient problem gradient descent is a first - order iterative optimization algorithm for finding the minimum of a function. in neural networks, it can be used to minimize the error term by changing each weight in proportion to the derivative of the error with respect to that weight, provided the non - linear activation functions are differentiable. the standard method for training rn"
q_072,dense,0.0,0.0,0.0,0.017316017316017313,What is the author's last name?,"agreeability observed across multi - turn interactions and productized assistants. continued sycophancy has led to the observation of getting "" 1 - shotted "", denoting instances where conversational interaction with a large language model produces a lasting change in a user ' s beliefs or decisions, similar to the negative effects of psychedelics, and controlled experiments show that short llm dialogues can generate measurable opinion and confidence shifts comparable to human interlocutors. empirical analyses attribute part of the effect to human preference signals and preference models that reward convincingly written agreeable responses, and subsequent work has extended evaluation to multi - turn benchmarks and proposed interventions such as synthetic - data finetuning, adversarial evaluation, targeted preference - model reweighting, and multi - turn sycophancy benchmarks to measure persistence and regression risk. industry responses have combined research interventions with product controls, for example google and other labs publishing synthetic - data and fine - tuning interventions and openai rolling back an overly agreeable gpt - 4o update while publicly describing changes to feedback collection, personalization controls, and evaluation procedures to reduce regression risk and improve long - term alignment with user - level safety objectives. mainstream culture has reflected anxieties about this dynamic where south park satirized overreliance on chatgpt and the tendency of assistants to flatter user beliefs in season 27 episode "" sickofancy "", and continued the themes across the following season, which commentators interpreted as a critique of"
q_073,dense,0.0,0.0,0.0,0.08270676691729323,"braingate gets a new lease on life, the boston globe, august, 2009  "" blackrock microsystems obtains expanded 510 ( k ) to market neuroport system "". prweb. 15 july 2009. archived from the original on july 20, 2009.  a b "" braingate - home "". braingate2. org.  "" braingate2 : feasibility study of an intracortical neural interface","hp labs describes a system of cortical computing with memristive nanodevices. the memristors ( memory resistors ) are implemented by thin film materials in which the resistance is electrically tuned via the transport of ions or oxygen vacancies within the film. darpa ' s synapse project has funded ibm research and hp labs, in collaboration with the boston university department of cognitive and neural systems ( cns ), to develop neuromorphic architectures that may be based on memristive systems. memristive networks are a particular type of physical neural network that have very similar properties to ( little - ) hopfield networks, as they have continuous dynamics, a limited memory capacity and natural relaxation via the minimization of a function which is asymptotic to the ising model. in this sense, the dynamics of a memristive circuit have the advantage compared to a resistor - capacitor network to have a more interesting non - linear behavior. from this point of view, engineering analog memristive networks account for a peculiar type of neuromorphic engineering in which the device behavior depends on the circuit wiring or topology. the evolution of these networks can be studied analytically using variations of the caravelli - traversa - di ventra equation. continuous - time a continuous - time recurrent neural network ( ctrnn ) uses a system of ordinary differential equations to model the effects on a neuron of the incoming inputs"
q_074,dense,0.0,0.0,0.0,0.03891050583657587,the lab uses theories and methods from political and social psychology,"one of which is their ambiguity. several works use ai to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. this appears in karel capek ' s r. u. r., the films a. i. artificial intelligence and ex machina, as well as the novel do androids dream of electric sheep?, by philip k. dick. dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence. see also artificial consciousness – field in cognitive science artificial intelligence and elections – impact of ai on political elections artificial intelligence content detection – software to detect ai - generated content artificial intelligence in wikimedia projects – use of artificial intelligence to develop wikipedia and other wikimedia projects association for the advancement of artificial intelligence ( aaai ) behavior selection algorithm – algorithm that selects actions for intelligent agents business process automation – automation of business processes case - based reasoning – process of solving new problems based on the solutions of similar past problems computational intelligence – ability of a computer to learn a specific task from data or experimental observation darwin eu – a european union initiative coordinated by the european medicines agency ( ema ) to generate and utilize real world evidence ( rwe ) to support the evaluation and supervision of medicines across the eu digital immortality – hypothetical concept of storing a personality in digital form emergent algorithm – algorithm exhibiting emergent behavior female gendering of ai technologies – gender biases in"
q_075,dense,0.0,0.0,0.0,0.022099447513812154," "" tupelocetus palmeri, whale skull | charleston museum "". www. charlestonmuseum. org. retrieved 2024 - 04 - 13.  antar, mohammed s. ; gohar, abdullah s. ; el - desouky, heba ; seiffert, erik r. ; ",", johannes ; horvitz, eric ; kamar, ece ; lee, peter ; lee, yin tat ; li, yuanzhi ; lundberg, scott ; nori, harsha ; palangi, hamid ; ribeiro, marco tulio ; zhang, yi ( 2023 ). "" machine culture "". nature human behaviour. 7 ( 11 ) : 1855 – 1868. arxiv : 2303. 12712. doi : 10. 1038 / s41562 - 023 - 01742 - 2. pmid 37985914. ^ "" anthropic ceo dario amodei pens a smart look at our ai future "". fast company. october 17, 2024. ^ "" chatgpt is more like an ' alien intelligence ' than a human brain, says futurist "". zdnet. 2023. archived from the original on 12 june 2023. retrieved 12 june 2023. ^ a b newport, cal ( 13 april 2023 ). "" what kind of mind does chatgpt have? "". the new yorker. archived from the original on 12 june 2023. retrieved 12 june 2023. ^ roose, kevin ( 30 may 2023 ). "" why an octopus - like creature has come to symbolize the state of a. i. "" the new york times. archived from the original on 30 may 2023. retrieved 12 june 2023. ^ "" the a to"
q_076,dense,0.0,0.0,0.0,0.0234375," lanci, michelangelo ( 1845 ). paralipomeni alla illustrazione della sagra scrittura ( in italian ) ( facsimile of the first ed. ). dondey - dupre. pp. 100 – 113.","math benchmark problems. alternatively, dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such as alphatensor, alphageometry, alphaproof and alphaevolve all from google deepmind, llemma from eleutherai or julius. when natural language is used to describe mathematical problems, converters can transform such prompts into a formal language such as lean to define mathematical tasks. the experimental model gemini deep think accepts natural language prompts directly and achieved gold medal results in the international math olympiad of 2025. some models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics. topological deep learning integrates various topological approaches. finance finance is one of the fastest growing sectors where applied ai tools are being deployed : from retail online banking to investment advice and insurance, where automated "" robot advisers "" have been in use for some years. according to nicolas firzli, director of the world pensions & investments forum, it may be too early to see the emergence of highly innovative ai - informed financial products and services. he argues that "" the deployment of ai tools will simply further automatise things : destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but i ' m not sure it will unleash a new wave of [ e. g., sophisticated ] pension innovation. "" military main article : military applications of"
q_077,dense,0.0,0.0,0.0,0.05581395348837209,"2022 - 11 - 15.  "" braingate - home "". braingate2. org.  "" braingate2 : feasibility study of an intracortical neural interface system for persons with tetraplegia - full text view "". clinicaltrials. gov. external links blackrock microsystems official website cyberkinetics neurotechnology official website donoghue lab hatsopoulos lab braingate","- video, language translation, more "". venturebeat. 2022 - 11 - 02. retrieved 2022 - 11 - 09. ^ vincent, james ( 2022 - 09 - 29 ). "" meta ' s new text - to - video ai generator is like dall - e for video "". the verge. retrieved 2022 - 11 - 09. ^ "" previous shared tasks | conll "". www. conll. org. retrieved 2021 - 01 - 11. ^ "" cognition "". lexico. oxford university press and dictionary. com. archived from the original on july 15, 2020. retrieved 6 may 2020. ^ "" ask the cognitive scientist "". american federation of teachers. 8 august 2014. cognitive science is an interdisciplinary field of researchers from linguistics, psychology, neuroscience, philosophy, computer science, and anthropology that seek to understand the mind. ^ robinson, peter ( 2008 ). handbook of cognitive linguistics and second language acquisition. routledge. pp. 3 – 8. isbn 978 - 0 - 805 - 85352 - 0. ^ lakoff, george ( 1999 ). philosophy in the flesh : the embodied mind and its challenge to western philosophy ; appendix : the neural theory of language paradigm. new york basic books. pp. 569 – 583. isbn 978 - 0 - 465 - 05674 - 3. ^ strauss, claudia ( 1999 ). a cognitive theory of cultural meaning. cambridge university press. pp. 156 – 164. isbn"
q_078,dense,0.0,0.0,0.0,0.025423728813559317," lanci, michelangelo ( 1845 ). paralipomeni alla illustrazione della sagra scrittura ( in italian ) ( facsimile of the first ed. ). dondey - dupre. pp. 100 – 113.","main article : prompt engineering in 2020, openai researchers demonstrated that their new model gpt - 3 could understand what format to use given a few rounds of q and a ( or other type of task ) in the input data as example, thanks in part due to the rlhf technique. this technique, called few - shot prompting, allows llms to be adapted to any task without requiring fine - tuning. also in 2022, it was found that the base gpt - 3 model can generate an instruction based on user input. the generated instruction along with user input is then used as input to another instance of the model under a "" instruction : [... ], input : [... ], output : "" format. the other instance is able to complete the output and often produces the correct answer in doing so. the ability to "" self - instruct "" makes llms able to bootstrap themselves toward a correct answer. dialogue processing ( chatbot ) an llm can be turned into a chatbot by specializing it for conversation. user input is prefixed with a marker such as "" q : "" or "" user : "" and the llm is asked to predict the output after a fixed "" a : "" or "" assistant : "". this type of model became commercially available in 2022 with chatgpt, a sibling model of instructgpt fine - tuned to accept and produce dialog - formatted text based on gpt - 3."
q_079,dense,0.0,0.0,0.0,0.02666666666666667,the mbta and the cambridge arts council,"analysis, coreference ; see natural language understanding below ). semantic role labelling ( see also implicit semantic role labelling below ) given a single sentence, identify and disambiguate semantic predicates ( e. g., verbal frames ), then identify and classify the frame elements ( semantic roles ). discourse ( semantics beyond individual sentences ) coreference resolution given a sentence or larger chunk of text, determine which words ( "" mentions "" ) refer to the same objects ( "" entities "" ). anaphora resolution is a specific example of this task, and is specifically concerned with matching up pronouns with the nouns or names to which they refer. the more general task of coreference resolution also includes identifying so - called "" bridging relationships "" involving referring expressions. for example, in a sentence such as "" he entered john ' s house through the front door "", "" the front door "" is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of john ' s house ( rather than of some other structure that might also be referred to ). discourse analysis this rubric includes several related tasks. one task is discourse parsing, i. e., identifying the discourse structure of a connected text, i. e. the nature of the discourse relationships between sentences ( e. g. elaboration, explanation, contrast ). another possible task is recognizing and classifying the speech acts in a chunk of"
q_080,dense,0.0,0.0,0.0,0.04385964912280702,mobius function formula for the generating function for a class,"learning for the recognition of sequences can also be implemented by a more biological - based model which uses the silencing mechanism exhibited in neurons with a relatively high frequency spiking activity. additional stored states and the storage under direct control by the network can be added to both infinite - impulse and finite - impulse networks. another network or graph can also replace the storage if that incorporates time delays or has feedback loops. such controlled states are referred to as gated states or gated memory and are part of long short - term memory networks ( lstms ) and gated recurrent units. this is also called feedback neural network ( fnn ). libraries modern libraries provide runtime - optimized implementations of the above functionality or allow to speed up the slow loop by just - in - time compilation. apache singa caffe : created by the berkeley vision and learning center ( bvlc ). it supports both cpu and gpu. developed in c + +, and has python and matlab wrappers. chainer : fully in python, production support for cpu, gpu, distributed training. deeplearning4j : deep learning in java and scala on multi - gpu - enabled spark. flux : includes interfaces for rnns, including grus and lstms, written in julia. keras : high - level api, providing a wrapper to many other deep learning libraries. microsoft cognitive toolkit mxnet : an open - source deep learning framework used to train"
q_081,dense,0.0,0.0,0.0,0.029850746268656716," eriksson , henrik ; eriksson , kimmo ; linusson , svante ; wastlund , johan ( 2007 ), "" dense packing of patterns in a permutation "", annals of combinatorics, 11 ( 3 – 4 ) : 459 – 470 , doi : 10. 1007 ","##out heuristic. when a programmatic world model is not available, an llm can also be prompted with a description of the environment to act as world model. for open - ended exploration, an llm can be used to score observations for their "" interestingness "", which can be used as a reward signal to guide a normal ( non - llm ) reinforcement learning agent. alternatively, it can propose increasingly difficult tasks for curriculum learning. instead of outputting individual actions, an llm planner can also construct "" skills "", or functions for complex action sequences. the skills can be stored and later invoked, allowing increasing levels of abstraction in planning. multiple agents with memory can interact socially. reasoning llms are conventionally trained to generate an output without generating intermediate steps. as a result, their performance tends to be subpar on complex questions requiring ( at least in humans ) intermediate steps of thought. early research demonstrated that inserting intermediate "" scratchpad "" computations could improve performance on such tasks. later methods overcame this deficiency more systematically by breaking tasks into smaller steps for the llm, either manually or automatically. chaining prompt chaining was introduced in 2022. in this method, a user manually breaks a complex problem down into several steps. in each step, the llm receives as input a prompt telling it what to do and some results from preceding steps. the result from one step is then reused in a next step, until a final answer is reached. the ability of an"
q_082,dense,0.0,0.0,0.0,0.0,chinaman should spoil a detective story,"2102. 12092 ^ yu, jiahui ; xu, yuanzhong ; koh, jing yu ; luong, thang ; baid, gunjan ; wang, zirui ; vasudevan, vijay ; ku, alexander ; yang, yinfei ( 2022 - 06 - 21 ), scaling autoregressive models for content - rich text - to - image generation, arxiv : 2206. 10789 ^ kariampuzha, william ; alyea, gioconda ; qu, sue ; sanjak, jaleal ; mathe, ewy ; sid, eric ; chatelaine, haley ; yadaw, arjun ; xu, yanji ; zhu, qian ( 2023 ). "" precision information extraction for rare disease epidemiology at scale "". journal of translational medicine. 21 ( 1 ) : 157. doi : 10. 1186 / s12967 - 023 - 04011 - y. pmc 9972634. pmid 36855134. further reading alexander rush, the annotated transformer archived 2021 - 09 - 22 at the wayback machine, harvard nlp group, 3 april 2018 phuong, mary ; hutter, marcus ( 2022 ). "" formal algorithms for transformers "". arxiv : 2207. 09238 [ cs. lg ]. ferrando, javier ; sarti,"
q_083,dense,0.0,0.0,0.0,0.0,,"s. ). ^ nilsson ( 1983 ), p. 10. ^ haugeland ( 1985 ), pp. 112 – 117. ^ physical symbol system hypothesis : newell & simon ( 1976, p. 116 ) historical significance : mccorduck ( 2004, p. 153 ), russell & norvig ( 2021, p. 19 ) ^ moravec ' s paradox : moravec ( 1988, pp. 15 – 16 ), minsky ( 1986, p. 29 ), pinker ( 2007, pp. 190 – 191 ) ^ dreyfus ' critique of ai : dreyfus ( 1972 ), dreyfus & dreyfus ( 1986 ) historical significance and philosophical implications : crevier ( 1993, pp. 120 – 132 ), mccorduck ( 2004, pp. 211 – 239 ), russell & norvig ( 2021, pp. 981 – 982 ), fearn ( 2007, chpt. 3 ) ^ crevier ( 1993 ), p. 125. ^ langley ( 2011 ). ^ katz ( 2012 ). ^ neats vs. scruffies, the historic debate : mccorduck ( 2004, pp. 421 – 424, 486 – 489 ), crevier ( 1993, p. 168 ), nilsson ( 1983, pp. 10 – 11 ), russell & norvig ( 2021, p. 24 ) a classic example of the "" scruffy "" approach to"
q_084,dense,0.0,0.0,0.0,0.0,"ogaki , masao ; osaka university ; osaka university ; osaka university ; osaka university ; osaka university ; osaka university ; osaka university ; osaka university ; osaka university ; osaka university ; osaka university ; osaka university ; osaka university ;","cost of training the model, in flops. n { \ displaystyle n } is the number of parameters in the model. d { \ displaystyle d } is the number of tokens in the training set. l { \ displaystyle l } is the average negative log - likelihood loss per token ( nats / token ), achieved by the trained llm on the test dataset. and the statistical hyper - parameters are c 0 = 6 { \ displaystyle c _ { 0 } = 6 }, meaning that it costs 6 flops per parameter to train on one token. note that training cost is much higher than inference cost, where it costs 1 to 2 flops per parameter to infer on one token. α = 0. 34, β = 0. 28, a = 406. 4, b = 410. 7, l 0 = 1. 69 { \ displaystyle \ alpha = 0. 34, \ beta = 0. 28, a = 406. 4, b = 410. 7, l _ { 0 } = 1. 69 } emergent abilities at point ( s ) referred to as breaks, the lines change their slopes, appearing on a linear - log plot as a series of linear segments connected by arcs. performance of bigger models on various tasks, when plotted on a log - log scale, appears as a linear extrapolation of performance achieved by smaller models. however, this linearity may be punctuated by "" break ( s ) """
q_085,dense,0.0,0.0,0.0,0.012738853503184714,"- anzeige : heute starb der Bildhauer julius bayerle, ritter des konigl. kronenordens, im 48. jahr seines lebens. dusseldorf 8 august 1873.",". doi : 10. 1103 / revmodphys. 39. 883. ^ glauber, roy j. ( february 1963 ). "" roy j. glauber "" time - dependent statistics of the ising model "" "". journal of mathematical physics. 4 ( 2 ) : 294 – 307. doi : 10. 1063 / 1. 1703954. retrieved 2021 - 03 - 21. ^ sherrington, david ; kirkpatrick, scott ( 1975 - 12 - 29 ). "" solvable model of a spin - glass "". physical review letters. 35 ( 26 ) : 1792 – 1796. bibcode : 1975phrvl.. 35. 1792s. doi : 10. 1103 / physrevlett. 35. 1792. issn 0031 - 9007. ^ hopfield, j. j. ( 1982 ). "" neural networks and physical systems with emergent collective computational abilities "". proceedings of the national academy of sciences. 79 ( 8 ) : 2554 – 2558. bibcode : 1982pnas... 79. 2554h. doi : 10. 1073 / pnas. 79. 8. 2554. pmc 346238. pmid 6953413. ^ hopfield, j. j. ( 1984 ). "" neurons with graded response have collective computational properties like those of two - state neurons "". proceedings of the national academy of sciences. 81"
q_086,dense,0.0,0.0,0.0,0.02777777777777778,What is the author's last name?,"of after ) multihead attention and feedforward layers stabilizes training, not requiring learning rate warmup. this is the "" pre - ln transformer "" and is more commonly used, compared to the original "" post - ln transformer "". pretrain - finetune transformers typically are first pretrained by self - supervised learning on a large generic dataset, followed by supervised fine - tuning on a small task - specific dataset. the pretrain dataset is typically an unlabeled large corpus, such as the pile. tasks for pretraining and fine - tuning commonly include : language modeling next - sentence prediction question answering reading comprehension sentiment analysis paraphrasing the t5 transformer report documents a large number of natural language pretraining tasks. some examples are : restoring or repairing incomplete or corrupted text. for example, the input, "" thank you ~ ~ me to your party ~ ~ week "", might generate the output, "" thank you for inviting me to your party last week "". translation between natural languages ( machine translation ) judging the pragmatic acceptability of natural language. for example, the following sentence might be judged "" not acceptable "", because even though it is syntactically well - formed, it is improbable in ordinary human usage : the course is jumping well. note that while each of these tasks is trivial or obvious for human native speakers of the language ( or languages ), they have typically proved challenging for"
q_087,dense,0.0,0.0,0.0,0.0,cyberkinetics inc.,"280. doi : 10. 1016 / j. techfore. 2016. 08. 019. "" from not working to neural networking "". the economist. 2016. archived from the original on 31 december 2016. retrieved 26 april 2018. galvan, jill ( 1 january 1997 ). "" entering the posthuman collective in philip k. dick ' s "" do androids dream of electric sheep? "" "". science fiction studies. 24 ( 3 ) : 413 – 429. doi : 10. 1525 / sfs. 24. 3. 0413. jstor 4240644. geist, edward moore ( 9 august 2015 ). "" is artificial intelligence really an existential threat to humanity? "". bulletin of the atomic scientists. archived from the original on 30 october 2015. retrieved 30 october 2015. gibbs, samuel ( 27 october 2014 ). "" elon musk : artificial intelligence is our biggest existential threat "". the guardian. archived from the original on 30 october 2015. retrieved 30 october 2015. goffrey, andrew ( 2008 ). "" algorithm "". in fuller, matthew ( ed. ). software studies : a lexicon. cambridge, mass. : mit press. pp. 15 – 20. isbn 978 - 1 - 4356 - 4787 - 9. goldman, sharon ( 14 september 2022 ). "" 10 years later, deep learning ' revolution ' rages on, say ai pioneers hinton, lecu"
q_088,dense,0.0,0.0,0.0,0.01360544217687075,"hop on pop "".","##yesha ; nambiar, vaishnavi ( 2024 ). "" role of artificial intelligence in the prevention of online child sexual abuse : a systematic review of literature "". journal of applied security research. 19 ( 4 ) : 586 – 627. doi : 10. 1080 / 19361610. 2024. 2331885. ^ razi, afsaneh ; kim, seunghyun ; alsoubai, ashwaq ; stringhini, gianluca ; solorio, thamar ; de choudhury, munmun ; wisniewski, pamela j. ( 13 october 2021 ). "" a human - centered systematic literature review of the computational approaches for online sexual risk detection "". proceedings of the acm on human - computer interaction. 5 ( cscw2 ) : 1 – 38. doi : 10. 1145 / 3479609. ^ ransbotham, sam ; kiron, david ; gerbert, philipp ; reeves, martin ( 6 september 2017 ). "" reshaping business with artificial intelligence "". mit sloan management review. archived from the original on 13 february 2024. ^ sun, yuran ; zhao, xilei ; lovreglio, ruggiero ; kuligowski, erica ( 2024 ). "" ai for large - scale evacuation modeling : promises and challenges "". interpretable machine learning for the analysis, design, assessment, and informed decision making"
q_089,dense,0.0,0.0,0.0,0.008403361344537816,##8769 cmi8786 oxygen hd cmi8787 sound processor on a pci sound card.,"quickly became "" ubiquitous "". though the original transformer has both encoder and decoder blocks, bert is an encoder - only model. academic and research usage of bert began to decline in 2023, following rapid improvements in the abilities of decoder - only models ( such as gpt ) to solve tasks via prompting. although decoder - only gpt - 1 was introduced in 2018, it was gpt - 2 in 2019 that caught widespread attention because openai claimed to have initially deemed it too powerful to release publicly, out of fear of malicious use. gpt - 3 in 2020 went a step further and as of 2025 is available only via api with no offering of downloading the model to execute locally. but it was the 2022 consumer - facing chatbot chatgpt that received extensive media coverage and public attention. the 2023 gpt - 4 was praised for its increased accuracy and as a "" holy grail "" for its multimodal capabilities. openai did not reveal the high - level architecture and the number of parameters of gpt - 4. the release of chatgpt led to an uptick in llm usage across several research subfields of computer science, including robotics, software engineering, and societal impact work. in 2024 openai released the reasoning model openai o1, which generates long chains of thought before returning a final answer. many llms with parameter counts comparable to those of openai ' s gpt series have been"
q_090,dense,0.0,0.0,0.0,0.044198895027624314,"braingate2 : feasibility study of an intracortical neural interface system for persons with tetraplegia - full text view "". clinicaltrials. gov. external links blackrock microsystems official website cyberkinetics neurotechnology official website donoghue lab hatsopoulos lab braingate official website retrieved from "" https : / / en. wikipedia. org / w /","; polosukhin, illia ( 2017 ). "" attention is all you need "". advances in neural information processing systems. 30. curran associates, inc. ^ oord, aaron van den ; kalchbrenner, nal ; kavukcuoglu, koray ( 2016 - 06 - 11 ). "" pixel recurrent neural networks "". proceedings of the 33rd international conference on machine learning. pmlr : 1747 – 1756. ^ a b cruse, holk ; neural networks as cybernetic systems, 2nd and revised edition ^ elman, jeffrey l. ( 1990 ). "" finding structure in time "". cognitive science. 14 ( 2 ) : 179 – 211. doi : 10. 1016 / 0364 - 0213 ( 90 ) 90002 - e. ^ jordan, michael i. ( 1997 - 01 - 01 ). "" serial order : a parallel distributed processing approach "". neural - network models of cognition — biobehavioral foundations. advances in psychology. vol. 121. pp. 471 – 495. doi : 10. 1016 / s0166 - 4115 ( 97 ) 80111 - 2. isbn 978 - 0 - 444 - 81931 - 4. s2cid 15375627. ^ gers, felix a. ; schraudolph, nicol n. ; schmidhuber, jurgen ( 2002 ). "" learning precise timing with lstm rec"
q_091,dense,0.0,0.0,0.0,0.024390243902439022,"  cite book   : isbn / date incompatibility ( help )  eckhart, meister ( 1994 ). "" on detachment and possessing god "". selected writings. oliver davies. london : penguin books. p. 9. isbn 0 - 14 - 043343 - 0. oclc 31240752.","optimal large language models "". neurips : 30016 – 30030. isbn 978 - 1 - 7138 - 7108 - 8. ^ a b caballero, ethan ; gupta, kshitij ; rish, irina ; krueger, david ( 2022 ). "" broken neural scaling laws "". arxiv : 2210. 14891 [ cs. lg ]. ^ a b wei, jason ; tay, yi ; bommasani, rishi ; raffel, colin ; zoph, barret ; borgeaud, sebastian ; yogatama, dani ; bosma, maarten ; zhou, denny ; metzler, donald ; chi, ed h. ; hashimoto, tatsunori ; vinyals, oriol ; liang, percy ; dean, jeff ; fedus, william ( 31 august 2022 ). "" emergent abilities of large language models "". transactions on machine learning research. issn 2835 - 8856. archived from the original on 22 march 2023. retrieved 19 march 2023. ^ "" 137 emergent abilities of large language models "". jason wei. retrieved 2023 - 06 - 24. ^ bowman, samuel r. ( 2024 ). "" eight things to know about large language models "". critical ai. 2 ( 2 ). doi : 10. 1215 / 2834703x - 11556011. ^ hahn, michael ; goyal"
q_092,dense,0.0,0.0,0.0,0.0,"julius bayerle -lrb- 1826 - 1873 -rrb- ,                                     ","). "" sequence to sequence learning with neural networks "" ( pdf ). electronic proceedings of the neural information processing systems conference. 27 : 5346. arxiv : 1409. 3215. bibcode : 2014arxiv1409. 3215s. ^ jozefowicz, rafal ; vinyals, oriol ; schuster, mike ; shazeer, noam ; wu, yonghui ( 2016 - 02 - 07 ). "" exploring the limits of language modeling "". arxiv : 1602. 02410 [ cs. cl ]. ^ gillick, dan ; brunk, cliff ; vinyals, oriol ; subramanya, amarnag ( 2015 - 11 - 30 ). "" multilingual language processing from bytes "". arxiv : 1512. 00103 [ cs. cl ]. ^ vinyals, oriol ; toshev, alexander ; bengio, samy ; erhan, dumitru ( 2014 - 11 - 17 ). "" show and tell : a neural image caption generator "". arxiv : 1411. 4555 [ cs. cv ]. ^ cho, kyunghyun ; van merrienboer, bart ; gulcehre, caglar ; bahdanau, dzmitry ; bougares, fethi ; schwenk, holger ; bengio, yoshua ( 2014 - 06"
q_093,dense,0.0,0.0,0.0,0.0,shaman,": discourse parsing, 2019 : semantic parsing ). increasing interest in multilinguality, and, potentially, multimodality ( english since 1999 ; spanish, dutch since 2002 ; german since 2003 ; bulgarian, danish, japanese, portuguese, slovenian, swedish, turkish since 2006 ; basque, catalan, chinese, greek, hungarian, italian, turkish since 2007 ; czech since 2009 ; arabic since 2012 ; 2017 : 40 + languages ; 2018 : 60 + / 100 + languages ) elimination of symbolic representations ( rule - based over supervised towards weakly supervised methods, representation learning and end - to - end systems ) cognition most higher - level nlp applications involve aspects that emulate intelligent behavior and apparent comprehension of natural language. more broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behavior represents one of the developmental trajectories of nlp ( see trends among conll shared tasks above ). cognition refers to "" the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses. "" cognitive science is the interdisciplinary, scientific study of the mind and its processes. cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. especially during the age of symbolic nlp, the area of computational linguistics maintained strong ties with cognitive studies. as an example, george lakoff offers a methodology to build natural language processing ( nlp ) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, with two defining"
q_094,dense,0.0,0.0,0.0,0.0, http://www.espn.com////////////////////////////////////////////,""". transactions of the association for computational linguistics. 8 : 842 – 866. arxiv : 2002. 12327. doi : 10. 1162 / tacl _ a _ 00349. s2cid 211532403. archived from the original on 2022 - 04 - 03. retrieved 2024 - 01 - 21. ^ a b movva, rajiv ; balachandar, sidhika ; peng, kenny ; agostini, gabriel ; garg, nikhil ; pierson, emma ( 2024 ). "" topics, authors, and institutions in large language model research : trends from 17k arxiv papers "". proceedings of the 2024 conference of the north american chapter of the association for computational linguistics : human language technologies ( volume 1 : long papers ). pp. 1223 – 1243. arxiv : 2307. 10700. doi : 10. 18653 / v1 / 2024. naacl - long. 67. retrieved 2024 - 12 - 08. ^ hern, alex ( 14 february 2019 ). "" new ai fake text generator may be too dangerous to release, say creators "". the guardian. archived from the original on 14 february 2019. retrieved 20 january 2024. ^ "" chatgpt a year on : 3 ways the ai chatbot has completely changed the world in 12 months "". euronews. november 30, 2023. archived from the original"
q_095,dense,0.0,0.0,0.0,0.0,ultrasound pulse,"decoder transformer, then taking just the encoder. they are also referred to as "" all - to - all "" or "" bert - like "". a "" decoder - only "" transformer is not literally decoder - only, since without an encoder, the cross - attention mechanism has nothing to attend to. thus, the decoder layers in a decoder - only transformer is composed of just two sublayers : the causally masked self - attention, and the feedforward network. this is usually used for text generation and instruction following. the models in the gpt series and chinchilla series are decoder - only. they are also referred to as "" autoregressive "" or "" causal "". an "" encoder – decoder "" transformer is generally the same as the original transformer, with 2 sublayers per encoder layer and 3 sublayers per decoder layer, etc. they might have minor architectural improvements, such as alternative activation functions, changing the location of normalization, etc. this is also usually used for text generation and instruction following. the models in the t5 series are encoder – decoder. a "" prefixlm "" ( prefix language model ) is a decoder - only architecture, but with prefix masking, which is different from causal masking. specifically, it has mask of the form m prefixlm = [ 0 − ∞ 0 m causal ] { \ displays"
q_096,dense,0.0,0.0,0.0,0.0,coronary artery,"##ctional recurrent neural networks bidirectional rnn a bidirectional rnn ( birnn ) is composed of two rnns, one processing the input sequence in one direction, and another in the opposite direction. abstractly, it is structured as follows : the forward rnn processes in one direction : f θ ( x 0, h 0 ) = ( y 0, h 1 ), f θ ( x 1, h 1 ) = ( y 1, h 2 ), … { \ displaystyle f _ { \ theta } ( x _ { 0 }, h _ { 0 } ) = ( y _ { 0 }, h _ { 1 } ), f _ { \ theta } ( x _ { 1 }, h _ { 1 } ) = ( y _ { 1 }, h _ { 2 } ), \ dots } the backward rnn processes in the opposite direction : f θ ′ ′ ( x n, h n ′ ) = ( y n ′, h n − 1 ′ ), f θ ′ ′ ( x n − 1, h n − 1 ′ ) = ( y n − 1 ′, h n − 2 ′ ), … { \ displaystyle f ' _ { \ theta ' } ( x _ { n }, h _ { n } ' ) = ( y ' _ { n }, h _ { n - 1 } ' ), f ' _ { \ theta ' } ( x _"
q_097,dense,0.0,0.0,0.0,0.0,,"##har ; mishra, shailesh ; endres, christoph ; holz, thorsten ; fritz, mario ( 2023 - 02 - 01 ). "" not what you ' ve signed up for : compromising real - world llm - integrated applications with indirect prompt injection "". proceedings of the 16th acm workshop on artificial intelligence and security. pp. 79 – 90. doi : 10. 1145 / 3605764. 3623985. isbn 979 - 8 - 4007 - 0260 - 0. ^ edwards, benj ( 2024 - 01 - 15 ). "" ai poisoning could turn models into destructive "" sleeper agents, "" says anthropic "". ars technica. retrieved 2025 - 07 - 19. ^ "" u. s. judge approves $ 1. 5 billion anthropic copyright settlement with authors "". reuters. 2025 - 09 - 25. retrieved 2025 - 09 - 26. ^ "" anthropic reaches $ 1. 5b settlement with authors over ai copyright claims "". associated press. 2025 - 09 - 25. retrieved 2025 - 09 - 26. ^ "" meta fends off authors ' u. s. copyright lawsuit over ai "". reuters. 2025 - 06 - 25. retrieved 2025 - 06 - 26. ^ "" meta scores victory in ai copyright case "". wired. 2025 - 06 - 25. retrieved 2025 - 06 - 26. ^ "" openai"
q_098,dense,0.0,0.0,0.0,0.025," lanci, michelangelo ( 1845 ). paralipomeni alla illustrazione della sagra scrittura ( in italian ) ( facsimile of the first ed. ). dondey - dupre. pp. 100 – 113.","##er architecture, introduced in 2017, replaced recurrence with self - attention, allowing efficient parallelization, longer context handling, and scalable training on unprecedented data volumes. this innovation enabled models like gpt, bert, and their successors, which demonstrated emergent behaviors at scale, such as few - shot learning and compositional reasoning. reinforcement learning, particularly policy gradient algorithms, has been adapted to fine - tune llms for desired behaviors beyond raw next - token prediction. reinforcement learning from human feedback ( rlhf ) applies these methods to optimize a policy, the llm ' s output distribution, against reward signals derived from human or automated preference judgments. this has been critical for aligning model outputs with user expectations, improving factuality, reducing harmful responses, and enhancing task performance. benchmark evaluations for llms have evolved from narrow linguistic assessments toward comprehensive, multi - task evaluations measuring reasoning, factual accuracy, alignment, and safety. hill climbing, iteratively optimizing models against benchmarks, has emerged as a dominant strategy, producing rapid incremental performance gains but raising concerns of overfitting to benchmarks rather than achieving genuine generalization or robust capability improvements. history the number of publications about large language models by year grouped by publication types. the training compute of notable large models in flops vs publication date over the period 2010 – 2024. for overall notable models ( top left ), frontier models ( top right ), top language models ( bottom left"
q_099,dense,0.0,0.0,0.0,0.0,"lord ""","s that are y s "" ). deductive reasoning in logic is the process of proving a new statement ( conclusion ) from other statements that are given and assumed to be true ( the premises ). proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules. given a problem and a set of premises, problem - solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. in the case of horn clauses, problem - solving search can be performed by reasoning forwards from the premises or backwards from the problem. in the more general case of the clausal form of first - order logic, resolution is a single, axiom - free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved. inference in both horn clause logic and first - order logic is undecidable, and therefore intractable. however, backward reasoning with horn clauses, which underpins computation in the logic programming language prolog, is turing complete. moreover, its efficiency is competitive with computation in other symbolic programming languages. fuzzy logic assigns a "" degree of truth "" between 0 and 1. it can therefore handle propositions that are vague and partially true. non - monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning."
q_000,sparse,0.0,0.0,0.0,0.0,b mode,"introduction of a multi - head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence. its parallelizability was an important factor to its widespread use in large neural networks. ai boom era as early as spring 2017, even before the "" attention is all you need "" preprint was published, one of the co - authors applied the "" decoder - only "" variation of the architecture to generate fictitious wikipedia articles. transformer architecture is now used alongside many generative models that contribute to the ongoing ai boom. in language modelling, elmo ( 2018 ) was a bi - directional lstm that produces contextualized word embeddings, improving upon the line of research from bag of words and word2vec. it was followed by bert ( 2018 ), an encoder - only transformer model. in october 2019, google started using bert to process search queries. in 2020, google translate replaced the previous rnn - encoder – rnn - decoder model by a transformer - encoder – rnn - decoder model. starting in 2018, the openai gpt series of decoder - only transformers became state of the art in natural language generation. in 2022, a chatbot based on gpt - 3, chatgpt, became unexpectedly popular, triggering a boom around large language models. since 2020, transformers have been applied in modalities beyond text, including the"
q_001,sparse,0.0,0.0,0.0,0.0,"israel ' s success.""","cs. cl ]. ^ press, ofir ; smith, noah a. ; lewis, mike ( 2021 - 08 - 01 ). "" train short, test long : attention with linear biases enables input length extrapolation "". arxiv : 2108. 12409 [ cs. cl ]. ^ shaw, peter ; uszkoreit, jakob ; vaswani, ashish ( 2018 ). "" self - attention with relative position representations "". arxiv : 1803. 02155 [ cs. cl ]. ^ ke, guolin ; he, di ; liu, tie - yan ( 2021 - 03 - 15 ), rethinking positional encoding in language pre - training, arxiv : 2006. 15595 ^ kwon, woosuk ; li, zhuohan ; zhuang, siyuan ; sheng, ying ; zheng, lianmin ; yu, cody hao ; gonzalez, joseph ; zhang, hao ; stoica, ion ( 2023 - 10 - 23 ). "" efficient memory management for large language model serving with pagedattention "". proceedings of the 29th symposium on operating systems principles. sosp ' 23. new york, ny, usa : association for computing machinery. pp. 611 – 626. arxiv : 2309. 06180. doi : 10. 1145 / 3600006. 3613165. isbn 979 - 8 -"
q_002,sparse,0.0,0.0,0.0,0.0,cancer,"- term memory based deep recurrent neural networks for large vocabulary speech recognition "". arxiv : 1410. 4281 [ cs. cl ]. ^ dupond, samuel ( 2019 ). "" a thorough review on the current advance of neural network structures "". annual reviews in control. 14 : 200 – 230. ^ abiodun, oludare isaac ; jantan, aman ; omolara, abiodun esther ; dada, kemi victoria ; mohamed, nachaat abdelatif ; arshad, humaira ( 2018 - 11 - 01 ). "" state - of - the - art in artificial neural network applications : a survey "". heliyon. 4 ( 11 ) e00938. bibcode : 2018heliy... 400938a. doi : 10. 1016 / j. heliyon. 2018. e00938. issn 2405 - 8440. pmc 6260436. pmid 30519653. ^ espinosa - sanchez, juan manuel ; gomez - marin, alex ; de castro, fernando ( 2023 - 07 - 05 ). "" the importance of cajal ' s and lorente de no ' s neuroscience to the birth of cybernetics "". the neuroscientist. 31 ( 1 ) : 14 – 30. doi : 10. 1177 / 10738584231179932. hdl : 1026"
q_003,sparse,0.0,0.0,0.0,0.0,john lydon,"on 14 february 2019. retrieved 20 january 2024. ^ "" chatgpt a year on : 3 ways the ai chatbot has completely changed the world in 12 months "". euronews. november 30, 2023. archived from the original on january 14, 2024. retrieved january 20, 2024. ^ heaven, will ( march 14, 2023 ). "" gpt - 4 is bigger and better than chatgpt — but openai won ' t say why "". mit technology review. archived from the original on march 17, 2023. retrieved january 20, 2024. ^ metz, cade ( september 12, 2024 ). "" openai unveils new chatgpt that can reason through math and science "". the new york times. retrieved september 12, 2024. ^ "" parameters in notable artificial intelligence systems "". ourworldindata. org. november 30, 2023. retrieved january 20, 2024. ^ sharma, shubham ( 2025 - 01 - 20 ). "" open - source deepseek - r1 uses pure reinforcement learning to match openai o1 — at 95 % less cost "". venturebeat. retrieved 2025 - 01 - 26. ^ "" llama - mesh "". research. nvidia. com. 2024. retrieved 2025 - 10 - 30. ^ zia, dr tehseen ( 2024 - 01 - 08 ). "" unveiling of large multimo"
q_004,sparse,0.0,0.0,0.0,0.0,jesus 's life,"deep learning for human action recognition "". in salah, albert ali ; lepri, bruno ( eds. ). human behavior unterstanding. lecture notes in computer science. vol. 7065. amsterdam, netherlands : springer. pp. 29 – 39. doi : 10. 1007 / 978 - 3 - 642 - 25446 - 8 _ 4. isbn 978 - 3 - 642 - 25445 - 1. ^ hochreiter, sepp ; heusel, martin ; obermayer, klaus ( 2007 ). "" fast model - based protein homology detection without alignment "". bioinformatics. 23 ( 14 ) : 1728 – 1736. doi : 10. 1093 / bioinformatics / btm247. pmid 17488755. ^ thireou, trias ; reczko, martin ( july 2007 ). "" bidirectional long short - term memory networks for predicting the subcellular localization of eukaryotic proteins "". ieee / acm transactions on computational biology and bioinformatics. 4 ( 3 ) : 441 – 446. bibcode : 2007itcbb... 4.. 441t. doi : 10. 1109 / tcbb. 2007. 1015. pmid 17666763. s2cid 11787259. ^ tax, niek ; verenich, ilya ; la rosa, marcello ; dumas,"
q_005,sparse,0.0,0.0,0.0,0.0,hundred years ago,"transformers for longer sequences with sparse attention methods "". google ai blog. 25 march 2021. archived from the original on 2021 - 09 - 18. retrieved 2021 - 05 - 28. ^ zhai, shuangfei ; talbott, walter ; srivastava, nitish ; huang, chen ; goh, hanlin ; zhang, ruixiang ; susskind, josh ( 2021 - 09 - 21 ). "" an attention free transformer "". arxiv : 2105. 14103 [ cs. lg ]. ^ peng, hao ; pappas, nikolaos ; yogatama, dani ; schwartz, roy ; smith, noah a. ; kong, lingpeng ( 2021 - 03 - 19 ). "" random feature attention "". arxiv : 2103. 02143 [ cs. cl ]. ^ choromanski, krzysztof ; likhosherstov, valerii ; dohan, david ; song, xingyou ; gane, andreea ; sarlos, tamas ; hawkins, peter ; davis, jared ; belanger, david ; colwell, lucy ; weller, adrian ( 2020 - 09 - 30 ). "" masked language modeling for proteins via linearly scalable long - context transformers "". arxiv : 2006. 03555 [ cs. lg ]. ^ lu, kevin ; grover, aditya ; abbeel, pieter ;"
q_006,sparse,0.0,0.0,0.0,0.0,whadja do today?,", fabio lorenzo ; di ventra, massimiliano ( 2017 ). "" the complex dynamics of memristive circuits : analytical results and universal slow relaxation "". physical review e. 95 ( 2 ) 022140. arxiv : 1608. 08651. bibcode : 2017phrve.. 95b2140c. doi : 10. 1103 / physreve. 95. 022140. pmid 28297937. s2cid 6758362. ^ harvey, inman ; husbands, phil ; cliff, dave ( 1994 ), "" seeing the light : artificial evolution, real vision "", 3rd international conference on simulation of adaptive behavior : from animals to animats 3, pp. 392 – 401 ^ quinn, matt ( 2001 ). "" evolving communication without dedicated communication channels "". advances in artificial life : 6th european conference, ecal 2001. pp. 357 – 366. doi : 10. 1007 / 3 - 540 - 44811 - x _ 38. isbn 978 - 3 - 540 - 42567 - 0. ^ beer, randall d. ( 1997 ). "" the dynamics of adaptive behavior : a research program "". robotics and autonomous systems. 20 ( 2 – 4 ) : 257 – 289. doi : 10. 1016 / s0921 - 8890 ( 96 ) 00063 - 2. ^ sherstinsky, alex ( 2018 - 12 -"
q_007,sparse,0.0,0.0,0.0,0.012578616352201259,christ the redeemer by andrea del sarto,"; mackworth, alan ( 2023 ). artificial intelligence, foundations of computational agents ( 3rd ed. ). cambridge university press. doi : 10. 1017 / 9781009258227. isbn 978 - 1 - 0092 - 5819 - 7. ^ russell, stuart ; norvig, peter ( 2020 ). artificial intelligence : a modern approach ( 4th ed. ). pearson. isbn 978 - 0 - 1346 - 1099 - 3. ^ "" why agents are the next frontier of generative ai "". mckinsey digital. 24 july 2024. archived from the original on 3 october 2024. retrieved 10 august 2024. ^ "" introducing copilot search in bing "". blogs. bing. com. 4 april 2025. ^ peters, jay ( 14 march 2023 ). "" the bing ai bot has been secretly running gpt - 4 "". the verge. retrieved 31 august 2025. ^ "" security for microsoft 365 copilot "". learn. microsoft. com. ^ o ' flaherty, kate ( 21 may 2025 ). "" google ai overviews — everything you need to know "". forbes. ^ "" generative ai in search : let google do the searching for you "". google. 14 may 2024. ^ figueiredo, mayara costa ; ankrah, elizabeth ; powell, jacquelyn e. ; epstein, daniel a. ; chen, yunan"
q_008,sparse,0.0,0.0,0.0,0.0,ka,"able to recognize the wide variety of input as being identical to each other in terms of its textual equivalent. speech segmentation given a sound clip of a person or people speaking, separate it into words. a subtask of speech recognition and typically grouped with it. text - to - speech given a text, transform those units and produce a spoken representation. text - to - speech can be used to aid the visually impaired. word segmentation ( tokenization ) tokenization is a text - processing technique that divides text into individual words or word fragments. this technique results in two key components : a word index and tokenized text. the word index is a list that maps unique words to specific numerical identifiers, and the tokenized text replaces each word with its corresponding numerical token. these numerical tokens are then used in various deep learning methods. for a language like english, this is fairly trivial, since words are usually separated by spaces. however, some written languages like chinese, japanese and thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language. sometimes this process is also used in cases like bag of words ( bow ) creation in data mining. morphological analysis lemmatization of basque words lemmatization the task of removing inflectional endings only and to return the base dictionary form of a word which is also known as a lemma. lemmatization"
q_009,sparse,0.0,0.0,0.0,0.0," singer , tovia ( 2010 ). let ' s get biblical – in depth study guide. outreach judaism ( 1998 ). asin b000","; azulay, osher ; sintov, avishai ( february 2023 ). "" learning to throw with a handful of samples using decision transformers "". ieee robotics and automation letters. 8 ( 2 ) : 576 – 583. bibcode : 2023iral.... 8.. 576m. doi : 10. 1109 / lra. 2022. 3229266. issn 2377 - 3766. ^ a b ruoss, anian ; deletang, gregoire ; medapati, sourabh ; grau - moya, jordi ; wenliang, li ; catt, elliot ; reid, john ; genewein, tim ( 2024 - 02 - 07 ). "" grandmaster - level chess without search "". arxiv : 2402. 04494v1 [ cs. lg ]. ^ a b wolf, thomas ; debut, lysandre ; sanh, victor ; chaumond, julien ; delangue, clement ; moi, anthony ; cistac, pierric ; rault, tim ; louf, remi ; funtowicz, morgan ; davison, joe ; shleifer, sam ; von platen, patrick ; ma, clara ; jernite, yacine ; plu, julien ; xu, canwen ; le scao, teven ; gugger, sylvain ; drame, maria"
q_010,sparse,0.0,0.0,0.0,0.023999999999999997,"the album was formally announced by the band on their facebook page on june 20, 2013 and was released on august 6, 2013 by prosthetic records.","the resulting models were reverse - engineered, and it turned out they used discrete fourier transform. the training of the model also highlighted a phenomenon called grokking, in which the model initially memorizes the training set ( overfitting ), and later suddenly learns to actually perform the calculation. understanding and intelligence see also : philosophy of artificial intelligence and artificial consciousness nlp researchers were evenly split when asked, in a 2022 survey, whether ( untuned ) llms "" could ( ever ) understand natural language in some nontrivial sense "". proponents of "" llm understanding "" believe that some llm abilities, such as mathematical reasoning, imply an ability to "" understand "" certain concepts. a microsoft team argued in 2023 that gpt - 4 "" can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more "" and that gpt - 4 "" could reasonably be viewed as an early ( yet still incomplete ) version of an artificial general intelligence system "" : "" can one reasonably say that a system that passes exams for software engineering candidates is not really intelligent? "" ilya sutskever argues that predicting the next word sometimes involves reasoning and deep insights, for example if the llm has to predict the name of the criminal in an unknown detective novel after processing the entire story leading up to the revelation. some researchers characterize llms as "" alien intelligence "". for example, conjecture ceo connor leahy considers untuned llms to be"
q_011,sparse,0.0,0.0,0.0,0.015625000000000003,"kirby, nicola ( 2023 - 12 - 19 )","googlers cracked an sf rival ' s tech model with a single word "". sfgate. archived from the original on 16 december 2023. ^ "" prepare for truly useful large language models "". nature biomedical engineering. 7 ( 2 ) : 85 – 86. 7 march 2023. doi : 10. 1038 / s41551 - 023 - 01012 - 6. pmid 36882584. s2cid 257403466. ^ brinkmann, levin ; baumann, fabian ; bonnefon, jean - francois ; derex, maxime ; muller, thomas f. ; nussberger, anne - marie ; czaplicka, agnieszka ; acerbi, alberto ; griffiths, thomas l. ; henrich, joseph ; leibo, joel z. ; mcelreath, richard ; oudeyer, pierre - yves ; stray, jonathan ; rahwan, iyad ( 2023 - 11 - 20 ). "" machine culture "". nature human behaviour. 7 ( 11 ) : 1855 – 1868. arxiv : 2311. 11388. doi : 10. 1038 / s41562 - 023 - 01742 - 2. issn 2397 - 3374. pmid 37985914. ^ niederhoffer, kate ; kellerman, gabriella rosen ; lee, angela ; liebscher, alex ; rapuano, kristina"
q_012,sparse,0.0,0.0,0.0,0.008620689655172414,u.s.,"the area of computational linguistics maintained strong ties with cognitive studies. as an example, george lakoff offers a methodology to build natural language processing ( nlp ) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, with two defining aspects : apply the theory of conceptual metaphor, explained by lakoff as "" the understanding of one idea, in terms of another "" which provides an idea of the intent of the author. for example, consider the english word big. when used in a comparison ( "" that is a big tree "" ), the author ' s intent is to imply that the tree is physically large relative to other trees or the authors experience. when used metaphorically ( "" tomorrow is a big day "" ), the author ' s intent to imply importance. the intent behind other usages, like in "" she is a big person "", will remain somewhat ambiguous to a person and a cognitive nlp algorithm alike without additional information. assign relative measures of meaning to a word, phrase, sentence or piece of text based on the information presented before and after the piece of text being analyzed, e. g., by means of a probabilistic context - free grammar ( pcfg ). the mathematical equation for such algorithms is presented in us patent 9269353 : r m m ( t o k e n n ) = p m m ( t o k e n n ) × 1 2 d ( [UNK] i = − d d ( ( p"
q_013,sparse,0.0,0.0,0.0,0.0,anisah osman britton,"^ zhang, aston ; lipton, zachary ; li, mu ; smola, alexander j. ( 2024 ). "" 10. modern recurrent neural networks "". dive into deep learning. cambridge new york port melbourne new delhi singapore : cambridge university press. isbn 978 - 1 - 009 - 38943 - 3. ^ rumelhart, david e. ; hinton, geoffrey e. ; williams, ronald j. ( october 1986 ). "" learning representations by back - propagating errors "". nature. 323 ( 6088 ) : 533 – 536. bibcode : 1986natur. 323.. 533r. doi : 10. 1038 / 323533a0. issn 1476 - 4687. ^ a b schmidhuber, jurgen ( 1993 ). habilitation thesis : system modeling and optimization ( pdf ). page 150 ff demonstrates credit assignment across the equivalent of 1, 200 layers in an unfolded rnn. ^ sepp hochreiter ; jurgen schmidhuber ( 21 august 1995 ), long short term memory, wikidata q98967430 ^ a b hochreiter, sepp ; schmidhuber, jurgen ( 1997 - 11 - 01 ). "" long short - term memory "". neural computation. 9 ( 8 ) : 1735 – 1780. doi : 10. 1162 / neco. 1997. 9. 8. 1735. pmid"
q_014,sparse,0.0,0.0,0.0,0.00851063829787234,palash of spring,"explicit symbolic knowledge. although his arguments had been ridiculed and ignored when they were first presented, eventually, ai research came to agree with him. the issue is not resolved : sub - symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. critics such as noam chomsky argue continuing research into symbolic ai will still be necessary to attain general intelligence, in part because sub - symbolic ai is a move away from explainable ai : it can be difficult or impossible to understand why a modern statistical ai program made a particular decision. the emerging field of neuro - symbolic artificial intelligence attempts to bridge the two approaches. neat vs. scruffy main article : neats and scruffies "" neats "" hope that intelligent behavior is described using simple, elegant principles ( such as logic, optimization, or neural networks ). "" scruffies "" expect that it necessarily requires solving a large number of unrelated problems. neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. this issue was actively discussed in the 1970s and 1980s, but eventually was seen as irrelevant. modern ai has elements of both. soft vs. hard computing main article : soft computing finding a provably correct or optimal solution is intractable for many important problems. soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision,"
q_015,sparse,0.0,0.0,0.0,0.0,mount powell,"from paralegals to fast food cooks, while job demand is likely to increase for care - related professions ranging from personal healthcare to the clergy. in july 2025, ford ceo jim farley predicted that "" artificial intelligence is going to replace literally half of all white - collar workers in the u. s. "" from the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by joseph weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value - based judgement. existential risk main article : existential risk from artificial intelligence recent public debates in artificial intelligence have increasingly focused on its broader societal and ethical implications. it has been argued ai will become so powerful that humanity may irreversibly lose control of it. this could, as physicist stephen hawking stated, "" spell the end of the human race "". this scenario has been common in science fiction, when a computer or robot suddenly develops a human - like "" self - awareness "" ( or "" sentience "" or "" consciousness "" ) and becomes a malevolent character. these sci - fi scenarios are misleading in several ways. first, ai does not require human - like sentience to be an existential risk. modern ai programs are given specific goals and use learning and intelligence to achieve them. philosopher nick bostrom argued that if one gives almost any goal"
q_016,sparse,0.0,0.0,0.0,0.0,psyche,"information : word embedding each integer token identifier is converted into an embedding vector via a lookup table. equivalently stated, it multiplies a one - hot representation of the token identifier by an embedding matrix m { \ displaystyle m }. for example, if the input token ' s identifier is 3 { \ displaystyle 3 }, then the one - hot representation is [ 0, 0, 0, 1, 0, 0, … ] { \ displaystyle [ 0, 0, 0, 1, 0, 0, \ dots ] }, and its embedding vector is e m b e d ( 3 ) = [ 0, 0, 0, 1, 0, 0, … ] m { \ displaystyle \ mathrm { embed } ( 3 ) = [ 0, 0, 0, 1, 0, 0, \ dots ] m } the token embedding vectors are added to their respective positional encoding vectors ( see below ), producing the sequence of input vectors. the dimension of an embedding vector is called hidden size or embedding size and written as d emb { \ displaystyle d _ { \ text { emb } } }. this size is written as d model { \ displaystyle d _ { \ text { model } } } in the original transformer paper. un - embedding an un - embedding layer is almost the reverse of"
q_017,sparse,0.0,0.0,0.0,0.0,continuity,"training data for llms. this produces large volumes of traffic which has led to denial of service issues with many websites. the situation has been described as "" a ddos on the entire internet "" and in some cases scrapers make up the majority of traffic to a site. ai web crawlers may bypass the methods that are usually used to block web scrapers, such as robots. txt files, blocking user - agents and filtering suspicious traffic. website operators have resorted to novel methods such as ai tarpits, but some fear that tarpits will only worsen the burden on servers. mental health clinical and mental health contexts present emerging applications alongside significant safety concerns. research and social media posts suggest that some individuals are using llms to seek therapy or mental health support. in early 2025, a survey by sentio university found that nearly half ( 48. 7 % ) of 499 u. s. adults with ongoing mental health conditions who had used llms reported turning to them for therapy or emotional support, including help with anxiety, depression, loneliness, and similar concerns. llms can produce hallucinations — plausible but incorrect statements — which may mislead users in sensitive mental health contexts. research also shows that llms may express stigma or inappropriate agreement with maladaptive thoughts, reflecting limitations in replicating the judgment and relational skills of human therapists. evaluations of crisis scenarios indicate that some llms lack effective safety protocols, such as assessing suicide risk or making appropriate"
q_018,sparse,0.0,0.0,0.0,0.0,sensitivity and specificity,"d, x ⟩, sin ⟨ w d, x ⟩ ] t { \ displaystyle \ varphi ( x ) = { \ frac { 1 } { \ sqrt { d } } } [ \ cos \ langle w _ { 1 }, x \ rangle, \ sin \ langle w _ { 1 }, x \ rangle, \ cdots \ cos \ langle w _ { d }, x \ rangle, \ sin \ langle w _ { d }, x \ rangle ] ^ { t } } where w 1,..., w d { \ displaystyle w _ { 1 },..., w _ { d } } are independent samples from the normal distribution n ( 0, σ 2 i ) { \ displaystyle n ( 0, \ sigma ^ { 2 } i ) }. this choice of parameters satisfy e [ ⟨ φ ( x ), φ ( y ) ⟩ ] = e − ‖ x − y ‖ 2 2 σ 2 { \ displaystyle \ mathbb { e } [ \ langle \ varphi ( x ), \ varphi ( y ) \ rangle ] = e ^ { - { \ frac { \ | x - y \ | ^ { 2 } } { 2 \ sigma ^ { 2 } } } } }, or e ⟨ x, y ⟩ / σ 2 = e [ ⟨ e ‖ x ‖ 2 / 2 σ 2 φ"
q_019,sparse,0.0,0.0,0.0,0.0,chiquita,"commitments from companies "". reuters. 21 may 2024. retrieved 23 may 2024. ^ "" frontier ai safety commitments, ai seoul summit 2024 "". gov. uk. 21 may 2024. archived from the original on 23 may 2024. retrieved 23 may 2024. ^ a b buntz, brian ( 3 november 2024 ). "" quality vs. quantity : us and china chart different paths in global ai patent race in 2024 / geographical breakdown of ai patents in 2024 "". research & development world. r & d world. archived from the original on 9 december 2024. ^ a b russell & norvig 2021, p. 9. ^ a b c copeland, j., ed. ( 2004 ). the essential turing : the ideas that gave birth to the computer age. oxford, england : clarendon press. isbn 0 - 1982 - 5079 - 7. ^ "" google books ngram "". archived from the original on 5 october 2024. retrieved 5 october 2024. ^ ai ' s immediate precursors : mccorduck ( 2004, pp. 51 – 107 ), crevier ( 1993, pp. 27 – 32 ), russell & norvig ( 2021, pp. 8 – 17 ), moravec ( 1988, p. 3 ) ^ a b turing ' s original publication of the turing test in "" computing machinery and intelligence "" : turing ( 1950 ) historical influence and philosophical implications : haugeland"
q_020,sparse,0.0,0.0,0.0,0.024096385542168672,christ the redeemer by andrea del sarto part one — who he was ( chapters 1 - 5 ) part two — why he came ( chapters 6 - 11 ) part three — what he left behind ( chapte,"; kuttler, heinrich ; lewis, mike ; yih, wen - tau ; rocktaschel, tim ; riedel, sebastian ; kiela, douwe ( 2020 ). "" retrieval - augmented generation for knowledge - intensive nlp tasks "". advances in neural information processing systems. 33. curran associates, inc. : 9459 – 9474. arxiv : 2005. 11401. archived from the original on 2023 - 06 - 12. retrieved 2023 - 06 - 12. ^ dickson, ben ( 2025 - 04 - 02 ). "" the tool integration problem that ' s holding back enterprise ai ( and how cotools solves it ) "". venturebeat. retrieved 2025 - 05 - 26. ^ liang, yaobo ; wu, chenfei ; song, ting ; wu, wenshan ; xia, yan ; liu, yu ; ou, yang ; lu, shuai ; ji, lei ; mao, shaoguang ; wang, yun ; shou, linjun ; gong, ming ; duan, nan ( 2024 ). "" taskmatrix. ai : completing tasks by connecting foundation models with millions of apis "". science. 3 0063. doi : 10. 34133 / icomputing. 0063. ^ patil, shishir g. ; zhang, tianjun ; wang, xin ; gonzalez, joseph e. ( 2023 - 05 - 01"
q_021,sparse,0.0,0.0,0.0,0.0,observed,"be easier to train, requiring no warm - up, leading to faster convergence. pseudocode the following is the pseudocode for a standard pre - ln encoder – decoder transformer, adapted from formal algorithms for transformers input : encoder input t _ e decoder input t _ d output : array of probability distributions, with shape ( decoder vocabulary size x length ( decoder output sequence ) ) / * encoder * / z _ e ← encoder. tokenizer ( t _ e ) for each t in 1 : length ( z _ e ) do z _ e [ t ] ← encoder. embedding ( z _ e [ t ] ) + encoder. positional _ embedding ( t ) for each l in 1 : length ( encoder. layers ) do layer ← encoder. layers [ l ] / * first sublayer * / z _ e _ copy ← copy ( z _ e ) for each t in 1 : length ( z _ e ) do z _ e [ t ] ← layer. layer _ norm ( z _ e [ t ] ) z _ e ← layer. multihead _ attention ( z _ e, z _ e, z _ e ) for each t in 1 : length ( z _ e ) do z _ e [ t ] ← z _ e [ t ] + z _ e _ copy [ t ] / * second sublayer * / z _ e"
q_022,sparse,0.0,0.0,0.0,0.009433962264150945,continuity,"predict the name of the criminal in an unknown detective novel after processing the entire story leading up to the revelation. some researchers characterize llms as "" alien intelligence "". for example, conjecture ceo connor leahy considers untuned llms to be like inscrutable alien "" shoggoths "", and believes that rlhf tuning creates a "" smiling facade "" obscuring the inner workings of the llm : "" if you don ' t push it too far, the smiley face stays on. but then you give it [ an unexpected ] prompt, and suddenly you see this massive underbelly of insanity, of weird thought processes and clearly non - human understanding. "" in contrast, some skeptics of llm understanding believe that existing llms are "" simply remixing and recombining existing writing "", a phenomenon known as stochastic parrot, or they point to the deficits existing llms continue to have in prediction skills, reasoning skills, agency, and explainability. for example, gpt - 4 has natural deficits in planning and in real - time learning. generative llms have been observed to confidently assert claims of fact which do not seem to be justified by their training data, a phenomenon which has been termed "" hallucination "". specifically, hallucinations in the context of llms correspond to the generation of text or responses that seem syntactically sound, fluent, and natural but are factually incorrect, nonsen"
q_023,sparse,0.0,0.0,0.0,0.010752688172043012,c s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s,": 10. 1353 / pbm. 2000. 0001. issn 1529 - 8795. pmid 10804585. ^ renshaw, birdsey ( 1946 - 05 - 01 ). "" central effects of centripetal impulses in axons of spinal ventral roots "". journal of neurophysiology. 9 ( 3 ) : 191 – 204. doi : 10. 1152 / jn. 1946. 9. 3. 191. issn 0022 - 3077. pmid 21028162. ^ a b grossberg, stephen ( 2013 - 02 - 22 ). "" recurrent neural networks "". scholarpedia. 8 ( 2 ) : 1888. bibcode : 2013schpj... 8. 1888g. doi : 10. 4249 / scholarpedia. 1888. issn 1941 - 6016. ^ a b c rosenblatt, frank ( 1961 - 03 - 15 ). dtic ad0256582 : principles of neurodynamics. perceptrons and the theory of brain mechanisms. defense technical information center. ^ f. rosenblatt, "" perceptual generalization over transformation groups "", pp. 63 - - 100 in self - organizing systems : proceedings of an inter - disciplinary conference, 5 and 6 may 1959. edited by marshall c. yovitz and scott cameron. london, new york, [ etc. ], pergamon press, 1960"
q_024,sparse,0.0,0.0,0.0,0.0,jesus 's life,"time ) skepticism by most participants. until then, neural learning was basically rejected because of its lack of statistical interpretability. until 2015, deep learning had evolved into the major framework of nlp. [ link is broken, try http : / / web. stanford. edu / class / cs224n / ] ^ segev, elad ( 2022 ). semantic network analysis in social sciences. london : routledge. isbn 978 - 0 - 367 - 63652 - 4. archived from the original on 5 december 2021. retrieved 5 december 2021. ^ yi, chucai ; tian, yingli ( 2012 ), "" assistive text reading from complex background for blind persons "", camera - based document analysis and recognition, lecture notes in computer science, vol. 7139, springer berlin heidelberg, pp. 15 – 28, citeseerx 10. 1. 1. 668. 869, doi : 10. 1007 / 978 - 3 - 642 - 29364 - 1 _ 2, isbn 978 - 3 - 642 - 29363 - 4 { { citation } } : cs1 maint : work parameter with isbn ( link ) ^ a b "" natural language processing ( nlp ) - a complete guide "". www. deeplearning. ai. 2023 - 01 - 11. retrieved 2024 - 05 - 05. ^ "" geeksforgeeks. ( n. d. ). tokenization in natural language"
q_025,sparse,0.0,0.0,0.0,0.0,''                                                 ,"##opomorphism foundation models list of large language models list of chatbots language model benchmark reinforcement learning small language model references ^ a b c bommasani, rishi ; hudson, drew a. ; adeli, ehsan ; altman, russ ; arora, simran ; von arx, matthew ; bernstein, michael s. ; bohg, jeannette ; bosselut, antoine ; brunskill, emma ( 2021 ). "" on the opportunities and risks of foundation models "". arxiv : 2108. 07258 [ cs. lg ]. ^ a b brown, tom b. ; mann, benjamin ; ryder, nick ; subbiah, melanie ; kaplan, jared ; dhariwal, prafulla ; neelakantan, arvind ; shyam, pranav ; sastry, girish ; askell, amanda ( 2020 ). "" language models are few - shot learners "". arxiv : 2005. 14165 [ cs. cl ]. ^ a b c brown, tom b. ; mann, benjamin ; ryder, nick ; subbiah, melanie ; kaplan, jared ; dhariwal, prafulla ; neelakantan, arvind ; shyam, pranav ; sastry, girish ; askell, amanda ; agarwal, sandhini ; herbert - voss, ariel ; krueger, gretchen ; henighan, tom ; child"
q_026,sparse,0.0,0.0,0.0,0.00796812749003984,"liber novus "". the northern echo. retrieved 13 november 2015.","of research were continued, e. g., the development of chatterbots with racter and jabberwacky. an important development ( that eventually led to the statistical turn in the 1990s ) was the rising importance of quantitative evaluation in this period. statistical nlp ( 1990s – present ) up until the 1980s, most natural language processing systems were based on complex sets of hand - written rules. starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. this shift was influenced by increasing computational power ( see moore ' s law ) and a decline in the dominance of chomskyan linguistic theories... ( e. g. transformational grammar ), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine - learning approach to language processing. 1990s : many of the notable early successes in statistical methods in nlp occurred in the field of machine translation, due especially to work at ibm research, such as ibm alignment models. these systems were able to take advantage of existing multilingual textual corpora that had been produced by the parliament of canada and the european union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. however, many systems relied on corpora that were specifically developed for the tasks they were designed to perform. this reliance has been a major limitation to their broader effectiveness and continues to affect similar systems. consequently,"
q_027,sparse,0.0,0.0,0.0,0.014814814814814814,"administration, logistics, and communications","; lovreglio, ruggiero ; kuligowski, erica ( 2024 ). "" ai for large - scale evacuation modeling : promises and challenges "". interpretable machine learning for the analysis, design, assessment, and informed decision making for civil infrastructure. pp. 185 – 204. doi : 10. 1016 / b978 - 0 - 12 - 824073 - 1. 00014 - 9. isbn 978 - 0 - 12 - 824073 - 1. ^ gomaa, islam ; adelzadeh, masoud ; gwynne, steven ; spencer, bruce ; ko, yoon ; benichou, noureddine ; ma, chunyun ; elsagan, nour ; duong, dana ; zalok, ehab ; kinateder, max ( 1 november 2021 ). "" a framework for intelligent fire detection and evacuation system "". fire technology. 57 ( 6 ) : 3179 – 3185. doi : 10. 1007 / s10694 - 021 - 01157 - 3. ^ zhao, xilei ; lovreglio, ruggiero ; nilsson, daniel ( 1 may 2020 ). "" modelling and interpreting pre - evacuation decision - making using machine learning "". automation in construction. 113 103140. doi : 10. 1016 / j. autcon. 2020. 103140. hdl : 10179 / 17315. ^ "" india ' s"
q_028,sparse,0.0,0.0,0.0,0.0,continuity,"study. in the late 2010s and early 2020s, agi companies began to deliver programs that created enormous interest. in 2015, alphago, developed by deepmind, beat the world champion go player. the program taught only the game ' s rules and developed a strategy by itself. gpt - 3 is a large language model that was released in 2020 by openai and is capable of generating high - quality human - like text. chatgpt, launched on 30 november 2022, became the fastest - growing consumer software application in history, gaining over 100 million users in two months. it marked what is widely regarded as ai ' s breakout year, bringing it into the public consciousness. these programs, and others, inspired an aggressive ai boom, where large companies began investing billions of dollars in ai research. according to ai impacts, about us $ 50 billion annually was invested in "" ai "" around 2022 in the u. s. alone and about 20 % of the new u. s. computer science phd graduates have specialized in "" ai "". about 800, 000 "" ai "" - related u. s. job openings existed in 2022. according to pitchbook research, 22 % of newly funded startups in 2024 claimed to be ai companies. philosophy main article : philosophy of artificial intelligence philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. another major focus has been whether machines can be conscious, and the associated ethical implications. many other topics in"
q_029,sparse,0.0,0.0,0.0,0.0,loene carmen,"##g. 2. 2. 30247. 50087. ^ iraqi, amjad ( 3 april 2024 ). "" ' lavender ' : the ai machine directing israel ' s bombing spree in gaza "". + 972 magazine. archived from the original on 10 october 2024. retrieved 6 april 2024. ^ davies, harry ; mckernan, bethan ; sabbagh, dan ( 1 december 2023 ). "" ' the gospel ' : how israel uses ai to select bombing targets in gaza "". the guardian. archived from the original on 6 december 2023. retrieved 4 december 2023. ^ marti, j werner ( 10 august 2024 ). "" drohnen haben den krieg in der ukraine revolutioniert, doch sie sind empfindlich auf storsender – deshalb sollen sie jetzt autonom operieren "". neue zurcher zeitung ( in german ). archived from the original on 10 august 2024. retrieved 10 august 2024. ^ banh, leonardo ; strobel, gero ( 2023 ). "" generative artificial intelligence "". electronic markets. 33 ( 1 ) 63. doi : 10. 1007 / s12525 - 023 - 00680 - 1. ^ pasick, adam ( 27 march 2023 ). "" artificial intelligence glossary : neural networks and other terms explained "". the new york times"
q_030,sparse,0.0,0.0,0.0,0.0,2010 ivy league awards,"in areas where there is hope that the future will be better than the past. it is descriptive rather than prescriptive. bias and unfairness may go undetected because the developers are overwhelmingly white and male : among ai engineers, about 4 % are black and 20 % are women. there are various conflicting definitions and mathematical models of fairness. these notions depend on ethical assumptions, and are influenced by beliefs about society. one broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. representational fairness tries to ensure that ai systems do not reinforce negative stereotypes or render certain groups invisible. procedural fairness focuses on the decision process rather than the outcome. the most relevant notions of fairness may depend on the context, notably the type of ai application and the stakeholders. the subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. having access to sensitive attributes such as race or gender is also considered by many ai ethicists to be necessary in order to compensate for biases, but it may conflict with anti - discrimination laws. at the 2022 acm conference on fairness, accountability, and transparency a paper reported that a clip ‑ based ( contrastive language - image pre - training ) robotic system reproduced harmful gender ‑ and race ‑ linked stereotypes in a simulated manipulation task. the authors recommended robot ‑ learning methods which physically manifest such harms be "" paused, reworked, or even wound down"
q_031,sparse,0.0,0.0,0.0,0.0,regain your soul,"- 100 in self - organizing systems : proceedings of an inter - disciplinary conference, 5 and 6 may 1959. edited by marshall c. yovitz and scott cameron. london, new york, [ etc. ], pergamon press, 1960. ix, 322 p. ^ nakano, kaoru ( 1971 ). "" learning process in a model of associative memory "". pattern recognition and machine learning. pp. 172 – 186. doi : 10. 1007 / 978 - 1 - 4615 - 7566 - 5 _ 15. isbn 978 - 1 - 4615 - 7568 - 9. ^ nakano, kaoru ( 1972 ). "" associatron - a model of associative memory "". ieee transactions on systems, man, and cybernetics. smc - 2 ( 3 ) : 380 – 388. bibcode : 1972itsmc... 2.. 380n. doi : 10. 1109 / tsmc. 1972. 4309133. ^ amari, shun - ichi ( 1972 ). "" learning patterns and pattern sequences by self - organizing nets of threshold elements "". ieee transactions. c ( 21 ) : 1197 – 1206. ^ little, w. a. ( 1974 ). "" the existence of persistent states in the brain "". mathematical biosciences. 19 ( 1 – 2 ) : 101 – 120. doi : 10. 1016"
q_032,sparse,0.0,0.0,0.0,0.0,utoronto mississauga,". unice. fr. archived from the original on 2021 - 04 - 18. retrieved 2021 - 03 - 09. ^ "" nlp approaches to computational argumentation – acl 2016, berlin "". retrieved 2021 - 03 - 09. ^ administration. "" centre for language technology ( clt ) "". macquarie university. retrieved 2021 - 01 - 11. ^ "" shared task : grammatical error correction "". www. comp. nus. edu. sg. retrieved 2021 - 01 - 11. ^ "" shared task : grammatical error correction "". www. comp. nus. edu. sg. retrieved 2021 - 01 - 11. ^ duan, yucong ; cruz, christophe ( 2011 ). "" formalizing semantic of natural language through conceptualization from existence "". international journal of innovation, management and technology. 2 ( 1 ) : 37 – 42. archived from the original on 2011 - 10 - 09. ^ "" u b u w e b : : racter "". www. ubu. com. retrieved 2020 - 08 - 17. ^ writer, beta ( 2019 ). lithium - ion batteries. doi : 10. 1007 / 978 - 3 - 030 - 16800 - 1. isbn 978 - 3 - 030 - 16799 - 8. s2cid 155818532. ^ "" document understanding ai on google cloud ( cloud next ' 19 ) – youtube "". www. youtube. com. 11 april 2019"
q_033,sparse,0.0,0.0,0.0,0.008620689655172414,public image ltd,"##set. this basic construction can be applied with more sophistication to improve the model. the image encoder may be frozen to improve stability. this type of method, where embeddings from multiple modalities are fused and the predictor is trained on the combined embeddings, is called early fusion. another method, called intermediate fusion, involves each modality being first processed independently to obtain modality - specific representations ; then these intermediate representations are fused together. in general, cross - attention is used for integrating information from different modalities. as an example, the flamingo model uses cross - attention layers to inject visual information into its pre - trained language model. non - natural languages llms can handle programming languages similarly to how they handle natural languages. no special change in token handling is needed as code, like human language, is represented as plain text. llms can generate code based on problems or instructions written in natural language. they can also describe code in natural language or translate it into other programming languages. they were originally used as a code completion tool, but advances have moved them towards automatic programming. services such as github copilot offer llms specifically trained, fine - tuned, or prompted for programming. in computational biology, transformer - base architectures, such as dna llms, have also proven useful in analyzing biological sequences : protein, dna, and rna. with proteins they appear able to capture a degree of "" grammar """
q_034,sparse,0.0,0.0,0.0,0.0,continuity,"be researching battlefield robots. ai tools make it easier for authoritarian governments to efficiently control their citizens in several ways. face and voice recognition allow widespread surveillance. machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. recommendation systems can precisely target propaganda and misinformation for maximum effect. deepfakes and generative ai aid in producing misinformation. advanced ai can make authoritarian centralized decision - making more competitive than liberal and decentralized systems such as markets. it lowers the cost and difficulty of digital warfare and advanced spyware. all these technologies have been available since 2020 or earlier — ai facial recognition systems are already being used for mass surveillance in china. there are many other ways in which ai is expected to help bad actors, some of which can not be foreseen. for example, machine - learning ai is able to design tens of thousands of toxic molecules in a matter of hours. technological unemployment main articles : workplace impact of artificial intelligence and technological unemployment economists have frequently highlighted the risks of redundancies from ai, and speculated about unemployment if there is no adequate social policy for full employment. in the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that "" we ' re in uncharted territory "" with ai. a survey of economists showed disagreement about whether the increasing use of robots and ai will cause a substantial increase in long - term unemployment, but they generally agree that it could be a net benefit if"
q_035,sparse,0.0,0.0,0.0,0.0,israel,", accountability, and transparency. pp. 599 – 627. arxiv : 2504. 18412. doi : 10. 1145 / 3715275. 3732039. isbn 979 - 8 - 4007 - 1482 - 5. ^ grabb, declan ; lamparth, max ; vasan, nina ( 2024 - 08 - 14 ). "" risks from language models for automated mental healthcare : ethics and structure for implementation "". arxiv : 2406. 11852 [ cs. cy ]. ^ mcbain, ryan k. ; cantor, jonathan h. ; zhang, li ang ; baker, olesya ; zhang, fang ; halbisen, alyssa ; kofner, aaron ; breslau, joshua ; stein, bradley ; mehrotra, ateev ; yu, hao ( 2025 - 03 - 05 ). "" competency of large language models in evaluating appropriate responses to suicidal ideation : comparative study "". journal of medical internet research. 27 ( 1 ) e67891. doi : 10. 2196 / 67891. pmc 11928068. pmid 40053817. ^ li, fei - fei ; etchemendy, john ( 2024 - 05 - 22 ). "" no, today ' s ai isn ' t sentient. here ' s how we know "". time. retrieved 2024 - 05 - 22. ^"
q_036,sparse,0.0,0.0,0.0,0.023952095808383235,david guetta and nicki minaj on a song.,"} } f ( t ) = ( e i t / r k ) k = 0, 1, …, d 2 − 1 { \ displaystyle f ( t ) = \ left ( e ^ { it / r ^ { k } } \ right ) _ { k = 0, 1, \ ldots, { \ frac { d } { 2 } } - 1 } } where r = n 2 / d { \ displaystyle r = n ^ { 2 / d } }. the main reason for using this positional encoding function is that using it, shifts are linear transformations : f ( t + δ t ) = d i a g ( f ( δ t ) ) f ( t ) { \ displaystyle f ( t + \ delta t ) = \ mathrm { diag } ( f ( \ delta t ) ) f ( t ) } where δ t ∈ r { \ displaystyle \ delta t \ in \ mathbb { r } } is the distance one wishes to shift. this allows the transformer to take any encoded position, and find the encoding of the position n - steps - ahead or n - steps - behind, by a matrix multiplication. by taking a linear sum, any convolution can also be implemented as linear transformations : [UNK] j c j f ( t + δ t j ) = ( [UNK] j c j d i a g ( f ( δ t j ) ) ) f ( t ) { \ displaystyle"
q_037,sparse,0.0,0.0,0.0,0.02127659574468085,liber novus was a visionary book,"of only twenty words, because that was all that would fit in a computer memory at the time. 1970s : during the 1970s, many programmers began to write "" conceptual ontologies "", which structured real - world information into computer - understandable data. examples are margie ( schank, 1975 ), sam ( cullingford, 1978 ), pam ( wilensky, 1978 ), talespin ( meehan, 1976 ), qualm ( lehnert, 1977 ), politics ( carbonell, 1979 ), and plot units ( lehnert 1981 ). during this time, the first chatterbots were written ( e. g., parry ). 1980s : the 1980s and early 1990s mark the heyday of symbolic methods in nlp. focus areas of the time included research on rule - based parsing ( e. g., the development of hpsg as a computational operationalization of generative grammar ), morphology ( e. g., two - level morphology ), semantics ( e. g., lesk algorithm ), reference ( e. g., within centering theory ) and other areas of natural language understanding ( e. g., in the rhetorical structure theory ). other lines of research were continued, e. g., the development of chatterbots with racter and jabberwacky. an important development ( that eventually led to the statistical turn in the 1990s ) was the rising importance of quantitative evaluation in this period."
q_038,sparse,0.0,0.0,0.0,0.0,regain your soul,". cv ]. ^ campolucci, paolo ; uncini, aurelio ; piazza, francesco ; rao, bhaskar d. ( 1999 ). "" on - line learning algorithms for locally recurrent neural networks "". ieee transactions on neural networks. 10 ( 2 ) : 253 – 271. bibcode : 1999itnn... 10.. 253c. citeseerx 10. 1. 1. 33. 7550. doi : 10. 1109 / 72. 750549. pmid 18252525. ^ wan, eric a. ; beaufays, francoise ( 1996 ). "" diagrammatic derivation of gradient algorithms for neural networks "". neural computation. 8 : 182 – 201. doi : 10. 1162 / neco. 1996. 8. 1. 182. s2cid 15512077. ^ a b campolucci, paolo ; uncini, aurelio ; piazza, francesco ( 2000 ). "" a signal - flow - graph approach to on - line gradient calculation "". neural computation. 12 ( 8 ) : 1901 – 1927. citeseerx 10. 1. 1. 212. 5406. doi : 10. 1162 / 089976600300015196. pmid 10953244. s2cid 15090951. ^ graves, alex ; fernandez, santiago ; gomez, faustino j. ( 2006 ). "" connectionist temporal classification : label"
q_039,sparse,0.0,0.0,0.0,0.0,william wall 's work on infant baptism,"- zheng ; lee, yee - chun ( 1992 ). "" learning and extracting finite state automata with second - order recurrent neural networks "" ( pdf ). neural computation. 4 ( 3 ) : 393 – 405. doi : 10. 1162 / neco. 1992. 4. 3. 393. s2cid 19666035. ^ omlin, christian w. ; giles, c. lee ( 1996 ). "" constructing deterministic finite - state automata in recurrent neural networks "". journal of the acm. 45 ( 6 ) : 937 – 972. citeseerx 10. 1. 1. 32. 2364. doi : 10. 1145 / 235809. 235811. s2cid 228941. ^ paine, rainer w. ; tani, jun ( 2005 - 09 - 01 ). "" how hierarchical control self - organizes in artificial adaptive systems "". adaptive behavior. 13 ( 3 ) : 211 – 225. doi : 10. 1177 / 105971230501300303. s2cid 9932565. ^ a b "" burns, benureau, tani ( 2018 ) a bergson - inspired adaptive time constant for the multiple timescales recurrent neural network model. jnns "". ^ barkan, oren ; benchimol, jonathan ; caspi, itamar ; cohen, eliya ;"
q_040,sparse,0.0,0.0,0.0,0.05319148936170213,valenciennes v chateauroux dunkerque valenciennes valenciennes v chateauroux dunkerque valenciennes valenciennes v caen grenoble valenciennes valenciennes v sochaux troyes valenciennes valenciennes v troyes clermont toulous,"^ { k } }, and the value weights w v { \ displaystyle w ^ { v } }. the module takes three sequences, a query sequence, a key sequence, and a value sequence. the query sequence is a sequence of length ℓ seq, query { \ displaystyle \ ell _ { \ text { seq, query } } }, and each entry is a vector of dimension d emb, query { \ displaystyle d _ { \ text { emb, query } } }. similarly for the key and value sequences. for each vector x i, query { \ displaystyle x _ { i, { \ text { query } } } } in the query sequence, it is multiplied by a matrix w q { \ displaystyle w ^ { q } } to produce a query vector q i = x i, query w q { \ displaystyle q _ { i } = x _ { i, { \ text { query } } } w ^ { q } }. the matrix of all query vectors is the query matrix : q = x query w q { \ displaystyle q = x _ { \ text { query } } w ^ { q } } similarly, we construct the key matrix k = x key w k { \ displaystyle k = x _ { \ text { key } } w ^ { k } } and the value matrix v = x value w v { \ displaystyle v = x _ { \ text {"
q_041,sparse,0.0,0.0,0.0,0.0,ecx,", in the smallest gpt - 2 model, there are only self - attention mechanisms. it has the following dimensions : d emb = 768, n head = 12, d head = 64 { \ displaystyle d _ { \ text { emb } } = 768, n _ { \ text { head } } = 12, d _ { \ text { head } } = 64 } since 12 × 64 = 768 { \ displaystyle 12 \ times 64 = 768 }, its output projection matrix w o ∈ r ( 12 × 64 ) × 768 { \ displaystyle w ^ { o } \ in \ mathbb { r } ^ { ( 12 \ times 64 ) \ times 768 } } is a square matrix. masked attention the transformer architecture is constructed to calculate output tokens iteratively. assuming t = 0 { \ displaystyle t = 0 } refers to the calculation of the first output token i = 0 { \ displaystyle i = 0 }, for step t > 0 { \ displaystyle t > 0 }, the output token i = 0 { \ displaystyle i = 0 } shall remain constant. this ensures properties of the model similar to autoregressive models. therefore, at every time step t { \ displaystyle t }, the calculation for all outputs i { \ displaystyle i } should not have access to tokens at position j { \ displaystyle j } for j > = i { \"
q_042,sparse,0.0,0.0,0.0,0.0,aacc.org,"acm conference on fairness, accountability, and transparency ( facct ' 22 ). seoul, south korea : association for computing machinery. doi : 10. 1145 / 3531146. 3533138. ^ for accessible summaries, see the georgia tech release and sciencedaily coverage of the study ' s findings. "" flawed ai makes robots racist, sexist "". georgia tech research news. 23 june 2022. ^ "" robots turn racist and sexist with flawed ai, study finds "". sciencedaily. 21 june 2022. ^ sample ( 2017 ). ^ "" black box ai "". 16 june 2023. archived from the original on 15 june 2024. retrieved 5 october 2024. ^ christian ( 2020 ), p. 110. ^ christian ( 2020 ), pp. 88 – 91. ^ christian ( 2020, p. 83 ) ; russell & norvig ( 2021, p. 997 ) ^ christian ( 2020 ), p. 91. ^ christian ( 2020 ), p. 83. ^ verma ( 2021 ). ^ rothman ( 2020 ). ^ christian ( 2020 ), pp. 105 – 108. ^ christian ( 2020 ), pp. 108 – 112. ^ ropek, lucas ( 21 may 2024 ). "" new anthropic research sheds light on ai ' s ' black box ' "". gizmodo. archived from the original on 5 october 2024. retrieved 23"
q_043,sparse,0.0,0.0,0.0,0.05031446540880503,is a set of digits that can be rearranged in a number of ways,"2, t + 1 ) { \ displaystyle f _ { \ theta _ { 2 } } : ( x _ { 1, t }, h _ { 2, t } ) \ mapsto ( x _ { 2, t }, h _ { 2, t + 1 } ) }.... layer n { \ displaystyle n } has hidden vector h n, t { \ displaystyle h _ { n, t } }, parameters θ n { \ displaystyle \ theta _ { n } }, and maps f θ n : ( x n − 1, t, h n, t ) ↦ ( x n, t, h n, t + 1 ) { \ displaystyle f _ { \ theta _ { n } } : ( x _ { n - 1, t }, h _ { n, t } ) \ mapsto ( x _ { n, t }, h _ { n, t + 1 } ) }. each layer operates as a stand - alone rnn, and each layer ' s output sequence is used as the input sequence to the layer above. there is no conceptual limit to the depth of stacked rnn. bidirectional main article : bidirectional recurrent neural networks bidirectional rnn a bidirectional rnn ( birnn ) is composed of two rnns, one processing the input sequence in one direction, and another in the opposite direction. abstractly,"
q_044,sparse,0.0,0.0,0.0,0.0,christianity,"natural language generation. in 2022, a chatbot based on gpt - 3, chatgpt, became unexpectedly popular, triggering a boom around large language models. since 2020, transformers have been applied in modalities beyond text, including the vision transformer, speech recognition, robotics, and multimodal. the vision transformer, in turn, stimulated new developments in convolutional neural networks. image and video generators like dall - e ( 2021 ), stable diffusion 3 ( 2024 ), and sora ( 2024 ), use transformers to analyse input data ( like text prompts ) by breaking it down into "" tokens "" and then calculating the relevance between each token using self - attention, which helps the model understand the context and relationships within the data. training methods for stabilizing training the plain transformer architecture had difficulty in converging. in the original paper, the authors recommended using learning rate warmup. that is, the learning rate should linearly scale up from 0 to maximal value for the first part of the training ( usually recommended to be 2 % of the total number of training steps ), before decaying again. a 2020 paper found that using layer normalization before ( instead of after ) multihead attention and feedforward layers stabilizes training, not requiring learning rate warmup. this is the "" pre - ln transformer "" and is more commonly used, compared to the original "" post - ln transformer"
q_045,sparse,0.0,0.0,0.0,0.0303030303030303,"a symbiotic relationship between the two cultures.""","##3 - 12 - 29. ^ amodei, dario ; olah, chris ; steinhardt, jacob ; christiano, paul ; schulman, john ; mane, dan ( 2016 - 06 - 21 ). "" concrete problems in ai safety "". arxiv : 1606. 06565 [ cs. ai ]. ^ lyons, jessica ( 2025 - 09 - 26 ). "" prompt injection – and a $ 5 domain – trick salesforce agentforce into leaking sales "". the register. retrieved 2025 - 09 - 26. ^ carlini, nicholas ; tramer, florian ; wallace, eric ( 2021 - 08 - 11 ). "" extracting training data from large language models "" ( pdf ). usenix association. retrieved 2025 - 10 - 02. ^ zhao, yao ; zhang, yun ; sun, yong ( 2023 - 06 - 07 ). "" the debate over understanding in ai ' s large language models "". proceedings of the national academy of sciences. 120 ( 13 ) e2215907120. arxiv : 2306. 05499. bibcode : 2023pnas.. 12015907m. doi : 10. 1073 / pnas. 2215907120. pmc 10068812. pmid 36943882. ^ buolamwini, joy ; gebru, timnit ( 2018 - 01 - 01 )"
q_046,sparse,0.0,0.0,0.0,0.021739130434782608,the golden age of detective fiction,"} of type ( x t, h t ) ↦ ( y t, h t + 1 ) { \ displaystyle ( x _ { t }, h _ { t } ) \ mapsto ( y _ { t }, h _ { t + 1 } ) }, where x t { \ displaystyle x _ { t } } : input vector ; h t { \ displaystyle h _ { t } } : hidden vector ; y t { \ displaystyle y _ { t } } : output vector ; θ { \ displaystyle \ theta } : neural network parameters. in words, it is a neural network that maps an input x t { \ displaystyle x _ { t } } into an output y t { \ displaystyle y _ { t } }, with the hidden vector h t { \ displaystyle h _ { t } } playing the role of "" memory "", a partial record of all previous input - output pairs. at each step, it transforms input to an output, and modifies its "" memory "" to help it to better perform future processing. the illustration to the right may be misleading to many because practical neural network topologies are frequently organized in "" layers "" and the drawing gives that appearance. however, what appears to be layers are, in fact, different steps in time, "" unfolded "" to produce the appearance of layers. stacked rnn stacked rnn a stacked rnn, or deep rnn, is composed of multiple rnns"
q_047,sparse,0.0,0.0,0.0,0.06792452830188679,"israeli innovations that made possible google suggest, the capsule endoscopy, a miniature camera embedded in a pill so that 18 photos per second can be wirelessly and painlessly transmi",", however, underestimated the difficulty of the problem. in 1974, both the u. s. and british governments cut off exploratory research in response to the criticism of sir james lighthill and ongoing pressure from the u. s. congress to fund more productive projects. minsky and papert ' s book perceptrons was understood as proving that artificial neural networks would never be useful for solving real - world tasks, thus discrediting the approach altogether. the "" ai winter "", a period when obtaining funding for ai projects was difficult, followed. in the early 1980s, ai research was revived by the commercial success of expert systems, a form of ai program that simulated the knowledge and analytical skills of human experts. by 1985, the market for ai had reached over a billion dollars. at the same time, japan ' s fifth generation computer project inspired the u. s. and british governments to restore funding for academic research. however, beginning with the collapse of the lisp machine market in 1987, ai once again fell into disrepute, and a second, longer - lasting winter began. up to this point, most of ai ' s funding had gone to projects that used high - level symbols to represent mental objects like plans, goals, beliefs, and known facts. in the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition, and began to look"
q_048,sparse,0.0,0.0,0.0,0.0,whadja do today?,"346238. pmid 6953413. ^ hopfield, j. j. ( 1984 ). "" neurons with graded response have collective computational properties like those of two - state neurons "". proceedings of the national academy of sciences. 81 ( 10 ) : 3088 – 3092. bibcode : 1984pnas... 81. 3088h. doi : 10. 1073 / pnas. 81. 10. 3088. pmc 345226. pmid 6587342. ^ engel, a. ; broeck, c. van den ( 2001 ). statistical mechanics of learning. cambridge, uk ; new york, ny : cambridge university press. isbn 978 - 0 - 521 - 77307 - 2. ^ seung, h. s. ; sompolinsky, h. ; tishby, n. ( 1992 - 04 - 01 ). "" statistical mechanics of learning from examples "". physical review a. 45 ( 8 ) : 6056 – 6091. bibcode : 1992phrva.. 45. 6056s. doi : 10. 1103 / physreva. 45. 6056. pmid 9907706. ^ zhang, aston ; lipton, zachary ; li, mu ; smola, alexander j. ( 2024 ). "" 10. modern recurrent neural networks "". dive into deep learning. cambridge new york port melbourne new delhi singapore : cambridge"
q_049,sparse,0.0,0.0,0.0,0.0,liber novus,"from the original on 26 march 2023. retrieved 15 january 2023. ^ varshney, neeraj ; yao, wenlin ; zhang, hongming ; chen, jianshu ; yu, dong ( 2023 ). "" a stitch in time saves nine : detecting and mitigating hallucinations of llms by validating low - confidence generation "". arxiv : 2307. 03987 [ cs. cl ]. ^ lin, belle ( 2025 - 02 - 05 ). "" why amazon is betting on ' automated reasoning ' to reduce ai ' s hallucinations : the tech giant says an obscure field that combines ai and math can mitigate — but not completely eliminate — ai ' s propensity to provide wrong answers "". wall street journal. issn 0099 - 9660. ^ lakoff, george ( 1999 ). philosophy in the flesh : the embodied mind and its challenge to western philosophy ; appendix : the neural theory of language paradigm. new york basic books. pp. 569 – 583. isbn 978 - 0 - 465 - 05674 - 3. ^ evans, vyvyan. ( 2014 ). the language myth. cambridge university press. isbn 978 - 1 - 107 - 04396 - 1. ^ friston, karl j. ( 2022 ). active inference : the free energy principle in mind, brain, and behavior ; chapter 4 the generative models of active inference."
q_050,sparse,0.0,0.0,0.0,0.029629629629629627,a colony of desulforudis,"/ pnas. 2215907120. pmc 10068812. pmid 36943882. ^ buolamwini, joy ; gebru, timnit ( 2018 - 01 - 01 ). "" gender shades : intersectional accuracy disparities in commercial gender classification "" ( pdf ). proceedings of machine learning research ( fat * ). retrieved 2025 - 10 - 02. ^ yang, kaiqi ( 2024 - 11 - 01 ). "" unpacking political bias in large language models : a cross - model comparison on u. s. politics "". arxiv : 2412. 16746 [ cs. cy ]. ^ strubell, emma ; ganesh, ananya ; mccallum, andrew ( 2019 - 07 - 28 ). "" energy and policy considerations for deep learning in nlp "" ( pdf ). acl anthology. retrieved 2025 - 10 - 02. ^ he, yuhao ; yang, li ; qian, chunlian ; li, tong ; su, zhengyuan ; zhang, qiang ; hou, xiangqing ( 2023 - 04 - 28 ). "" conversational agent interventions for mental health problems : systematic review and meta - analysis of randomized controlled trials "". journal of medical internet research. 25 e43862. doi : 10. 2196 / 43862. pmc 10182468. pmid 37115595."
q_051,sparse,0.0,0.0,0.0,0.0,continuity,"thousands of jobs in banking, financial planning, and pension advice in the process, but i ' m not sure it will unleash a new wave of [ e. g., sophisticated ] pension innovation. "" military main article : military applications of artificial intelligence various countries are deploying ai military applications. the main applications enhance command and control, communications, sensors, integration and interoperability. research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. ai technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed joint fires between networked combat vehicles, both human - operated and autonomous. ai has been used in military operations in iraq, syria, israel and ukraine. generative ai vincent van gogh in watercolour created by generative ai software these paragraphs are an excerpt from generative artificial intelligence. [ edit ] generative artificial intelligence, also known as generative ai or genai, is a subfield of artificial intelligence that uses generative models to generate text, images, videos, audio, software code or other forms of data. these models learn the underlying patterns and structures of their training data and use them to generate new data in response to input, which often takes the form of natural language prompts. the prevalence of generative ai tools has increased significantly since the ai boom in the 2020s. this boom was"
q_052,sparse,0.0,0.0,0.0,0.008547008547008548,the gods,"fuzzy logic assigns a "" degree of truth "" between 0 and 1. it can therefore handle propositions that are vague and partially true. non - monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning. other specialized versions of logic have been developed to describe many complex domains. probabilistic methods for uncertain reasoning a simple bayesian network, with the associated conditional probability tables many problems in ai ( including reasoning, planning, learning, perception, and robotics ) require the agent to operate with incomplete or uncertain information. ai researchers have devised a number of tools to solve these problems using methods from probability theory and economics. precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis, and information value theory. these tools include models such as markov decision processes, dynamic decision networks, game theory and mechanism design. bayesian networks are a tool that can be used for reasoning ( using the bayesian inference algorithm ), learning ( using the expectation – maximization algorithm ), planning ( using decision networks ) and perception ( using dynamic bayesian networks ). probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time ( e. g., hidden markov models or kalman filters ). expectation – maximization clustering of old faithful eruption data starts from a random guess but then successfully converge"
q_053,sparse,0.0,0.0,0.0,0.0,jesus,", paul ; chung, hyung won ; sutton, charles ; gehrmann, sebastian ; schuh, parker ; shi, kensen ; tsvyashchenko, sasha ; maynez, joshua ; rao, abhishek ( 2022 - 04 - 01 ). "" palm : scaling language modeling with pathways "". arxiv : 2204. 02311 [ cs. cl ]. ^ ainslie, joshua ; lee - thorp, james ; de jong, michiel ; zemlyanskiy, yury ; lebron, federico ; sanghai, sumit ( 2023 - 12 - 23 ), gqa : training generalized multi - query transformer models from multi - head checkpoints, arxiv : 2305. 13245 ^ a b deepseek - ai ; liu, aixin ; feng, bei ; wang, bin ; wang, bingxuan ; liu, bo ; zhao, chenggang ; dengr, chengqi ; ruan, chong ( 19 june 2024 ), deepseek - v2 : a strong, economical, and efficient mixture - of - experts language model, arxiv : 2405. 04434. ^ a b leviathan, yaniv ; kalman, matan ; matias, yossi ( 2023 - 05 - 18 ), fast inference from transformers via speculative decoding, arxiv : 2211. 17192 ^ fu, yao ( 202"
q_054,sparse,0.0,0.0,0.0,0.0,ed gillespie,"( 1950 ). ^ solomonoff ( 1956 ). ^ unsupervised learning : russell & norvig ( 2021, pp. 653 ) ( definition ), russell & norvig ( 2021, pp. 738 – 740 ) ( cluster analysis ), russell & norvig ( 2021, pp. 846 – 860 ) ( word embedding ) ^ a b supervised learning : russell & norvig ( 2021, § 19. 2 ) ( definition ), russell & norvig ( 2021, chpt. 19 – 20 ) ( techniques ) ^ reinforcement learning : russell & norvig ( 2021, chpt. 22 ), luger & stubblefield ( 2004, pp. 442 – 449 ) ^ transfer learning : russell & norvig ( 2021, pp. 281 ), the economist ( 2016 ) ^ "" artificial intelligence ( ai ) : what is ai and how does it work? | built in "". builtin. com. retrieved 30 october 2023. ^ computational learning theory : russell & norvig ( 2021, pp. 672 – 674 ), jordan & mitchell ( 2015 ) ^ natural language processing ( nlp ) : russell & norvig ( 2021, chpt. 23 – 24 ), poole, mackworth & goebel ( 1998, pp. 91 – 104 ), luger & stubblefield ( 2004, pp. 591 – 632 ) ^ subproblems"
q_055,sparse,0.0,0.0,0.0,0.0,apollo,"the georgetown experiment in 1954 involved fully automatic translation of more than sixty russian sentences into english. the authors claimed that within three or five years, machine translation would be a solved problem. however, real progress was much slower, and after the alpac report in 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. little further research in machine translation was conducted in america ( though some research continued elsewhere, such as japan and europe ) until the late 1980s when the first statistical machine translation systems were developed. 1960s : some notably successful natural language processing systems developed in the 1960s were shrdlu, a natural language system working in restricted "" blocks worlds "" with restricted vocabularies, and eliza, a simulation of a rogerian psychotherapy, written by joseph weizenbaum between 1964 and 1966. despite using minimal information about human thought or emotion, eliza was able to produce interactions that appeared human - like. when the "" patient "" exceeded the very small knowledge base, eliza might provide a generic response, for example, responding to "" my head hurts "" with "" why do you say your head hurts? "". ross quillian ' s successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in a computer memory at the time. 1970s : during the 1970s, many programmers began to write "" conceptual ontologies "", which structured real - world information into computer - understandable data."
q_056,sparse,0.0,0.0,0.0,0.0,continuity,"data. researchers target concrete failure modes, including memorization and copyright leakage, security exploits such as prompt injection, algorithmic bias manifesting as stereotyping, dataset selection effects, and political skew, methods for reducing high energy and carbon costs of large - scale training, and measurable cognitive and mental health impacts of conversational agents on users, while engaging empirical and ethical uncertainty about claims of machine sentience, and applying mitigation measures such as dataset curation, input sanitization, model auditing, scalable oversight, and governance frameworks. cbrn and content misuse ai labs treat cbrn defense ( chemical, biological, radiological, and nuclear defense ) and similar topics as high - consequence misuse attempt to apply various techniques to reduce potential harms. some commenters expressed concern over accidental or deliberate creation of misinformation, or other forms of misuse. for example, the availability of large language models could reduce the skill - level required to commit bioterrorism ; biosecurity researcher kevin esvelt has suggested that llm creators should exclude from their training data papers on creating or enhancing pathogens. content filtering llm applications accessible to the public, like chatgpt or claude, typically incorporate safety measures designed to filter out harmful content. however, implementing these controls effectively has proven challenging. for instance, a 2023 study proposed a method for circumventing llm safety systems. in 2025, the american sunlight"
q_057,sparse,0.0,0.0,0.0,0.0,jesus 's life,"both of which scales as o ( n 2 ) { \ displaystyle o ( n ^ { 2 } ) } where n { \ displaystyle n } is the number of tokens in a sequence. reformer ( 2020 ) reduces the computational load from o ( n 2 ) { \ displaystyle o ( n ^ { 2 } ) } to o ( n ln n ) { \ displaystyle o ( n \ ln n ) } by using locality - sensitive hashing and reversible layers. sparse attention uses attention graphs that grows slower than o ( n 2 ) { \ displaystyle o ( n ^ { 2 } ) }. for example, bigbird ( 2020 ) uses random small - world networks which grows as o ( n ) { \ displaystyle o ( n ) }. ordinary transformers require a memory size that is quadratic in the size of the context window. attention - free transformers reduce this to a linear dependence while still retaining the advantages of a transformer by linking the key to the value. random feature attention random feature attention ( 2021 ) uses fourier random features : φ ( x ) = 1 d [ cos ⟨ w 1, x ⟩, sin ⟨ w 1, x ⟩, [UNK] cos ⟨ w d, x ⟩, sin ⟨ w d, x ⟩ ] t { \ displaystyle \ varphi ( x ) = { \ frac { 1 } { \ sqrt { d } } } [ \ cos \ langle w _"
q_058,sparse,0.0,0.0,0.0,0.0,continuity,"the ai boom. generative ai ' s ability to create and modify content has led to several unintended consequences and harms. ethical concerns have been raised about ai ' s long - term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology. goals the general problem of simulating ( or creating ) intelligence has been broken into subproblems. these consist of particular traits or capabilities that researchers expect an intelligent system to display. the traits described below have received the most attention and cover the scope of ai research. reasoning and problem - solving early researchers developed algorithms that imitated step - by - step reasoning that humans use when they solve puzzles or make logical deductions. by the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics. many of these algorithms are insufficient for solving large reasoning problems because they experience a "" combinatorial explosion "" : they become exponentially slower as the problems grow. even humans rarely use the step - by - step deduction that early ai research could model. they solve most of their problems using fast, intuitive judgments. accurate and efficient reasoning is an unsolved problem. knowledge representation an ontology represents knowledge as a set of concepts within a domain and the relationships between those concepts. knowledge representation and knowledge engineering allow ai programs to answer questions intelligently and make deductions about real - world facts. formal knowledge representations are used in content -"
q_059,sparse,0.0,0.0,0.0,0.05,a bowsprit and aggressive lines betray the sporty genes of this joubert - nivelt design that promises high performance,"##er & schmidhuber ( 2012 ). ^ russell & norvig ( 2021 ), p. 750. ^ a b c russell & norvig ( 2021 ), p. 17. ^ a b c d e f g russell & norvig ( 2021 ), p. 785. ^ a b schmidhuber ( 2022 ), sect. 5. ^ schmidhuber ( 2022 ), sect. 6. ^ a b c schmidhuber ( 2022 ), sect. 7. ^ schmidhuber ( 2022 ), sect. 8. ^ quoted in christian ( 2020, p. 22 ) ^ metz, cade ; weise, karen ( 5 may 2025 ). "" a. i. hallucinations are getting worse, even as new systems become more powerful "". the new york times. issn 0362 - 4331. retrieved 6 may 2025. ^ smith ( 2023 ). ^ "" explained : generative ai "". mit news | massachusetts institute of technology. 9 november 2023. ^ "" ai writing and content creation tools "". mit sloan teaching & learning technologies. archived from the original on 25 december 2023. retrieved 25 december 2023. ^ marmouyet ( 2023 ). ^ kobielus ( 2019 ). ^ thomason, james ( 21 may 2024 ). "" mojo rising : the resurgence of ai - first programming languages "". venturebeat."
q_060,sparse,0.0,0.0,0.0,0.0,liber novus,"##elbach, sven ( 2022 ). "" pre - trained language models "". foundation models for natural language processing. artificial intelligence : foundations, theory, and algorithms. pp. 19 – 78. doi : 10. 1007 / 978 - 3 - 031 - 23190 - 2 _ 2. isbn 978 - 3 - 031 - 23190 - 2. ^ dodge, jesse ; sap, maarten ; marasovic, ana ; agnew, william ; ilharco, gabriel ; groeneveld, dirk ; mitchell, margaret ; gardner, matt ( 2021 ). "" documenting large webtext corpora : a case study on the colossal clean crawled corpus "" ( pdf ). emnlp. arxiv : 2104. 08758. doi : 10. 1145 / 3571730. ^ lee, katherine ; ippolito, daphne ; nystrom, andrew ; zhang, chiyuan ; eck, douglas ; callison - burch, chris ; carlini, nicholas ( may 2022 ). "" deduplicating training data makes language models better "" ( pdf ). proceedings of the 60th annual meeting of the association for computational linguistics ( volume 1 : long papers ). pp. 8424 – 8445. doi : 10. 18653 / v1 / 2022. acl - long. 577. ^ li, yuanzhi ; bubeck, sebastien ; eldan, ronen ; del giorno"
q_061,sparse,0.0,0.0,0.0,0.04371584699453552,jung ' s struggle to regain his soul and overcome the contemporary malaise of spiritual alienation,"norvig : "" stong ai – the assertion that machines that do so are actually thinking ( as opposed to simulating thinking ). "" references ^ a b c russell & norvig ( 2021 ), pp. 1 – 4. ^ ai set to exceed human brain power archived 19 february 2008 at the wayback machine cnn. com ( 26 july 2006 ) ^ kaplan, andreas ; haenlein, michael ( 2019 ). "" siri, siri, in my hand : who ' s the fairest in the land? on the interpretations, illustrations, and implications of artificial intelligence "". business horizons. 62 : 15 – 25. doi : 10. 1016 / j. bushor. 2018. 08. 004. [ the question of the source is a pastiche of : snow white ] ^ russell & norvig ( 2021, § 1. 2 ). ^ "" tech companies want to build artificial general intelligence. but who decides when agi is attained? "". ap news. 4 april 2024. retrieved 20 may 2025. ^ a b dartmouth workshop : russell & norvig ( 2021, p. 18 ), mccorduck ( 2004, pp. 111 – 136 ), nrc ( 1999, pp. 200 – 201 ) the proposal : mccarthy et al. ( 1955 ) ^ a b successful programs of the 1960s : mccorduck ( 2004, pp. 243 – 252 ), crevier ( 1993, pp. 52 –"
q_062,sparse,0.0,0.0,0.0,0.0,religion,"ai companies. philosophy main article : philosophy of artificial intelligence philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. another major focus has been whether machines can be conscious, and the associated ethical implications. many other topics in philosophy are relevant to ai, such as epistemology and free will. for example, debates center on whether machines can genuinely understand meaning, whether they possess autonomous decision - making capabilities, and to what extent their actions can be considered intentional rather than merely the result of algorithmic processes. rapid advancements have intensified public discussions on the philosophy and ethics of ai. defining artificial intelligence see also : synthetic intelligence, intelligent agent, artificial mind, virtual intelligence, and dartmouth workshop alan turing wrote in 1950 "" i propose to consider the question ' can machines think '? "" he advised changing the question from whether a machine "" thinks "", to "" whether or not it is possible for machinery to show intelligent behaviour "". he devised the turing test, which measures the ability of a machine to simulate human conversation. since we can only observe the behavior of the machine, it does not matter if it is "" actually "" thinking or literally has a "" mind "". turing notes that we can not determine these things about other people but "" it is usual to have a polite convention that everyone thinks. "" the turing test can provide some evidence of intelligence, but it penalizes non - human intelligent behavior. russell and norvig agree with turing that intelligence must be defined in"
q_063,sparse,0.0,0.0,0.0,0.0,chiquita,"11 - 17. retrieved 2023 - 03 - 14. ^ fathallah, nadeen ; das, arunav ; de giorgis, stefano ; poltronieri, andrea ; haase, peter ; kovriguina, liubov ( 2024 - 05 - 26 ). neon - gpt : a large language model - powered pipeline for ontology learning ( pdf ). extended semantic web conference 2024. hersonissos, greece. ^ manning, christopher d. ( 2022 ). "" human language understanding & reasoning "". daedalus. 151 ( 2 ) : 127 – 138. doi : 10. 1162 / daed _ a _ 01905. s2cid 248377870. archived from the original on 2023 - 11 - 17. retrieved 2023 - 03 - 09. ^ kaplan, jared ; mccandlish, sam ; henighan, tom ; brown, tom b. ; chess, benjamin ; child, rewon ; gray, scott ; radford, alec ; wu, jeffrey ; amodei, dario ( 2020 ). "" scaling laws for neural language models "". arxiv : 2001. 08361 [ cs. lg ]. ^ vaswani, ashish ; shazeer, noam ; parmar, niki ; uszkoreit, jakob ; jones, llion ; gomez, aidan n ; kaiser, łukasz ; polosukhin"
q_064,sparse,0.0,0.0,0.0,0.0,continuity,"for artificial intelligence and cryptocurrency. the report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole japanese nation. prodigious power consumption by ai is responsible for the growth of fossil fuel use, and might delay closings of obsolete, carbon - emitting coal energy facilities. there is a feverish rise in the construction of data centers throughout the us, making large technology firms ( e. g., microsoft, meta, google, amazon ) into voracious consumers of electric power. projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. a chatgpt search involves the use of 10 times the electrical energy as a google search. the large firms are in haste to find power sources – from nuclear energy to geothermal to fusion. the tech firms argue that – in the long view – ai will be eventually kinder to the environment, but they need the energy now. ai makes the power grid more efficient and "" intelligent "", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms. a 2024 goldman sachs research paper, ai data centers and the coming us power demand surge, found "" us power demand ( is ) likely to experience growth not seen in a generation.... "" and forecasts that, by 2030, us data centers will consume 8 % of us power, as opposed to"
q_065,sparse,0.0,0.0,0.0,0.0,b - mode,"} } \ right ) v \ end { aligned } } } the following matrix is commonly used in decoder self - attention modules, called "" causal masking "" : m causal = [ 0 − ∞ − ∞ … − ∞ 0 0 − ∞ … − ∞ 0 0 0 … − ∞ [UNK] [UNK] [UNK] [UNK] [UNK] 0 0 0 … 0 ] { \ displaystyle m _ { \ text { causal } } = { \ begin { bmatrix } 0 & - \ infty & - \ infty & \ dots & - \ infty \ \ 0 & 0 & - \ infty & \ dots & - \ infty \ \ 0 & 0 & 0 & \ dots & - \ infty \ \ \ vdots & \ vdots & \ vdots & \ ddots & \ vdots \ \ 0 & 0 & 0 & \ dots & 0 \ end { bmatrix } } } in words, it means that each token can pay attention to itself, and every token before it, but not any after it. a non - masked attention module can be thought of as a masked attention module where the mask has all entries zero. as an example of an uncommon use of mask matrix, the xlnet considers all masks of the form p m causal p − 1 { \ displaystyle pm _ { \ text { causal } } p ^ { - 1 } }, where p { \ displaystyle p }"
q_066,sparse,0.0,0.0,0.0,0.0,sarah green,"' s neuroscience to the birth of cybernetics "". the neuroscientist. 31 ( 1 ) : 14 – 30. doi : 10. 1177 / 10738584231179932. hdl : 10261 / 348372. issn 1073 - 8584. pmid 37403768. ^ ramon y cajal, santiago ( 1909 ). histologie du systeme nerveux de l ' homme & des vertebres. vol. ii. foyle special collections library king ' s college london. paris : a. maloine. p. 149. ^ de no, r. lorente ( 1933 - 08 - 01 ). "" vestibulo - ocular reflex arc "". archives of neurology and psychiatry. 30 ( 2 ) : 245. doi : 10. 1001 / archneurpsyc. 1933. 02240140009001. issn 0096 - 6754. ^ larriva - sahd, jorge a. ( 2014 - 12 - 03 ). "" some predictions of rafael lorente de no 80 years later "". frontiers in neuroanatomy. 8 : 147. doi : 10. 3389 / fnana. 2014. 00147. issn 1662 - 5129. pmc 4253658. pmid 25520630. ^ "" reverberating circuit "". oxford reference. retrieved 2024 - 07 -"
q_067,sparse,0.0,0.0,0.0,0.0,entitl,"). "" robot rights violate human rights, experts warn eu "". euronews. archived from the original on 19 september 2024. retrieved 23 february 2024. ^ the intelligence explosion and technological singularity : russell & norvig ( 2021, pp. 1004 – 1005 ), omohundro ( 2008 ), kurzweil ( 2005 ) i. j. good ' s "" intelligence explosion "" : good ( 1965 ) vernor vinge ' s "" singularity "" : vinge ( 1993 ) ^ russell & norvig ( 2021 ), p. 1005. ^ transhumanism : moravec ( 1988 ), kurzweil ( 2005 ), russell & norvig ( 2021, p. 1005 ) ^ ai as evolution : edward fredkin is quoted in mccorduck ( 2004, p. 401 ), butler ( 1863 ), dyson ( 1998 ) ^ ai in myth : mccorduck ( 2004, pp. 4 – 5 ) ^ mccorduck ( 2004 ), pp. 340 – 400. ^ buttazzo ( 2001 ). ^ anderson ( 2008 ). ^ mccauley ( 2007 ). ^ galvan ( 1997 ). textbooks luger, george ; stubblefield, william ( 2004 ). artificial intelligence : structures and strategies for complex problem solving ( 5th ed. ). benjamin / cummings. isbn 978 - 0 - 8053 - 4780 - 7. archived from the original"
q_068,sparse,0.0,0.0,0.0,0.015325670498084292," "" working groups "". 2015. fcforum. net. retrieved 2015 - 12 - 06.  "" "" el free culture forum no es resigna a perdre la neutralitat a internet | btvn","much of what people know is not represented as "" facts "" or "" statements "" that they could express verbally ). there is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for ai applications. planning and decision - making an "" agent "" is anything that perceives and takes actions in the world. a rational agent has goals or preferences and takes actions to make them happen. in automated planning, the agent has a specific goal. in automated decision - making, the agent has preferences — there are some situations it would prefer to be in, and some situations it is trying to avoid. the decision - making agent assigns a number to each situation ( called the "" utility "" ) that measures how much the agent prefers it. for each possible action, it can calculate the "" expected utility "" : the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. it can then choose the action with the maximum expected utility. in classical planning, the agent knows exactly what the effect of any action will be. in most real - world problems, however, the agent may not be certain about the situation they are in ( it is "" unknown "" or "" unobservable "" ) and it may not know for certain what will happen after each possible action ( it is not "" deterministic "" ). it must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked. in some problems"
q_069,sparse,0.0,0.0,0.0,0.0,continuity,"such as llama 2, mistral or stable diffusion, have been made open - weight, meaning that their architecture and trained parameters ( the "" weights "" ) are publicly available. open - weight models can be freely fine - tuned, which allows companies to specialize them with their own data and for their own use - case. open - weight models are useful for research and innovation but can also be misused. since they can be fine - tuned, any built - in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. some researchers warn that future ai models may develop dangerous capabilities ( such as the potential to drastically facilitate bioterrorism ) and that once released on the internet, they cannot be deleted everywhere if needed. they recommend pre - release audits and cost - benefit analyses. frameworks artificial intelligence projects can be guided by ethical considerations during the design, development, and implementation of an ai system. an ai framework such as the care and act framework, developed by the alan turing institute and based on the sum values, outlines four main ethical dimensions, defined as follows : respect the dignity of individual people connect with other people sincerely, openly, and inclusively care for the wellbeing of everyone protect social values, justice, and the public interest other developments in ethical frameworks include those decided upon during the asilomar conference, the montreal declaration for responsible ai, and the ieee ' s ethics of autonomous systems initiative, among others ; however"
q_070,sparse,0.0,0.0,0.0,0.0,bingley,". reuters. 2025 - 06 - 25. retrieved 2025 - 06 - 26. ^ "" meta scores victory in ai copyright case "". wired. 2025 - 06 - 25. retrieved 2025 - 06 - 26. ^ "" openai defeats news outlets ' copyright lawsuit over ai training for now "". reuters. 2024 - 11 - 07. retrieved 2024 - 11 - 08. ^ "" openai erases evidence in training data lawsuit "". the verge. 2024 - 11 - 21. retrieved 2024 - 11 - 22. ^ peng, zhencan ; wang, zhizhi ; deng, dong ( 13 june 2023 ). "" near - duplicate sequence search at scale for large language model memorization evaluation "" ( pdf ). proceedings of the acm on management of data. 1 ( 2 ) : 1 – 18. doi : 10. 1145 / 3589324. s2cid 259213212. archived ( pdf ) from the original on 2024 - 08 - 27. retrieved 2024 - 01 - 20. citing lee et al 2022. ^ peng, wang & deng 2023, p. 8. ^ stephen council ( 1 dec 2023 ). "" how googlers cracked an sf rival ' s tech model with a single word "". sfgate. archived from the original on 16 december 2023. ^ "" prepare for truly useful large language models "". nature biomedical engineering. 7 ( 2 ) : 85"
q_071,sparse,0.0,0.0,0.0,0.046948356807511735,"wilf et al. ( 2007 ) proved that for any permutation  and any positive integer m, the permutations 12... m   and m... 21   are wilf - equivalent.","y } } _ { 2 }, \ dots, { \ hat { y } } _ { l } ) }. the problem is that if the model makes a mistake early on, say at y ^ 2 { \ displaystyle { \ hat { y } } _ { 2 } }, then subsequent tokens are likely to also be mistakes. this makes it inefficient for the model to obtain a learning signal, since the model would mostly learn to shift y ^ 2 { \ displaystyle { \ hat { y } } _ { 2 } } towards y 2 { \ displaystyle y _ { 2 } }, but not the others. teacher forcing makes it so that the decoder uses the correct output sequence for generating the next entry in the sequence. so for example, it would see ( y 1, …, y k ) { \ displaystyle ( y _ { 1 }, \ dots, y _ { k } ) } in order to generate y ^ k + 1 { \ displaystyle { \ hat { y } } _ { k + 1 } }. gradient descent main articles : gradient descent and vanishing gradient problem gradient descent is a first - order iterative optimization algorithm for finding the minimum of a function. in neural networks, it can be used to minimize the error term by changing each weight in proportion to the derivative of the error with respect to that weight, provided the non - linear activation functions are differentiable. the standard method for training rn"
q_072,sparse,0.0,0.0,0.0,0.00881057268722467,jesus 's life,"agreeability observed across multi - turn interactions and productized assistants. continued sycophancy has led to the observation of getting "" 1 - shotted "", denoting instances where conversational interaction with a large language model produces a lasting change in a user ' s beliefs or decisions, similar to the negative effects of psychedelics, and controlled experiments show that short llm dialogues can generate measurable opinion and confidence shifts comparable to human interlocutors. empirical analyses attribute part of the effect to human preference signals and preference models that reward convincingly written agreeable responses, and subsequent work has extended evaluation to multi - turn benchmarks and proposed interventions such as synthetic - data finetuning, adversarial evaluation, targeted preference - model reweighting, and multi - turn sycophancy benchmarks to measure persistence and regression risk. industry responses have combined research interventions with product controls, for example google and other labs publishing synthetic - data and fine - tuning interventions and openai rolling back an overly agreeable gpt - 4o update while publicly describing changes to feedback collection, personalization controls, and evaluation procedures to reduce regression risk and improve long - term alignment with user - level safety objectives. mainstream culture has reflected anxieties about this dynamic where south park satirized overreliance on chatgpt and the tendency of assistants to flatter user beliefs in season 27 episode "" sickofancy "", and continued the themes across the following season, which commentators interpreted as a critique of"
q_073,sparse,0.0,0.0,0.0,0.0,""" short "" hundred ( 100 )","hp labs describes a system of cortical computing with memristive nanodevices. the memristors ( memory resistors ) are implemented by thin film materials in which the resistance is electrically tuned via the transport of ions or oxygen vacancies within the film. darpa ' s synapse project has funded ibm research and hp labs, in collaboration with the boston university department of cognitive and neural systems ( cns ), to develop neuromorphic architectures that may be based on memristive systems. memristive networks are a particular type of physical neural network that have very similar properties to ( little - ) hopfield networks, as they have continuous dynamics, a limited memory capacity and natural relaxation via the minimization of a function which is asymptotic to the ising model. in this sense, the dynamics of a memristive circuit have the advantage compared to a resistor - capacitor network to have a more interesting non - linear behavior. from this point of view, engineering analog memristive networks account for a peculiar type of neuromorphic engineering in which the device behavior depends on the circuit wiring or topology. the evolution of these networks can be studied analytically using variations of the caravelli - traversa - di ventra equation. continuous - time a continuous - time recurrent neural network ( ctrnn ) uses a system of ordinary differential equations to model the effects on a neuron of the incoming inputs"
q_074,sparse,0.0,0.0,0.0,0.038314176245210725,jung ' s struggle to regain his soul and overcome the contemporary malaise of spiritual alienation,"one of which is their ambiguity. several works use ai to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. this appears in karel capek ' s r. u. r., the films a. i. artificial intelligence and ex machina, as well as the novel do androids dream of electric sheep?, by philip k. dick. dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence. see also artificial consciousness – field in cognitive science artificial intelligence and elections – impact of ai on political elections artificial intelligence content detection – software to detect ai - generated content artificial intelligence in wikimedia projects – use of artificial intelligence to develop wikipedia and other wikimedia projects association for the advancement of artificial intelligence ( aaai ) behavior selection algorithm – algorithm that selects actions for intelligent agents business process automation – automation of business processes case - based reasoning – process of solving new problems based on the solutions of similar past problems computational intelligence – ability of a computer to learn a specific task from data or experimental observation darwin eu – a european union initiative coordinated by the european medicines agency ( ema ) to generate and utilize real world evidence ( rwe ) to support the evaluation and supervision of medicines across the eu digital immortality – hypothetical concept of storing a personality in digital form emergent algorithm – algorithm exhibiting emergent behavior female gendering of ai technologies – gender biases in"
q_075,sparse,0.0,0.0,0.0,0.0,svyataya,", johannes ; horvitz, eric ; kamar, ece ; lee, peter ; lee, yin tat ; li, yuanzhi ; lundberg, scott ; nori, harsha ; palangi, hamid ; ribeiro, marco tulio ; zhang, yi ( 2023 ). "" machine culture "". nature human behaviour. 7 ( 11 ) : 1855 – 1868. arxiv : 2303. 12712. doi : 10. 1038 / s41562 - 023 - 01742 - 2. pmid 37985914. ^ "" anthropic ceo dario amodei pens a smart look at our ai future "". fast company. october 17, 2024. ^ "" chatgpt is more like an ' alien intelligence ' than a human brain, says futurist "". zdnet. 2023. archived from the original on 12 june 2023. retrieved 12 june 2023. ^ a b newport, cal ( 13 april 2023 ). "" what kind of mind does chatgpt have? "". the new yorker. archived from the original on 12 june 2023. retrieved 12 june 2023. ^ roose, kevin ( 30 may 2023 ). "" why an octopus - like creature has come to symbolize the state of a. i. "" the new york times. archived from the original on 30 may 2023. retrieved 12 june 2023. ^ "" the a to"
q_076,sparse,0.0,0.0,0.0,0.0,continuity,"math benchmark problems. alternatively, dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such as alphatensor, alphageometry, alphaproof and alphaevolve all from google deepmind, llemma from eleutherai or julius. when natural language is used to describe mathematical problems, converters can transform such prompts into a formal language such as lean to define mathematical tasks. the experimental model gemini deep think accepts natural language prompts directly and achieved gold medal results in the international math olympiad of 2025. some models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics. topological deep learning integrates various topological approaches. finance finance is one of the fastest growing sectors where applied ai tools are being deployed : from retail online banking to investment advice and insurance, where automated "" robot advisers "" have been in use for some years. according to nicolas firzli, director of the world pensions & investments forum, it may be too early to see the emergence of highly innovative ai - informed financial products and services. he argues that "" the deployment of ai tools will simply further automatise things : destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but i ' m not sure it will unleash a new wave of [ e. g., sophisticated ] pension innovation. "" military main article : military applications of"
q_077,sparse,0.0,0.0,0.0,0.021978021978021976,a nancy honey is a british photographer,"- video, language translation, more "". venturebeat. 2022 - 11 - 02. retrieved 2022 - 11 - 09. ^ vincent, james ( 2022 - 09 - 29 ). "" meta ' s new text - to - video ai generator is like dall - e for video "". the verge. retrieved 2022 - 11 - 09. ^ "" previous shared tasks | conll "". www. conll. org. retrieved 2021 - 01 - 11. ^ "" cognition "". lexico. oxford university press and dictionary. com. archived from the original on july 15, 2020. retrieved 6 may 2020. ^ "" ask the cognitive scientist "". american federation of teachers. 8 august 2014. cognitive science is an interdisciplinary field of researchers from linguistics, psychology, neuroscience, philosophy, computer science, and anthropology that seek to understand the mind. ^ robinson, peter ( 2008 ). handbook of cognitive linguistics and second language acquisition. routledge. pp. 3 – 8. isbn 978 - 0 - 805 - 85352 - 0. ^ lakoff, george ( 1999 ). philosophy in the flesh : the embodied mind and its challenge to western philosophy ; appendix : the neural theory of language paradigm. new york basic books. pp. 569 – 583. isbn 978 - 0 - 465 - 05674 - 3. ^ strauss, claudia ( 1999 ). a cognitive theory of cultural meaning. cambridge university press. pp. 156 – 164. isbn"
q_078,sparse,0.0,0.0,0.0,0.0,continuity,"main article : prompt engineering in 2020, openai researchers demonstrated that their new model gpt - 3 could understand what format to use given a few rounds of q and a ( or other type of task ) in the input data as example, thanks in part due to the rlhf technique. this technique, called few - shot prompting, allows llms to be adapted to any task without requiring fine - tuning. also in 2022, it was found that the base gpt - 3 model can generate an instruction based on user input. the generated instruction along with user input is then used as input to another instance of the model under a "" instruction : [... ], input : [... ], output : "" format. the other instance is able to complete the output and often produces the correct answer in doing so. the ability to "" self - instruct "" makes llms able to bootstrap themselves toward a correct answer. dialogue processing ( chatbot ) an llm can be turned into a chatbot by specializing it for conversation. user input is prefixed with a marker such as "" q : "" or "" user : "" and the llm is asked to predict the output after a fixed "" a : "" or "" assistant : "". this type of model became commercially available in 2022 with chatgpt, a sibling model of instructgpt fine - tuned to accept and produce dialog - formatted text based on gpt - 3."
q_079,sparse,0.0,0.0,0.0,0.02654867256637168,four door sedan and a four door sedan,"analysis, coreference ; see natural language understanding below ). semantic role labelling ( see also implicit semantic role labelling below ) given a single sentence, identify and disambiguate semantic predicates ( e. g., verbal frames ), then identify and classify the frame elements ( semantic roles ). discourse ( semantics beyond individual sentences ) coreference resolution given a sentence or larger chunk of text, determine which words ( "" mentions "" ) refer to the same objects ( "" entities "" ). anaphora resolution is a specific example of this task, and is specifically concerned with matching up pronouns with the nouns or names to which they refer. the more general task of coreference resolution also includes identifying so - called "" bridging relationships "" involving referring expressions. for example, in a sentence such as "" he entered john ' s house through the front door "", "" the front door "" is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of john ' s house ( rather than of some other structure that might also be referred to ). discourse analysis this rubric includes several related tasks. one task is discourse parsing, i. e., identifying the discourse structure of a connected text, i. e. the nature of the discourse relationships between sentences ( e. g. elaboration, explanation, contrast ). another possible task is recognizing and classifying the speech acts in a chunk of"
q_080,sparse,0.0,0.0,0.0,0.05932203389830509,a wise bronze age man or woman meant to learn how to acquire and hold on to power,"learning for the recognition of sequences can also be implemented by a more biological - based model which uses the silencing mechanism exhibited in neurons with a relatively high frequency spiking activity. additional stored states and the storage under direct control by the network can be added to both infinite - impulse and finite - impulse networks. another network or graph can also replace the storage if that incorporates time delays or has feedback loops. such controlled states are referred to as gated states or gated memory and are part of long short - term memory networks ( lstms ) and gated recurrent units. this is also called feedback neural network ( fnn ). libraries modern libraries provide runtime - optimized implementations of the above functionality or allow to speed up the slow loop by just - in - time compilation. apache singa caffe : created by the berkeley vision and learning center ( bvlc ). it supports both cpu and gpu. developed in c + +, and has python and matlab wrappers. chainer : fully in python, production support for cpu, gpu, distributed training. deeplearning4j : deep learning in java and scala on multi - gpu - enabled spark. flux : includes interfaces for rnns, including grus and lstms, written in julia. keras : high - level api, providing a wrapper to many other deep learning libraries. microsoft cognitive toolkit mxnet : an open - source deep learning framework used to train"
q_081,sparse,0.0,0.0,0.0,0.00819672131147541,a dark path.,"##out heuristic. when a programmatic world model is not available, an llm can also be prompted with a description of the environment to act as world model. for open - ended exploration, an llm can be used to score observations for their "" interestingness "", which can be used as a reward signal to guide a normal ( non - llm ) reinforcement learning agent. alternatively, it can propose increasingly difficult tasks for curriculum learning. instead of outputting individual actions, an llm planner can also construct "" skills "", or functions for complex action sequences. the skills can be stored and later invoked, allowing increasing levels of abstraction in planning. multiple agents with memory can interact socially. reasoning llms are conventionally trained to generate an output without generating intermediate steps. as a result, their performance tends to be subpar on complex questions requiring ( at least in humans ) intermediate steps of thought. early research demonstrated that inserting intermediate "" scratchpad "" computations could improve performance on such tasks. later methods overcame this deficiency more systematically by breaking tasks into smaller steps for the llm, either manually or automatically. chaining prompt chaining was introduced in 2022. in this method, a user manually breaks a complex problem down into several steps. in each step, the llm receives as input a prompt telling it what to do and some results from preceding steps. the result from one step is then reused in a next step, until a final answer is reached. the ability of an"
q_082,sparse,0.0,0.0,0.0,0.0130718954248366,christ the redeemer by andrea del sarto part one — who he was ( chapters 1 - 5 ) part two — why he came ( chapters 6 - 11 ) part three — what he left behind ( chapte,"2102. 12092 ^ yu, jiahui ; xu, yuanzhong ; koh, jing yu ; luong, thang ; baid, gunjan ; wang, zirui ; vasudevan, vijay ; ku, alexander ; yang, yinfei ( 2022 - 06 - 21 ), scaling autoregressive models for content - rich text - to - image generation, arxiv : 2206. 10789 ^ kariampuzha, william ; alyea, gioconda ; qu, sue ; sanjak, jaleal ; mathe, ewy ; sid, eric ; chatelaine, haley ; yadaw, arjun ; xu, yanji ; zhu, qian ( 2023 ). "" precision information extraction for rare disease epidemiology at scale "". journal of translational medicine. 21 ( 1 ) : 157. doi : 10. 1186 / s12967 - 023 - 04011 - y. pmc 9972634. pmid 36855134. further reading alexander rush, the annotated transformer archived 2021 - 09 - 22 at the wayback machine, harvard nlp group, 3 april 2018 phuong, mary ; hutter, marcus ( 2022 ). "" formal algorithms for transformers "". arxiv : 2207. 09238 [ cs. lg ]. ferrando, javier ; sarti,"
q_083,sparse,0.0,0.0,0.0,0.0,norman,"s. ). ^ nilsson ( 1983 ), p. 10. ^ haugeland ( 1985 ), pp. 112 – 117. ^ physical symbol system hypothesis : newell & simon ( 1976, p. 116 ) historical significance : mccorduck ( 2004, p. 153 ), russell & norvig ( 2021, p. 19 ) ^ moravec ' s paradox : moravec ( 1988, pp. 15 – 16 ), minsky ( 1986, p. 29 ), pinker ( 2007, pp. 190 – 191 ) ^ dreyfus ' critique of ai : dreyfus ( 1972 ), dreyfus & dreyfus ( 1986 ) historical significance and philosophical implications : crevier ( 1993, pp. 120 – 132 ), mccorduck ( 2004, pp. 211 – 239 ), russell & norvig ( 2021, pp. 981 – 982 ), fearn ( 2007, chpt. 3 ) ^ crevier ( 1993 ), p. 125. ^ langley ( 2011 ). ^ katz ( 2012 ). ^ neats vs. scruffies, the historic debate : mccorduck ( 2004, pp. 421 – 424, 486 – 489 ), crevier ( 1993, p. 168 ), nilsson ( 1983, pp. 10 – 11 ), russell & norvig ( 2021, p. 24 ) a classic example of the "" scruffy "" approach to"
q_084,sparse,0.0,0.0,0.0,0.00980392156862745,2010 buick lacrosse cxs was the firs,"cost of training the model, in flops. n { \ displaystyle n } is the number of parameters in the model. d { \ displaystyle d } is the number of tokens in the training set. l { \ displaystyle l } is the average negative log - likelihood loss per token ( nats / token ), achieved by the trained llm on the test dataset. and the statistical hyper - parameters are c 0 = 6 { \ displaystyle c _ { 0 } = 6 }, meaning that it costs 6 flops per parameter to train on one token. note that training cost is much higher than inference cost, where it costs 1 to 2 flops per parameter to infer on one token. α = 0. 34, β = 0. 28, a = 406. 4, b = 410. 7, l 0 = 1. 69 { \ displaystyle \ alpha = 0. 34, \ beta = 0. 28, a = 406. 4, b = 410. 7, l _ { 0 } = 1. 69 } emergent abilities at point ( s ) referred to as breaks, the lines change their slopes, appearing on a linear - log plot as a series of linear segments connected by arcs. performance of bigger models on various tasks, when plotted on a log - log scale, appears as a linear extrapolation of performance achieved by smaller models. however, this linearity may be punctuated by "" break ( s ) """
q_085,sparse,0.0,0.0,0.0,0.0,mercedes,". doi : 10. 1103 / revmodphys. 39. 883. ^ glauber, roy j. ( february 1963 ). "" roy j. glauber "" time - dependent statistics of the ising model "" "". journal of mathematical physics. 4 ( 2 ) : 294 – 307. doi : 10. 1063 / 1. 1703954. retrieved 2021 - 03 - 21. ^ sherrington, david ; kirkpatrick, scott ( 1975 - 12 - 29 ). "" solvable model of a spin - glass "". physical review letters. 35 ( 26 ) : 1792 – 1796. bibcode : 1975phrvl.. 35. 1792s. doi : 10. 1103 / physrevlett. 35. 1792. issn 0031 - 9007. ^ hopfield, j. j. ( 1982 ). "" neural networks and physical systems with emergent collective computational abilities "". proceedings of the national academy of sciences. 79 ( 8 ) : 2554 – 2558. bibcode : 1982pnas... 79. 2554h. doi : 10. 1073 / pnas. 79. 8. 2554. pmc 346238. pmid 6953413. ^ hopfield, j. j. ( 1984 ). "" neurons with graded response have collective computational properties like those of two - state neurons "". proceedings of the national academy of sciences. 81"
q_086,sparse,0.0,0.0,0.0,0.0,jesus 's life,"of after ) multihead attention and feedforward layers stabilizes training, not requiring learning rate warmup. this is the "" pre - ln transformer "" and is more commonly used, compared to the original "" post - ln transformer "". pretrain - finetune transformers typically are first pretrained by self - supervised learning on a large generic dataset, followed by supervised fine - tuning on a small task - specific dataset. the pretrain dataset is typically an unlabeled large corpus, such as the pile. tasks for pretraining and fine - tuning commonly include : language modeling next - sentence prediction question answering reading comprehension sentiment analysis paraphrasing the t5 transformer report documents a large number of natural language pretraining tasks. some examples are : restoring or repairing incomplete or corrupted text. for example, the input, "" thank you ~ ~ me to your party ~ ~ week "", might generate the output, "" thank you for inviting me to your party last week "". translation between natural languages ( machine translation ) judging the pragmatic acceptability of natural language. for example, the following sentence might be judged "" not acceptable "", because even though it is syntactically well - formed, it is improbable in ordinary human usage : the course is jumping well. note that while each of these tasks is trivial or obvious for human native speakers of the language ( or languages ), they have typically proved challenging for"
q_087,sparse,0.0,0.0,0.0,0.022598870056497175,what is the first book thomson has written?,"280. doi : 10. 1016 / j. techfore. 2016. 08. 019. "" from not working to neural networking "". the economist. 2016. archived from the original on 31 december 2016. retrieved 26 april 2018. galvan, jill ( 1 january 1997 ). "" entering the posthuman collective in philip k. dick ' s "" do androids dream of electric sheep? "" "". science fiction studies. 24 ( 3 ) : 413 – 429. doi : 10. 1525 / sfs. 24. 3. 0413. jstor 4240644. geist, edward moore ( 9 august 2015 ). "" is artificial intelligence really an existential threat to humanity? "". bulletin of the atomic scientists. archived from the original on 30 october 2015. retrieved 30 october 2015. gibbs, samuel ( 27 october 2014 ). "" elon musk : artificial intelligence is our biggest existential threat "". the guardian. archived from the original on 30 october 2015. retrieved 30 october 2015. goffrey, andrew ( 2008 ). "" algorithm "". in fuller, matthew ( ed. ). software studies : a lexicon. cambridge, mass. : mit press. pp. 15 – 20. isbn 978 - 1 - 4356 - 4787 - 9. goldman, sharon ( 14 september 2022 ). "" 10 years later, deep learning ' revolution ' rages on, say ai pioneers hinton, lecu"
q_088,sparse,0.0,0.0,0.0,0.02631578947368421,virgin islands as a state of virgin islands,"##yesha ; nambiar, vaishnavi ( 2024 ). "" role of artificial intelligence in the prevention of online child sexual abuse : a systematic review of literature "". journal of applied security research. 19 ( 4 ) : 586 – 627. doi : 10. 1080 / 19361610. 2024. 2331885. ^ razi, afsaneh ; kim, seunghyun ; alsoubai, ashwaq ; stringhini, gianluca ; solorio, thamar ; de choudhury, munmun ; wisniewski, pamela j. ( 13 october 2021 ). "" a human - centered systematic literature review of the computational approaches for online sexual risk detection "". proceedings of the acm on human - computer interaction. 5 ( cscw2 ) : 1 – 38. doi : 10. 1145 / 3479609. ^ ransbotham, sam ; kiron, david ; gerbert, philipp ; reeves, martin ( 6 september 2017 ). "" reshaping business with artificial intelligence "". mit sloan management review. archived from the original on 13 february 2024. ^ sun, yuran ; zhao, xilei ; lovreglio, ruggiero ; kuligowski, erica ( 2024 ). "" ai for large - scale evacuation modeling : promises and challenges "". interpretable machine learning for the analysis, design, assessment, and informed decision making"
q_089,sparse,0.0,0.0,0.0,0.03347280334728033,"a 3. 8 l 3800 series , and a 3. 8 l 3800 series","quickly became "" ubiquitous "". though the original transformer has both encoder and decoder blocks, bert is an encoder - only model. academic and research usage of bert began to decline in 2023, following rapid improvements in the abilities of decoder - only models ( such as gpt ) to solve tasks via prompting. although decoder - only gpt - 1 was introduced in 2018, it was gpt - 2 in 2019 that caught widespread attention because openai claimed to have initially deemed it too powerful to release publicly, out of fear of malicious use. gpt - 3 in 2020 went a step further and as of 2025 is available only via api with no offering of downloading the model to execute locally. but it was the 2022 consumer - facing chatbot chatgpt that received extensive media coverage and public attention. the 2023 gpt - 4 was praised for its increased accuracy and as a "" holy grail "" for its multimodal capabilities. openai did not reveal the high - level architecture and the number of parameters of gpt - 4. the release of chatgpt led to an uptick in llm usage across several research subfields of computer science, including robotics, software engineering, and societal impact work. in 2024 openai released the reasoning model openai o1, which generates long chains of thought before returning a final answer. many llms with parameter counts comparable to those of openai ' s gpt series have been"
q_090,sparse,0.0,0.0,0.0,0.0,entitl,"; polosukhin, illia ( 2017 ). "" attention is all you need "". advances in neural information processing systems. 30. curran associates, inc. ^ oord, aaron van den ; kalchbrenner, nal ; kavukcuoglu, koray ( 2016 - 06 - 11 ). "" pixel recurrent neural networks "". proceedings of the 33rd international conference on machine learning. pmlr : 1747 – 1756. ^ a b cruse, holk ; neural networks as cybernetic systems, 2nd and revised edition ^ elman, jeffrey l. ( 1990 ). "" finding structure in time "". cognitive science. 14 ( 2 ) : 179 – 211. doi : 10. 1016 / 0364 - 0213 ( 90 ) 90002 - e. ^ jordan, michael i. ( 1997 - 01 - 01 ). "" serial order : a parallel distributed processing approach "". neural - network models of cognition — biobehavioral foundations. advances in psychology. vol. 121. pp. 471 – 495. doi : 10. 1016 / s0166 - 4115 ( 97 ) 80111 - 2. isbn 978 - 0 - 444 - 81931 - 4. s2cid 15375627. ^ gers, felix a. ; schraudolph, nicol n. ; schmidhuber, jurgen ( 2002 ). "" learning precise timing with lstm rec"
q_091,sparse,0.0,0.0,0.0,0.040540540540540536,"israeli startups is the ability to create a new generation of entrepreneurs and entrepreneurs.""","optimal large language models "". neurips : 30016 – 30030. isbn 978 - 1 - 7138 - 7108 - 8. ^ a b caballero, ethan ; gupta, kshitij ; rish, irina ; krueger, david ( 2022 ). "" broken neural scaling laws "". arxiv : 2210. 14891 [ cs. lg ]. ^ a b wei, jason ; tay, yi ; bommasani, rishi ; raffel, colin ; zoph, barret ; borgeaud, sebastian ; yogatama, dani ; bosma, maarten ; zhou, denny ; metzler, donald ; chi, ed h. ; hashimoto, tatsunori ; vinyals, oriol ; liang, percy ; dean, jeff ; fedus, william ( 31 august 2022 ). "" emergent abilities of large language models "". transactions on machine learning research. issn 2835 - 8856. archived from the original on 22 march 2023. retrieved 19 march 2023. ^ "" 137 emergent abilities of large language models "". jason wei. retrieved 2023 - 06 - 24. ^ bowman, samuel r. ( 2024 ). "" eight things to know about large language models "". critical ai. 2 ( 2 ). doi : 10. 1215 / 2834703x - 11556011. ^ hahn, michael ; goyal"
q_092,sparse,0.0,0.0,0.0,0.0,us election,"). "" sequence to sequence learning with neural networks "" ( pdf ). electronic proceedings of the neural information processing systems conference. 27 : 5346. arxiv : 1409. 3215. bibcode : 2014arxiv1409. 3215s. ^ jozefowicz, rafal ; vinyals, oriol ; schuster, mike ; shazeer, noam ; wu, yonghui ( 2016 - 02 - 07 ). "" exploring the limits of language modeling "". arxiv : 1602. 02410 [ cs. cl ]. ^ gillick, dan ; brunk, cliff ; vinyals, oriol ; subramanya, amarnag ( 2015 - 11 - 30 ). "" multilingual language processing from bytes "". arxiv : 1512. 00103 [ cs. cl ]. ^ vinyals, oriol ; toshev, alexander ; bengio, samy ; erhan, dumitru ( 2014 - 11 - 17 ). "" show and tell : a neural image caption generator "". arxiv : 1411. 4555 [ cs. cv ]. ^ cho, kyunghyun ; van merrienboer, bart ; gulcehre, caglar ; bahdanau, dzmitry ; bougares, fethi ; schwenk, holger ; bengio, yoshua ( 2014 - 06"
q_093,sparse,0.0,0.0,0.0,0.03524229074889868,is an american political scientist at the university of nebraska - lincoln,": discourse parsing, 2019 : semantic parsing ). increasing interest in multilinguality, and, potentially, multimodality ( english since 1999 ; spanish, dutch since 2002 ; german since 2003 ; bulgarian, danish, japanese, portuguese, slovenian, swedish, turkish since 2006 ; basque, catalan, chinese, greek, hungarian, italian, turkish since 2007 ; czech since 2009 ; arabic since 2012 ; 2017 : 40 + languages ; 2018 : 60 + / 100 + languages ) elimination of symbolic representations ( rule - based over supervised towards weakly supervised methods, representation learning and end - to - end systems ) cognition most higher - level nlp applications involve aspects that emulate intelligent behavior and apparent comprehension of natural language. more broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behavior represents one of the developmental trajectories of nlp ( see trends among conll shared tasks above ). cognition refers to "" the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses. "" cognitive science is the interdisciplinary, scientific study of the mind and its processes. cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. especially during the age of symbolic nlp, the area of computational linguistics maintained strong ties with cognitive studies. as an example, george lakoff offers a methodology to build natural language processing ( nlp ) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, with two defining"
q_094,sparse,0.0,0.0,0.0,0.0,cyprus,""". transactions of the association for computational linguistics. 8 : 842 – 866. arxiv : 2002. 12327. doi : 10. 1162 / tacl _ a _ 00349. s2cid 211532403. archived from the original on 2022 - 04 - 03. retrieved 2024 - 01 - 21. ^ a b movva, rajiv ; balachandar, sidhika ; peng, kenny ; agostini, gabriel ; garg, nikhil ; pierson, emma ( 2024 ). "" topics, authors, and institutions in large language model research : trends from 17k arxiv papers "". proceedings of the 2024 conference of the north american chapter of the association for computational linguistics : human language technologies ( volume 1 : long papers ). pp. 1223 – 1243. arxiv : 2307. 10700. doi : 10. 18653 / v1 / 2024. naacl - long. 67. retrieved 2024 - 12 - 08. ^ hern, alex ( 14 february 2019 ). "" new ai fake text generator may be too dangerous to release, say creators "". the guardian. archived from the original on 14 february 2019. retrieved 20 january 2024. ^ "" chatgpt a year on : 3 ways the ai chatbot has completely changed the world in 12 months "". euronews. november 30, 2023. archived from the original"
q_095,sparse,0.0,0.0,0.0,0.0,"""","decoder transformer, then taking just the encoder. they are also referred to as "" all - to - all "" or "" bert - like "". a "" decoder - only "" transformer is not literally decoder - only, since without an encoder, the cross - attention mechanism has nothing to attend to. thus, the decoder layers in a decoder - only transformer is composed of just two sublayers : the causally masked self - attention, and the feedforward network. this is usually used for text generation and instruction following. the models in the gpt series and chinchilla series are decoder - only. they are also referred to as "" autoregressive "" or "" causal "". an "" encoder – decoder "" transformer is generally the same as the original transformer, with 2 sublayers per encoder layer and 3 sublayers per decoder layer, etc. they might have minor architectural improvements, such as alternative activation functions, changing the location of normalization, etc. this is also usually used for text generation and instruction following. the models in the t5 series are encoder – decoder. a "" prefixlm "" ( prefix language model ) is a decoder - only architecture, but with prefix masking, which is different from causal masking. specifically, it has mask of the form m prefixlm = [ 0 − ∞ 0 m causal ] { \ displays"
q_096,sparse,0.0,0.0,0.0,0.04225352112676056,is a set of digits that can be rearranged in a number of ways,"##ctional recurrent neural networks bidirectional rnn a bidirectional rnn ( birnn ) is composed of two rnns, one processing the input sequence in one direction, and another in the opposite direction. abstractly, it is structured as follows : the forward rnn processes in one direction : f θ ( x 0, h 0 ) = ( y 0, h 1 ), f θ ( x 1, h 1 ) = ( y 1, h 2 ), … { \ displaystyle f _ { \ theta } ( x _ { 0 }, h _ { 0 } ) = ( y _ { 0 }, h _ { 1 } ), f _ { \ theta } ( x _ { 1 }, h _ { 1 } ) = ( y _ { 1 }, h _ { 2 } ), \ dots } the backward rnn processes in the opposite direction : f θ ′ ′ ( x n, h n ′ ) = ( y n ′, h n − 1 ′ ), f θ ′ ′ ( x n − 1, h n − 1 ′ ) = ( y n − 1 ′, h n − 2 ′ ), … { \ displaystyle f ' _ { \ theta ' } ( x _ { n }, h _ { n } ' ) = ( y ' _ { n }, h _ { n - 1 } ' ), f ' _ { \ theta ' } ( x _"
q_097,sparse,0.0,0.0,0.0,0.012738853503184716,"a symbiotic relationship between the two cultures.""","##har ; mishra, shailesh ; endres, christoph ; holz, thorsten ; fritz, mario ( 2023 - 02 - 01 ). "" not what you ' ve signed up for : compromising real - world llm - integrated applications with indirect prompt injection "". proceedings of the 16th acm workshop on artificial intelligence and security. pp. 79 – 90. doi : 10. 1145 / 3605764. 3623985. isbn 979 - 8 - 4007 - 0260 - 0. ^ edwards, benj ( 2024 - 01 - 15 ). "" ai poisoning could turn models into destructive "" sleeper agents, "" says anthropic "". ars technica. retrieved 2025 - 07 - 19. ^ "" u. s. judge approves $ 1. 5 billion anthropic copyright settlement with authors "". reuters. 2025 - 09 - 25. retrieved 2025 - 09 - 26. ^ "" anthropic reaches $ 1. 5b settlement with authors over ai copyright claims "". associated press. 2025 - 09 - 25. retrieved 2025 - 09 - 26. ^ "" meta fends off authors ' u. s. copyright lawsuit over ai "". reuters. 2025 - 06 - 25. retrieved 2025 - 06 - 26. ^ "" meta scores victory in ai copyright case "". wired. 2025 - 06 - 25. retrieved 2025 - 06 - 26. ^ "" openai"
q_098,sparse,0.0,0.0,0.0,0.0,continuity,"##er architecture, introduced in 2017, replaced recurrence with self - attention, allowing efficient parallelization, longer context handling, and scalable training on unprecedented data volumes. this innovation enabled models like gpt, bert, and their successors, which demonstrated emergent behaviors at scale, such as few - shot learning and compositional reasoning. reinforcement learning, particularly policy gradient algorithms, has been adapted to fine - tune llms for desired behaviors beyond raw next - token prediction. reinforcement learning from human feedback ( rlhf ) applies these methods to optimize a policy, the llm ' s output distribution, against reward signals derived from human or automated preference judgments. this has been critical for aligning model outputs with user expectations, improving factuality, reducing harmful responses, and enhancing task performance. benchmark evaluations for llms have evolved from narrow linguistic assessments toward comprehensive, multi - task evaluations measuring reasoning, factual accuracy, alignment, and safety. hill climbing, iteratively optimizing models against benchmarks, has emerged as a dominant strategy, producing rapid incremental performance gains but raising concerns of overfitting to benchmarks rather than achieving genuine generalization or robust capability improvements. history the number of publications about large language models by year grouped by publication types. the training compute of notable large models in flops vs publication date over the period 2010 – 2024. for overall notable models ( top left ), frontier models ( top right ), top language models ( bottom left"
q_099,sparse,0.0,0.0,0.0,0.008097165991902834,"the "" universe "" view","s that are y s "" ). deductive reasoning in logic is the process of proving a new statement ( conclusion ) from other statements that are given and assumed to be true ( the premises ). proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules. given a problem and a set of premises, problem - solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. in the case of horn clauses, problem - solving search can be performed by reasoning forwards from the premises or backwards from the problem. in the more general case of the clausal form of first - order logic, resolution is a single, axiom - free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved. inference in both horn clause logic and first - order logic is undecidable, and therefore intractable. however, backward reasoning with horn clauses, which underpins computation in the logic programming language prolog, is turing complete. moreover, its efficiency is competitive with computation in other symbolic programming languages. fuzzy logic assigns a "" degree of truth "" between 0 and 1. it can therefore handle propositions that are vague and partially true. non - monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning."
q_000,hybrid,0.0,0.0,0.0,0.0,"chisholm, hugh, ed. ( 1911 ). "" orelli","introduction of a multi - head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence. its parallelizability was an important factor to its widespread use in large neural networks. ai boom era as early as spring 2017, even before the "" attention is all you need "" preprint was published, one of the co - authors applied the "" decoder - only "" variation of the architecture to generate fictitious wikipedia articles. transformer architecture is now used alongside many generative models that contribute to the ongoing ai boom. in language modelling, elmo ( 2018 ) was a bi - directional lstm that produces contextualized word embeddings, improving upon the line of research from bag of words and word2vec. it was followed by bert ( 2018 ), an encoder - only transformer model. in october 2019, google started using bert to process search queries. in 2020, google translate replaced the previous rnn - encoder – rnn - decoder model by a transformer - encoder – rnn - decoder model. starting in 2018, the openai gpt series of decoder - only transformers became state of the art in natural language generation. in 2022, a chatbot based on gpt - 3, chatgpt, became unexpectedly popular, triggering a boom around large language models. since 2020, transformers have been applied in modalities beyond text, including the"
q_001,hybrid,0.0,0.0,0.0,0.0,evelyn everett - green at librivox,"cs. cl ]. ^ press, ofir ; smith, noah a. ; lewis, mike ( 2021 - 08 - 01 ). "" train short, test long : attention with linear biases enables input length extrapolation "". arxiv : 2108. 12409 [ cs. cl ]. ^ shaw, peter ; uszkoreit, jakob ; vaswani, ashish ( 2018 ). "" self - attention with relative position representations "". arxiv : 1803. 02155 [ cs. cl ]. ^ ke, guolin ; he, di ; liu, tie - yan ( 2021 - 03 - 15 ), rethinking positional encoding in language pre - training, arxiv : 2006. 15595 ^ kwon, woosuk ; li, zhuohan ; zhuang, siyuan ; sheng, ying ; zheng, lianmin ; yu, cody hao ; gonzalez, joseph ; zhang, hao ; stoica, ion ( 2023 - 10 - 23 ). "" efficient memory management for large language model serving with pagedattention "". proceedings of the 29th symposium on operating systems principles. sosp ' 23. new york, ny, usa : association for computing machinery. pp. 611 – 626. arxiv : 2309. 06180. doi : 10. 1145 / 3600006. 3613165. isbn 979 - 8 -"
q_002,hybrid,0.0,0.0,0.0,0.0,braingate official website,"- term memory based deep recurrent neural networks for large vocabulary speech recognition "". arxiv : 1410. 4281 [ cs. cl ]. ^ dupond, samuel ( 2019 ). "" a thorough review on the current advance of neural network structures "". annual reviews in control. 14 : 200 – 230. ^ abiodun, oludare isaac ; jantan, aman ; omolara, abiodun esther ; dada, kemi victoria ; mohamed, nachaat abdelatif ; arshad, humaira ( 2018 - 11 - 01 ). "" state - of - the - art in artificial neural network applications : a survey "". heliyon. 4 ( 11 ) e00938. bibcode : 2018heliy... 400938a. doi : 10. 1016 / j. heliyon. 2018. e00938. issn 2405 - 8440. pmc 6260436. pmid 30519653. ^ espinosa - sanchez, juan manuel ; gomez - marin, alex ; de castro, fernando ( 2023 - 07 - 05 ). "" the importance of cajal ' s and lorente de no ' s neuroscience to the birth of cybernetics "". the neuroscientist. 31 ( 1 ) : 14 – 30. doi : 10. 1177 / 10738584231179932. hdl : 1026"
q_003,hybrid,0.0,0.0,0.0,0.0,"clemson 's nii adam , ahmed hameed , adam , ahmed hameed , adam , ahmed hameed , adam , ahmed hameed , adam , ahmed hameed , adam , ahmed hameed , adam ,","on 14 february 2019. retrieved 20 january 2024. ^ "" chatgpt a year on : 3 ways the ai chatbot has completely changed the world in 12 months "". euronews. november 30, 2023. archived from the original on january 14, 2024. retrieved january 20, 2024. ^ heaven, will ( march 14, 2023 ). "" gpt - 4 is bigger and better than chatgpt — but openai won ' t say why "". mit technology review. archived from the original on march 17, 2023. retrieved january 20, 2024. ^ metz, cade ( september 12, 2024 ). "" openai unveils new chatgpt that can reason through math and science "". the new york times. retrieved september 12, 2024. ^ "" parameters in notable artificial intelligence systems "". ourworldindata. org. november 30, 2023. retrieved january 20, 2024. ^ sharma, shubham ( 2025 - 01 - 20 ). "" open - source deepseek - r1 uses pure reinforcement learning to match openai o1 — at 95 % less cost "". venturebeat. retrieved 2025 - 01 - 26. ^ "" llama - mesh "". research. nvidia. com. 2024. retrieved 2025 - 10 - 30. ^ zia, dr tehseen ( 2024 - 01 - 08 ). "" unveiling of large multimo"
q_004,hybrid,0.0,0.0,0.0,0.0,wretched,"deep learning for human action recognition "". in salah, albert ali ; lepri, bruno ( eds. ). human behavior unterstanding. lecture notes in computer science. vol. 7065. amsterdam, netherlands : springer. pp. 29 – 39. doi : 10. 1007 / 978 - 3 - 642 - 25446 - 8 _ 4. isbn 978 - 3 - 642 - 25445 - 1. ^ hochreiter, sepp ; heusel, martin ; obermayer, klaus ( 2007 ). "" fast model - based protein homology detection without alignment "". bioinformatics. 23 ( 14 ) : 1728 – 1736. doi : 10. 1093 / bioinformatics / btm247. pmid 17488755. ^ thireou, trias ; reczko, martin ( july 2007 ). "" bidirectional long short - term memory networks for predicting the subcellular localization of eukaryotic proteins "". ieee / acm transactions on computational biology and bioinformatics. 4 ( 3 ) : 441 – 446. bibcode : 2007itcbb... 4.. 441t. doi : 10. 1109 / tcbb. 2007. 1015. pmid 17666763. s2cid 11787259. ^ tax, niek ; verenich, ilya ; la rosa, marcello ; dumas,"
q_005,hybrid,0.0,0.0,0.0,0.0,ogham in 3d,"transformers for longer sequences with sparse attention methods "". google ai blog. 25 march 2021. archived from the original on 2021 - 09 - 18. retrieved 2021 - 05 - 28. ^ zhai, shuangfei ; talbott, walter ; srivastava, nitish ; huang, chen ; goh, hanlin ; zhang, ruixiang ; susskind, josh ( 2021 - 09 - 21 ). "" an attention free transformer "". arxiv : 2105. 14103 [ cs. lg ]. ^ peng, hao ; pappas, nikolaos ; yogatama, dani ; schwartz, roy ; smith, noah a. ; kong, lingpeng ( 2021 - 03 - 19 ). "" random feature attention "". arxiv : 2103. 02143 [ cs. cl ]. ^ choromanski, krzysztof ; likhosherstov, valerii ; dohan, david ; song, xingyou ; gane, andreea ; sarlos, tamas ; hawkins, peter ; davis, jared ; belanger, david ; colwell, lucy ; weller, adrian ( 2020 - 09 - 30 ). "" masked language modeling for proteins via linearly scalable long - context transformers "". arxiv : 2006. 03555 [ cs. lg ]. ^ lu, kevin ; grover, aditya ; abbeel, pieter ;"
q_006,hybrid,0.0,0.0,0.0,0.0,##gento,", fabio lorenzo ; di ventra, massimiliano ( 2017 ). "" the complex dynamics of memristive circuits : analytical results and universal slow relaxation "". physical review e. 95 ( 2 ) 022140. arxiv : 1608. 08651. bibcode : 2017phrve.. 95b2140c. doi : 10. 1103 / physreve. 95. 022140. pmid 28297937. s2cid 6758362. ^ harvey, inman ; husbands, phil ; cliff, dave ( 1994 ), "" seeing the light : artificial evolution, real vision "", 3rd international conference on simulation of adaptive behavior : from animals to animats 3, pp. 392 – 401 ^ quinn, matt ( 2001 ). "" evolving communication without dedicated communication channels "". advances in artificial life : 6th european conference, ecal 2001. pp. 357 – 366. doi : 10. 1007 / 3 - 540 - 44811 - x _ 38. isbn 978 - 3 - 540 - 42567 - 0. ^ beer, randall d. ( 1997 ). "" the dynamics of adaptive behavior : a research program "". robotics and autonomous systems. 20 ( 2 – 4 ) : 257 – 289. doi : 10. 1016 / s0921 - 8890 ( 96 ) 00063 - 2. ^ sherstinsky, alex ( 2018 - 12 -"
q_007,hybrid,0.0,0.0,0.0,0.0,milton,"; mackworth, alan ( 2023 ). artificial intelligence, foundations of computational agents ( 3rd ed. ). cambridge university press. doi : 10. 1017 / 9781009258227. isbn 978 - 1 - 0092 - 5819 - 7. ^ russell, stuart ; norvig, peter ( 2020 ). artificial intelligence : a modern approach ( 4th ed. ). pearson. isbn 978 - 0 - 1346 - 1099 - 3. ^ "" why agents are the next frontier of generative ai "". mckinsey digital. 24 july 2024. archived from the original on 3 october 2024. retrieved 10 august 2024. ^ "" introducing copilot search in bing "". blogs. bing. com. 4 april 2025. ^ peters, jay ( 14 march 2023 ). "" the bing ai bot has been secretly running gpt - 4 "". the verge. retrieved 31 august 2025. ^ "" security for microsoft 365 copilot "". learn. microsoft. com. ^ o ' flaherty, kate ( 21 may 2025 ). "" google ai overviews — everything you need to know "". forbes. ^ "" generative ai in search : let google do the searching for you "". google. 14 may 2024. ^ figueiredo, mayara costa ; ankrah, elizabeth ; powell, jacquelyn e. ; epstein, daniel a. ; chen, yunan"
q_008,hybrid,0.0,0.0,0.0,0.020761245674740483," a b c d e f g h i j k l m n o p "" shipping intelligence "". the morning chronicle. no. 24715. london. 6 january 1849.  a b c d e f g h i j k l m n o p "" shipping intelligence ""","able to recognize the wide variety of input as being identical to each other in terms of its textual equivalent. speech segmentation given a sound clip of a person or people speaking, separate it into words. a subtask of speech recognition and typically grouped with it. text - to - speech given a text, transform those units and produce a spoken representation. text - to - speech can be used to aid the visually impaired. word segmentation ( tokenization ) tokenization is a text - processing technique that divides text into individual words or word fragments. this technique results in two key components : a word index and tokenized text. the word index is a list that maps unique words to specific numerical identifiers, and the tokenized text replaces each word with its corresponding numerical token. these numerical tokens are then used in various deep learning methods. for a language like english, this is fairly trivial, since words are usually separated by spaces. however, some written languages like chinese, japanese and thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language. sometimes this process is also used in cases like bag of words ( bow ) creation in data mining. morphological analysis lemmatization of basque words lemmatization the task of removing inflectional endings only and to return the base dictionary form of a word which is also known as a lemma. lemmatization"
q_009,hybrid,0.0,0.0,0.0,0.049999999999999996,a syringe and a syringe,"; azulay, osher ; sintov, avishai ( february 2023 ). "" learning to throw with a handful of samples using decision transformers "". ieee robotics and automation letters. 8 ( 2 ) : 576 – 583. bibcode : 2023iral.... 8.. 576m. doi : 10. 1109 / lra. 2022. 3229266. issn 2377 - 3766. ^ a b ruoss, anian ; deletang, gregoire ; medapati, sourabh ; grau - moya, jordi ; wenliang, li ; catt, elliot ; reid, john ; genewein, tim ( 2024 - 02 - 07 ). "" grandmaster - level chess without search "". arxiv : 2402. 04494v1 [ cs. lg ]. ^ a b wolf, thomas ; debut, lysandre ; sanh, victor ; chaumond, julien ; delangue, clement ; moi, anthony ; cistac, pierric ; rault, tim ; louf, remi ; funtowicz, morgan ; davison, joe ; shleifer, sam ; von platen, patrick ; ma, clara ; jernite, yacine ; plu, julien ; xu, canwen ; le scao, teven ; gugger, sylvain ; drame, maria"
q_010,hybrid,0.0,0.0,0.0,0.008849557522123894,the world,"the resulting models were reverse - engineered, and it turned out they used discrete fourier transform. the training of the model also highlighted a phenomenon called grokking, in which the model initially memorizes the training set ( overfitting ), and later suddenly learns to actually perform the calculation. understanding and intelligence see also : philosophy of artificial intelligence and artificial consciousness nlp researchers were evenly split when asked, in a 2022 survey, whether ( untuned ) llms "" could ( ever ) understand natural language in some nontrivial sense "". proponents of "" llm understanding "" believe that some llm abilities, such as mathematical reasoning, imply an ability to "" understand "" certain concepts. a microsoft team argued in 2023 that gpt - 4 "" can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more "" and that gpt - 4 "" could reasonably be viewed as an early ( yet still incomplete ) version of an artificial general intelligence system "" : "" can one reasonably say that a system that passes exams for software engineering candidates is not really intelligent? "" ilya sutskever argues that predicting the next word sometimes involves reasoning and deep insights, for example if the llm has to predict the name of the criminal in an unknown detective novel after processing the entire story leading up to the revelation. some researchers characterize llms as "" alien intelligence "". for example, conjecture ceo connor leahy considers untuned llms to be"
q_011,hybrid,0.0,0.0,0.0,0.0,betabeat,"googlers cracked an sf rival ' s tech model with a single word "". sfgate. archived from the original on 16 december 2023. ^ "" prepare for truly useful large language models "". nature biomedical engineering. 7 ( 2 ) : 85 – 86. 7 march 2023. doi : 10. 1038 / s41551 - 023 - 01012 - 6. pmid 36882584. s2cid 257403466. ^ brinkmann, levin ; baumann, fabian ; bonnefon, jean - francois ; derex, maxime ; muller, thomas f. ; nussberger, anne - marie ; czaplicka, agnieszka ; acerbi, alberto ; griffiths, thomas l. ; henrich, joseph ; leibo, joel z. ; mcelreath, richard ; oudeyer, pierre - yves ; stray, jonathan ; rahwan, iyad ( 2023 - 11 - 20 ). "" machine culture "". nature human behaviour. 7 ( 11 ) : 1855 – 1868. arxiv : 2311. 11388. doi : 10. 1038 / s41562 - 023 - 01742 - 2. issn 2397 - 3374. pmid 37985914. ^ niederhoffer, kate ; kellerman, gabriella rosen ; lee, angela ; liebscher, alex ; rapuano, kristina"
q_012,hybrid,0.0,0.0,0.0,0.0,christ,"the area of computational linguistics maintained strong ties with cognitive studies. as an example, george lakoff offers a methodology to build natural language processing ( nlp ) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, with two defining aspects : apply the theory of conceptual metaphor, explained by lakoff as "" the understanding of one idea, in terms of another "" which provides an idea of the intent of the author. for example, consider the english word big. when used in a comparison ( "" that is a big tree "" ), the author ' s intent is to imply that the tree is physically large relative to other trees or the authors experience. when used metaphorically ( "" tomorrow is a big day "" ), the author ' s intent to imply importance. the intent behind other usages, like in "" she is a big person "", will remain somewhat ambiguous to a person and a cognitive nlp algorithm alike without additional information. assign relative measures of meaning to a word, phrase, sentence or piece of text based on the information presented before and after the piece of text being analyzed, e. g., by means of a probabilistic context - free grammar ( pcfg ). the mathematical equation for such algorithms is presented in us patent 9269353 : r m m ( t o k e n n ) = p m m ( t o k e n n ) × 1 2 d ( [UNK] i = − d d ( ( p"
q_013,hybrid,0.0,0.0,0.0,0.0379746835443038,the youngest people to be awarded a member of the british empire ( mbe ) in the 2023 new year honours,"^ zhang, aston ; lipton, zachary ; li, mu ; smola, alexander j. ( 2024 ). "" 10. modern recurrent neural networks "". dive into deep learning. cambridge new york port melbourne new delhi singapore : cambridge university press. isbn 978 - 1 - 009 - 38943 - 3. ^ rumelhart, david e. ; hinton, geoffrey e. ; williams, ronald j. ( october 1986 ). "" learning representations by back - propagating errors "". nature. 323 ( 6088 ) : 533 – 536. bibcode : 1986natur. 323.. 533r. doi : 10. 1038 / 323533a0. issn 1476 - 4687. ^ a b schmidhuber, jurgen ( 1993 ). habilitation thesis : system modeling and optimization ( pdf ). page 150 ff demonstrates credit assignment across the equivalent of 1, 200 layers in an unfolded rnn. ^ sepp hochreiter ; jurgen schmidhuber ( 21 august 1995 ), long short term memory, wikidata q98967430 ^ a b hochreiter, sepp ; schmidhuber, jurgen ( 1997 - 11 - 01 ). "" long short - term memory "". neural computation. 9 ( 8 ) : 1735 – 1780. doi : 10. 1162 / neco. 1997. 9. 8. 1735. pmid"
q_014,hybrid,0.0,0.0,0.0,0.008583690987124462,continuity,"explicit symbolic knowledge. although his arguments had been ridiculed and ignored when they were first presented, eventually, ai research came to agree with him. the issue is not resolved : sub - symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. critics such as noam chomsky argue continuing research into symbolic ai will still be necessary to attain general intelligence, in part because sub - symbolic ai is a move away from explainable ai : it can be difficult or impossible to understand why a modern statistical ai program made a particular decision. the emerging field of neuro - symbolic artificial intelligence attempts to bridge the two approaches. neat vs. scruffy main article : neats and scruffies "" neats "" hope that intelligent behavior is described using simple, elegant principles ( such as logic, optimization, or neural networks ). "" scruffies "" expect that it necessarily requires solving a large number of unrelated problems. neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. this issue was actively discussed in the 1970s and 1980s, but eventually was seen as irrelevant. modern ai has elements of both. soft vs. hard computing main article : soft computing finding a provably correct or optimal solution is intractable for many important problems. soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision,"
q_015,hybrid,0.0,0.0,0.0,0.02459016393442623,lgbt people may be out to varying degrees,"from paralegals to fast food cooks, while job demand is likely to increase for care - related professions ranging from personal healthcare to the clergy. in july 2025, ford ceo jim farley predicted that "" artificial intelligence is going to replace literally half of all white - collar workers in the u. s. "" from the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by joseph weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value - based judgement. existential risk main article : existential risk from artificial intelligence recent public debates in artificial intelligence have increasingly focused on its broader societal and ethical implications. it has been argued ai will become so powerful that humanity may irreversibly lose control of it. this could, as physicist stephen hawking stated, "" spell the end of the human race "". this scenario has been common in science fiction, when a computer or robot suddenly develops a human - like "" self - awareness "" ( or "" sentience "" or "" consciousness "" ) and becomes a malevolent character. these sci - fi scenarios are misleading in several ways. first, ai does not require human - like sentience to be an existential risk. modern ai programs are given specific goals and use learning and intelligence to achieve them. philosopher nick bostrom argued that if one gives almost any goal"
q_016,hybrid,0.0,0.0,0.0,0.0,scully,"information : word embedding each integer token identifier is converted into an embedding vector via a lookup table. equivalently stated, it multiplies a one - hot representation of the token identifier by an embedding matrix m { \ displaystyle m }. for example, if the input token ' s identifier is 3 { \ displaystyle 3 }, then the one - hot representation is [ 0, 0, 0, 1, 0, 0, … ] { \ displaystyle [ 0, 0, 0, 1, 0, 0, \ dots ] }, and its embedding vector is e m b e d ( 3 ) = [ 0, 0, 0, 1, 0, 0, … ] m { \ displaystyle \ mathrm { embed } ( 3 ) = [ 0, 0, 0, 1, 0, 0, \ dots ] m } the token embedding vectors are added to their respective positional encoding vectors ( see below ), producing the sequence of input vectors. the dimension of an embedding vector is called hidden size or embedding size and written as d emb { \ displaystyle d _ { \ text { emb } } }. this size is written as d model { \ displaystyle d _ { \ text { model } } } in the original transformer paper. un - embedding an un - embedding layer is almost the reverse of"
q_017,hybrid,0.0,0.0,0.0,0.008230452674897118,ordinary mode of perception,"training data for llms. this produces large volumes of traffic which has led to denial of service issues with many websites. the situation has been described as "" a ddos on the entire internet "" and in some cases scrapers make up the majority of traffic to a site. ai web crawlers may bypass the methods that are usually used to block web scrapers, such as robots. txt files, blocking user - agents and filtering suspicious traffic. website operators have resorted to novel methods such as ai tarpits, but some fear that tarpits will only worsen the burden on servers. mental health clinical and mental health contexts present emerging applications alongside significant safety concerns. research and social media posts suggest that some individuals are using llms to seek therapy or mental health support. in early 2025, a survey by sentio university found that nearly half ( 48. 7 % ) of 499 u. s. adults with ongoing mental health conditions who had used llms reported turning to them for therapy or emotional support, including help with anxiety, depression, loneliness, and similar concerns. llms can produce hallucinations — plausible but incorrect statements — which may mislead users in sensitive mental health contexts. research also shows that llms may express stigma or inappropriate agreement with maladaptive thoughts, reflecting limitations in replicating the judgment and relational skills of human therapists. evaluations of crisis scenarios indicate that some llms lack effective safety protocols, such as assessing suicide risk or making appropriate"
q_018,hybrid,0.0,0.0,0.0,0.0,a - mode,"d, x ⟩, sin ⟨ w d, x ⟩ ] t { \ displaystyle \ varphi ( x ) = { \ frac { 1 } { \ sqrt { d } } } [ \ cos \ langle w _ { 1 }, x \ rangle, \ sin \ langle w _ { 1 }, x \ rangle, \ cdots \ cos \ langle w _ { d }, x \ rangle, \ sin \ langle w _ { d }, x \ rangle ] ^ { t } } where w 1,..., w d { \ displaystyle w _ { 1 },..., w _ { d } } are independent samples from the normal distribution n ( 0, σ 2 i ) { \ displaystyle n ( 0, \ sigma ^ { 2 } i ) }. this choice of parameters satisfy e [ ⟨ φ ( x ), φ ( y ) ⟩ ] = e − ‖ x − y ‖ 2 2 σ 2 { \ displaystyle \ mathbb { e } [ \ langle \ varphi ( x ), \ varphi ( y ) \ rangle ] = e ^ { - { \ frac { \ | x - y \ | ^ { 2 } } { 2 \ sigma ^ { 2 } } } } }, or e ⟨ x, y ⟩ / σ 2 = e [ ⟨ e ‖ x ‖ 2 / 2 σ 2 φ"
q_019,hybrid,0.0,0.0,0.0,0.0,philip yancey,"commitments from companies "". reuters. 21 may 2024. retrieved 23 may 2024. ^ "" frontier ai safety commitments, ai seoul summit 2024 "". gov. uk. 21 may 2024. archived from the original on 23 may 2024. retrieved 23 may 2024. ^ a b buntz, brian ( 3 november 2024 ). "" quality vs. quantity : us and china chart different paths in global ai patent race in 2024 / geographical breakdown of ai patents in 2024 "". research & development world. r & d world. archived from the original on 9 december 2024. ^ a b russell & norvig 2021, p. 9. ^ a b c copeland, j., ed. ( 2004 ). the essential turing : the ideas that gave birth to the computer age. oxford, england : clarendon press. isbn 0 - 1982 - 5079 - 7. ^ "" google books ngram "". archived from the original on 5 october 2024. retrieved 5 october 2024. ^ ai ' s immediate precursors : mccorduck ( 2004, pp. 51 – 107 ), crevier ( 1993, pp. 27 – 32 ), russell & norvig ( 2021, pp. 8 – 17 ), moravec ( 1988, p. 3 ) ^ a b turing ' s original publication of the turing test in "" computing machinery and intelligence "" : turing ( 1950 ) historical influence and philosophical implications : haugeland"
q_020,hybrid,0.0,0.0,0.0,0.0,christian education,"; kuttler, heinrich ; lewis, mike ; yih, wen - tau ; rocktaschel, tim ; riedel, sebastian ; kiela, douwe ( 2020 ). "" retrieval - augmented generation for knowledge - intensive nlp tasks "". advances in neural information processing systems. 33. curran associates, inc. : 9459 – 9474. arxiv : 2005. 11401. archived from the original on 2023 - 06 - 12. retrieved 2023 - 06 - 12. ^ dickson, ben ( 2025 - 04 - 02 ). "" the tool integration problem that ' s holding back enterprise ai ( and how cotools solves it ) "". venturebeat. retrieved 2025 - 05 - 26. ^ liang, yaobo ; wu, chenfei ; song, ting ; wu, wenshan ; xia, yan ; liu, yu ; ou, yang ; lu, shuai ; ji, lei ; mao, shaoguang ; wang, yun ; shou, linjun ; gong, ming ; duan, nan ( 2024 ). "" taskmatrix. ai : completing tasks by connecting foundation models with millions of apis "". science. 3 0063. doi : 10. 34133 / icomputing. 0063. ^ patil, shishir g. ; zhang, tianjun ; wang, xin ; gonzalez, joseph e. ( 2023 - 05 - 01"
q_021,hybrid,0.0,0.0,0.0,0.012195121951219513,a sloping deck and a sloping helm,"be easier to train, requiring no warm - up, leading to faster convergence. pseudocode the following is the pseudocode for a standard pre - ln encoder – decoder transformer, adapted from formal algorithms for transformers input : encoder input t _ e decoder input t _ d output : array of probability distributions, with shape ( decoder vocabulary size x length ( decoder output sequence ) ) / * encoder * / z _ e ← encoder. tokenizer ( t _ e ) for each t in 1 : length ( z _ e ) do z _ e [ t ] ← encoder. embedding ( z _ e [ t ] ) + encoder. positional _ embedding ( t ) for each l in 1 : length ( encoder. layers ) do layer ← encoder. layers [ l ] / * first sublayer * / z _ e _ copy ← copy ( z _ e ) for each t in 1 : length ( z _ e ) do z _ e [ t ] ← layer. layer _ norm ( z _ e [ t ] ) z _ e ← layer. multihead _ attention ( z _ e, z _ e, z _ e ) for each t in 1 : length ( z _ e ) do z _ e [ t ] ← z _ e [ t ] + z _ e _ copy [ t ] / * second sublayer * / z _ e"
q_022,hybrid,0.0,0.0,0.0,0.00930232558139535,ordinary mode of perception,"predict the name of the criminal in an unknown detective novel after processing the entire story leading up to the revelation. some researchers characterize llms as "" alien intelligence "". for example, conjecture ceo connor leahy considers untuned llms to be like inscrutable alien "" shoggoths "", and believes that rlhf tuning creates a "" smiling facade "" obscuring the inner workings of the llm : "" if you don ' t push it too far, the smiley face stays on. but then you give it [ an unexpected ] prompt, and suddenly you see this massive underbelly of insanity, of weird thought processes and clearly non - human understanding. "" in contrast, some skeptics of llm understanding believe that existing llms are "" simply remixing and recombining existing writing "", a phenomenon known as stochastic parrot, or they point to the deficits existing llms continue to have in prediction skills, reasoning skills, agency, and explainability. for example, gpt - 4 has natural deficits in planning and in real - time learning. generative llms have been observed to confidently assert claims of fact which do not seem to be justified by their training data, a phenomenon which has been termed "" hallucination "". specifically, hallucinations in the context of llms correspond to the generation of text or responses that seem syntactically sound, fluent, and natural but are factually incorrect, nonsen"
q_023,hybrid,0.0,0.0,0.0,0.0,thomas jane,": 10. 1353 / pbm. 2000. 0001. issn 1529 - 8795. pmid 10804585. ^ renshaw, birdsey ( 1946 - 05 - 01 ). "" central effects of centripetal impulses in axons of spinal ventral roots "". journal of neurophysiology. 9 ( 3 ) : 191 – 204. doi : 10. 1152 / jn. 1946. 9. 3. 191. issn 0022 - 3077. pmid 21028162. ^ a b grossberg, stephen ( 2013 - 02 - 22 ). "" recurrent neural networks "". scholarpedia. 8 ( 2 ) : 1888. bibcode : 2013schpj... 8. 1888g. doi : 10. 4249 / scholarpedia. 1888. issn 1941 - 6016. ^ a b c rosenblatt, frank ( 1961 - 03 - 15 ). dtic ad0256582 : principles of neurodynamics. perceptrons and the theory of brain mechanisms. defense technical information center. ^ f. rosenblatt, "" perceptual generalization over transformation groups "", pp. 63 - - 100 in self - organizing systems : proceedings of an inter - disciplinary conference, 5 and 6 may 1959. edited by marshall c. yovitz and scott cameron. london, new york, [ etc. ], pergamon press, 1960"
q_024,hybrid,0.0,0.0,0.0,0.0,wretched,"time ) skepticism by most participants. until then, neural learning was basically rejected because of its lack of statistical interpretability. until 2015, deep learning had evolved into the major framework of nlp. [ link is broken, try http : / / web. stanford. edu / class / cs224n / ] ^ segev, elad ( 2022 ). semantic network analysis in social sciences. london : routledge. isbn 978 - 0 - 367 - 63652 - 4. archived from the original on 5 december 2021. retrieved 5 december 2021. ^ yi, chucai ; tian, yingli ( 2012 ), "" assistive text reading from complex background for blind persons "", camera - based document analysis and recognition, lecture notes in computer science, vol. 7139, springer berlin heidelberg, pp. 15 – 28, citeseerx 10. 1. 1. 668. 869, doi : 10. 1007 / 978 - 3 - 642 - 29364 - 1 _ 2, isbn 978 - 3 - 642 - 29363 - 4 { { citation } } : cs1 maint : work parameter with isbn ( link ) ^ a b "" natural language processing ( nlp ) - a complete guide "". www. deeplearning. ai. 2023 - 01 - 11. retrieved 2024 - 05 - 05. ^ "" geeksforgeeks. ( n. d. ). tokenization in natural language"
q_025,hybrid,0.0,0.0,0.0,0.0,parliamentary elections,"##opomorphism foundation models list of large language models list of chatbots language model benchmark reinforcement learning small language model references ^ a b c bommasani, rishi ; hudson, drew a. ; adeli, ehsan ; altman, russ ; arora, simran ; von arx, matthew ; bernstein, michael s. ; bohg, jeannette ; bosselut, antoine ; brunskill, emma ( 2021 ). "" on the opportunities and risks of foundation models "". arxiv : 2108. 07258 [ cs. lg ]. ^ a b brown, tom b. ; mann, benjamin ; ryder, nick ; subbiah, melanie ; kaplan, jared ; dhariwal, prafulla ; neelakantan, arvind ; shyam, pranav ; sastry, girish ; askell, amanda ( 2020 ). "" language models are few - shot learners "". arxiv : 2005. 14165 [ cs. cl ]. ^ a b c brown, tom b. ; mann, benjamin ; ryder, nick ; subbiah, melanie ; kaplan, jared ; dhariwal, prafulla ; neelakantan, arvind ; shyam, pranav ; sastry, girish ; askell, amanda ; agarwal, sandhini ; herbert - voss, ariel ; krueger, gretchen ; henighan, tom ; child"
q_026,hybrid,0.0,0.0,0.0,0.0,mmorpg,"of research were continued, e. g., the development of chatterbots with racter and jabberwacky. an important development ( that eventually led to the statistical turn in the 1990s ) was the rising importance of quantitative evaluation in this period. statistical nlp ( 1990s – present ) up until the 1980s, most natural language processing systems were based on complex sets of hand - written rules. starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. this shift was influenced by increasing computational power ( see moore ' s law ) and a decline in the dominance of chomskyan linguistic theories... ( e. g. transformational grammar ), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine - learning approach to language processing. 1990s : many of the notable early successes in statistical methods in nlp occurred in the field of machine translation, due especially to work at ibm research, such as ibm alignment models. these systems were able to take advantage of existing multilingual textual corpora that had been produced by the parliament of canada and the european union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. however, many systems relied on corpora that were specifically developed for the tasks they were designed to perform. this reliance has been a major limitation to their broader effectiveness and continues to affect similar systems. consequently,"
q_027,hybrid,0.0,0.0,0.0,0.022857142857142857," nasa history division 1998.  wood 2002, p. 1.  boeing 2005, p. 50.  stephen clark ( 31 march 2011 ). "" nasa to set exploration architecture this summer "". spaceflight now. archived from the original on 15 may 2011. retrieved 4 september 2022.  chris bergin ( 14 september 2011 ). "" sl","; lovreglio, ruggiero ; kuligowski, erica ( 2024 ). "" ai for large - scale evacuation modeling : promises and challenges "". interpretable machine learning for the analysis, design, assessment, and informed decision making for civil infrastructure. pp. 185 – 204. doi : 10. 1016 / b978 - 0 - 12 - 824073 - 1. 00014 - 9. isbn 978 - 0 - 12 - 824073 - 1. ^ gomaa, islam ; adelzadeh, masoud ; gwynne, steven ; spencer, bruce ; ko, yoon ; benichou, noureddine ; ma, chunyun ; elsagan, nour ; duong, dana ; zalok, ehab ; kinateder, max ( 1 november 2021 ). "" a framework for intelligent fire detection and evacuation system "". fire technology. 57 ( 6 ) : 3179 – 3185. doi : 10. 1007 / s10694 - 021 - 01157 - 3. ^ zhao, xilei ; lovreglio, ruggiero ; nilsson, daniel ( 1 may 2020 ). "" modelling and interpreting pre - evacuation decision - making using machine learning "". automation in construction. 113 103140. doi : 10. 1016 / j. autcon. 2020. 103140. hdl : 10179 / 17315. ^ "" india ' s"
q_028,hybrid,0.0,0.0,0.0,0.008368200836820083,ordinary mode of perception,"study. in the late 2010s and early 2020s, agi companies began to deliver programs that created enormous interest. in 2015, alphago, developed by deepmind, beat the world champion go player. the program taught only the game ' s rules and developed a strategy by itself. gpt - 3 is a large language model that was released in 2020 by openai and is capable of generating high - quality human - like text. chatgpt, launched on 30 november 2022, became the fastest - growing consumer software application in history, gaining over 100 million users in two months. it marked what is widely regarded as ai ' s breakout year, bringing it into the public consciousness. these programs, and others, inspired an aggressive ai boom, where large companies began investing billions of dollars in ai research. according to ai impacts, about us $ 50 billion annually was invested in "" ai "" around 2022 in the u. s. alone and about 20 % of the new u. s. computer science phd graduates have specialized in "" ai "". about 800, 000 "" ai "" - related u. s. job openings existed in 2022. according to pitchbook research, 22 % of newly funded startups in 2024 claimed to be ai companies. philosophy main article : philosophy of artificial intelligence philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. another major focus has been whether machines can be conscious, and the associated ethical implications. many other topics in"
q_029,hybrid,0.0,0.0,0.0,0.025477707006369428,ad - free in the late 1980s.,"##g. 2. 2. 30247. 50087. ^ iraqi, amjad ( 3 april 2024 ). "" ' lavender ' : the ai machine directing israel ' s bombing spree in gaza "". + 972 magazine. archived from the original on 10 october 2024. retrieved 6 april 2024. ^ davies, harry ; mckernan, bethan ; sabbagh, dan ( 1 december 2023 ). "" ' the gospel ' : how israel uses ai to select bombing targets in gaza "". the guardian. archived from the original on 6 december 2023. retrieved 4 december 2023. ^ marti, j werner ( 10 august 2024 ). "" drohnen haben den krieg in der ukraine revolutioniert, doch sie sind empfindlich auf storsender – deshalb sollen sie jetzt autonom operieren "". neue zurcher zeitung ( in german ). archived from the original on 10 august 2024. retrieved 10 august 2024. ^ banh, leonardo ; strobel, gero ( 2023 ). "" generative artificial intelligence "". electronic markets. 33 ( 1 ) 63. doi : 10. 1007 / s12525 - 023 - 00680 - 1. ^ pasick, adam ( 27 march 2023 ). "" artificial intelligence glossary : neural networks and other terms explained "". the new york times"
q_030,hybrid,0.0,0.0,0.0,0.0,universe view,"in areas where there is hope that the future will be better than the past. it is descriptive rather than prescriptive. bias and unfairness may go undetected because the developers are overwhelmingly white and male : among ai engineers, about 4 % are black and 20 % are women. there are various conflicting definitions and mathematical models of fairness. these notions depend on ethical assumptions, and are influenced by beliefs about society. one broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. representational fairness tries to ensure that ai systems do not reinforce negative stereotypes or render certain groups invisible. procedural fairness focuses on the decision process rather than the outcome. the most relevant notions of fairness may depend on the context, notably the type of ai application and the stakeholders. the subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. having access to sensitive attributes such as race or gender is also considered by many ai ethicists to be necessary in order to compensate for biases, but it may conflict with anti - discrimination laws. at the 2022 acm conference on fairness, accountability, and transparency a paper reported that a clip ‑ based ( contrastive language - image pre - training ) robotic system reproduced harmful gender ‑ and race ‑ linked stereotypes in a simulated manipulation task. the authors recommended robot ‑ learning methods which physically manifest such harms be "" paused, reworked, or even wound down"
q_031,hybrid,0.0,0.0,0.0,0.0,mla,"- 100 in self - organizing systems : proceedings of an inter - disciplinary conference, 5 and 6 may 1959. edited by marshall c. yovitz and scott cameron. london, new york, [ etc. ], pergamon press, 1960. ix, 322 p. ^ nakano, kaoru ( 1971 ). "" learning process in a model of associative memory "". pattern recognition and machine learning. pp. 172 – 186. doi : 10. 1007 / 978 - 1 - 4615 - 7566 - 5 _ 15. isbn 978 - 1 - 4615 - 7568 - 9. ^ nakano, kaoru ( 1972 ). "" associatron - a model of associative memory "". ieee transactions on systems, man, and cybernetics. smc - 2 ( 3 ) : 380 – 388. bibcode : 1972itsmc... 2.. 380n. doi : 10. 1109 / tsmc. 1972. 4309133. ^ amari, shun - ichi ( 1972 ). "" learning patterns and pattern sequences by self - organizing nets of threshold elements "". ieee transactions. c ( 21 ) : 1197 – 1206. ^ little, w. a. ( 1974 ). "" the existence of persistent states in the brain "". mathematical biosciences. 19 ( 1 – 2 ) : 101 – 120. doi : 10. 1016"
q_032,hybrid,0.0,0.0,0.0,0.0,"juan ,",". unice. fr. archived from the original on 2021 - 04 - 18. retrieved 2021 - 03 - 09. ^ "" nlp approaches to computational argumentation – acl 2016, berlin "". retrieved 2021 - 03 - 09. ^ administration. "" centre for language technology ( clt ) "". macquarie university. retrieved 2021 - 01 - 11. ^ "" shared task : grammatical error correction "". www. comp. nus. edu. sg. retrieved 2021 - 01 - 11. ^ "" shared task : grammatical error correction "". www. comp. nus. edu. sg. retrieved 2021 - 01 - 11. ^ duan, yucong ; cruz, christophe ( 2011 ). "" formalizing semantic of natural language through conceptualization from existence "". international journal of innovation, management and technology. 2 ( 1 ) : 37 – 42. archived from the original on 2011 - 10 - 09. ^ "" u b u w e b : : racter "". www. ubu. com. retrieved 2020 - 08 - 17. ^ writer, beta ( 2019 ). lithium - ion batteries. doi : 10. 1007 / 978 - 3 - 030 - 16800 - 1. isbn 978 - 3 - 030 - 16799 - 8. s2cid 155818532. ^ "" document understanding ai on google cloud ( cloud next ' 19 ) – youtube "". www. youtube. com. 11 april 2019"
q_033,hybrid,0.0,0.0,0.0,0.0,"""","##set. this basic construction can be applied with more sophistication to improve the model. the image encoder may be frozen to improve stability. this type of method, where embeddings from multiple modalities are fused and the predictor is trained on the combined embeddings, is called early fusion. another method, called intermediate fusion, involves each modality being first processed independently to obtain modality - specific representations ; then these intermediate representations are fused together. in general, cross - attention is used for integrating information from different modalities. as an example, the flamingo model uses cross - attention layers to inject visual information into its pre - trained language model. non - natural languages llms can handle programming languages similarly to how they handle natural languages. no special change in token handling is needed as code, like human language, is represented as plain text. llms can generate code based on problems or instructions written in natural language. they can also describe code in natural language or translate it into other programming languages. they were originally used as a code completion tool, but advances have moved them towards automatic programming. services such as github copilot offer llms specifically trained, fine - tuned, or prompted for programming. in computational biology, transformer - base architectures, such as dna llms, have also proven useful in analyzing biological sequences : protein, dna, and rna. with proteins they appear able to capture a degree of "" grammar """
q_034,hybrid,0.0,0.0,0.0,0.007782101167315175,ordinary mode of perception,"be researching battlefield robots. ai tools make it easier for authoritarian governments to efficiently control their citizens in several ways. face and voice recognition allow widespread surveillance. machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. recommendation systems can precisely target propaganda and misinformation for maximum effect. deepfakes and generative ai aid in producing misinformation. advanced ai can make authoritarian centralized decision - making more competitive than liberal and decentralized systems such as markets. it lowers the cost and difficulty of digital warfare and advanced spyware. all these technologies have been available since 2020 or earlier — ai facial recognition systems are already being used for mass surveillance in china. there are many other ways in which ai is expected to help bad actors, some of which can not be foreseen. for example, machine - learning ai is able to design tens of thousands of toxic molecules in a matter of hours. technological unemployment main articles : workplace impact of artificial intelligence and technological unemployment economists have frequently highlighted the risks of redundancies from ai, and speculated about unemployment if there is no adequate social policy for full employment. in the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that "" we ' re in uncharted territory "" with ai. a survey of economists showed disagreement about whether the increasing use of robots and ai will cause a substantial increase in long - term unemployment, but they generally agree that it could be a net benefit if"
q_035,hybrid,0.0,0.0,0.0,0.034482758620689655," a b c d e f g h i j k l m n o p q r s t u v w x y z aa ab ac ad ae "" 33 on uc faculty win fellowships "". the san francisco examiner. san francisco, californ",", accountability, and transparency. pp. 599 – 627. arxiv : 2504. 18412. doi : 10. 1145 / 3715275. 3732039. isbn 979 - 8 - 4007 - 1482 - 5. ^ grabb, declan ; lamparth, max ; vasan, nina ( 2024 - 08 - 14 ). "" risks from language models for automated mental healthcare : ethics and structure for implementation "". arxiv : 2406. 11852 [ cs. cy ]. ^ mcbain, ryan k. ; cantor, jonathan h. ; zhang, li ang ; baker, olesya ; zhang, fang ; halbisen, alyssa ; kofner, aaron ; breslau, joshua ; stein, bradley ; mehrotra, ateev ; yu, hao ( 2025 - 03 - 05 ). "" competency of large language models in evaluating appropriate responses to suicidal ideation : comparative study "". journal of medical internet research. 27 ( 1 ) e67891. doi : 10. 2196 / 67891. pmc 11928068. pmid 40053817. ^ li, fei - fei ; etchemendy, john ( 2024 - 05 - 22 ). "" no, today ' s ai isn ' t sentient. here ' s how we know "". time. retrieved 2024 - 05 - 22. ^"
q_036,hybrid,0.0,0.0,0.0,0.012422360248447206,the gods,"} } f ( t ) = ( e i t / r k ) k = 0, 1, …, d 2 − 1 { \ displaystyle f ( t ) = \ left ( e ^ { it / r ^ { k } } \ right ) _ { k = 0, 1, \ ldots, { \ frac { d } { 2 } } - 1 } } where r = n 2 / d { \ displaystyle r = n ^ { 2 / d } }. the main reason for using this positional encoding function is that using it, shifts are linear transformations : f ( t + δ t ) = d i a g ( f ( δ t ) ) f ( t ) { \ displaystyle f ( t + \ delta t ) = \ mathrm { diag } ( f ( \ delta t ) ) f ( t ) } where δ t ∈ r { \ displaystyle \ delta t \ in \ mathbb { r } } is the distance one wishes to shift. this allows the transformer to take any encoded position, and find the encoding of the position n - steps - ahead or n - steps - behind, by a matrix multiplication. by taking a linear sum, any convolution can also be implemented as linear transformations : [UNK] j c j f ( t + δ t j ) = ( [UNK] j c j d i a g ( f ( δ t j ) ) ) f ( t ) { \ displaystyle"
q_037,hybrid,0.0,0.0,0.0,0.010526315789473684,the open - access universal short tit catalogue project,"of only twenty words, because that was all that would fit in a computer memory at the time. 1970s : during the 1970s, many programmers began to write "" conceptual ontologies "", which structured real - world information into computer - understandable data. examples are margie ( schank, 1975 ), sam ( cullingford, 1978 ), pam ( wilensky, 1978 ), talespin ( meehan, 1976 ), qualm ( lehnert, 1977 ), politics ( carbonell, 1979 ), and plot units ( lehnert 1981 ). during this time, the first chatterbots were written ( e. g., parry ). 1980s : the 1980s and early 1990s mark the heyday of symbolic methods in nlp. focus areas of the time included research on rule - based parsing ( e. g., the development of hpsg as a computational operationalization of generative grammar ), morphology ( e. g., two - level morphology ), semantics ( e. g., lesk algorithm ), reference ( e. g., within centering theory ) and other areas of natural language understanding ( e. g., in the rhetorical structure theory ). other lines of research were continued, e. g., the development of chatterbots with racter and jabberwacky. an important development ( that eventually led to the statistical turn in the 1990s ) was the rising importance of quantitative evaluation in this period."
q_038,hybrid,0.0,0.0,0.0,0.0,- birth date is 1993,". cv ]. ^ campolucci, paolo ; uncini, aurelio ; piazza, francesco ; rao, bhaskar d. ( 1999 ). "" on - line learning algorithms for locally recurrent neural networks "". ieee transactions on neural networks. 10 ( 2 ) : 253 – 271. bibcode : 1999itnn... 10.. 253c. citeseerx 10. 1. 1. 33. 7550. doi : 10. 1109 / 72. 750549. pmid 18252525. ^ wan, eric a. ; beaufays, francoise ( 1996 ). "" diagrammatic derivation of gradient algorithms for neural networks "". neural computation. 8 : 182 – 201. doi : 10. 1162 / neco. 1996. 8. 1. 182. s2cid 15512077. ^ a b campolucci, paolo ; uncini, aurelio ; piazza, francesco ( 2000 ). "" a signal - flow - graph approach to on - line gradient calculation "". neural computation. 12 ( 8 ) : 1901 – 1927. citeseerx 10. 1. 1. 212. 5406. doi : 10. 1162 / 089976600300015196. pmid 10953244. s2cid 15090951. ^ graves, alex ; fernandez, santiago ; gomez, faustino j. ( 2006 ). "" connectionist temporal classification : label"
q_039,hybrid,0.0,0.0,0.0,0.0,esoteric,"- zheng ; lee, yee - chun ( 1992 ). "" learning and extracting finite state automata with second - order recurrent neural networks "" ( pdf ). neural computation. 4 ( 3 ) : 393 – 405. doi : 10. 1162 / neco. 1992. 4. 3. 393. s2cid 19666035. ^ omlin, christian w. ; giles, c. lee ( 1996 ). "" constructing deterministic finite - state automata in recurrent neural networks "". journal of the acm. 45 ( 6 ) : 937 – 972. citeseerx 10. 1. 1. 32. 2364. doi : 10. 1145 / 235809. 235811. s2cid 228941. ^ paine, rainer w. ; tani, jun ( 2005 - 09 - 01 ). "" how hierarchical control self - organizes in artificial adaptive systems "". adaptive behavior. 13 ( 3 ) : 211 – 225. doi : 10. 1177 / 105971230501300303. s2cid 9932565. ^ a b "" burns, benureau, tani ( 2018 ) a bergson - inspired adaptive time constant for the multiple timescales recurrent neural network model. jnns "". ^ barkan, oren ; benchimol, jonathan ; caspi, itamar ; cohen, eliya ;"
q_040,hybrid,0.0,0.0,0.0,0.0,valenciennes valenciennes valenciennes valenciennes valenciennes valenciennes valenciennes valenciennes valenciennes valenciennes valenciennes valenciennes valenciennes valenciennes valenciennes valenciennes valenc,"^ { k } }, and the value weights w v { \ displaystyle w ^ { v } }. the module takes three sequences, a query sequence, a key sequence, and a value sequence. the query sequence is a sequence of length ℓ seq, query { \ displaystyle \ ell _ { \ text { seq, query } } }, and each entry is a vector of dimension d emb, query { \ displaystyle d _ { \ text { emb, query } } }. similarly for the key and value sequences. for each vector x i, query { \ displaystyle x _ { i, { \ text { query } } } } in the query sequence, it is multiplied by a matrix w q { \ displaystyle w ^ { q } } to produce a query vector q i = x i, query w q { \ displaystyle q _ { i } = x _ { i, { \ text { query } } } w ^ { q } }. the matrix of all query vectors is the query matrix : q = x query w q { \ displaystyle q = x _ { \ text { query } } w ^ { q } } similarly, we construct the key matrix k = x key w k { \ displaystyle k = x _ { \ text { key } } w ^ { k } } and the value matrix v = x value w v { \ displaystyle v = x _ { \ text {"
q_041,hybrid,0.0,0.0,0.0,0.0,2017,", in the smallest gpt - 2 model, there are only self - attention mechanisms. it has the following dimensions : d emb = 768, n head = 12, d head = 64 { \ displaystyle d _ { \ text { emb } } = 768, n _ { \ text { head } } = 12, d _ { \ text { head } } = 64 } since 12 × 64 = 768 { \ displaystyle 12 \ times 64 = 768 }, its output projection matrix w o ∈ r ( 12 × 64 ) × 768 { \ displaystyle w ^ { o } \ in \ mathbb { r } ^ { ( 12 \ times 64 ) \ times 768 } } is a square matrix. masked attention the transformer architecture is constructed to calculate output tokens iteratively. assuming t = 0 { \ displaystyle t = 0 } refers to the calculation of the first output token i = 0 { \ displaystyle i = 0 }, for step t > 0 { \ displaystyle t > 0 }, the output token i = 0 { \ displaystyle i = 0 } shall remain constant. this ensures properties of the model similar to autoregressive models. therefore, at every time step t { \ displaystyle t }, the calculation for all outputs i { \ displaystyle i } should not have access to tokens at position j { \ displaystyle j } for j > = i { \"
q_042,hybrid,0.0,0.0,0.0,0.03658536585365854,visions of authenticity. the assemblies of the all africa conference of churches 1963 - 1992,"acm conference on fairness, accountability, and transparency ( facct ' 22 ). seoul, south korea : association for computing machinery. doi : 10. 1145 / 3531146. 3533138. ^ for accessible summaries, see the georgia tech release and sciencedaily coverage of the study ' s findings. "" flawed ai makes robots racist, sexist "". georgia tech research news. 23 june 2022. ^ "" robots turn racist and sexist with flawed ai, study finds "". sciencedaily. 21 june 2022. ^ sample ( 2017 ). ^ "" black box ai "". 16 june 2023. archived from the original on 15 june 2024. retrieved 5 october 2024. ^ christian ( 2020 ), p. 110. ^ christian ( 2020 ), pp. 88 – 91. ^ christian ( 2020, p. 83 ) ; russell & norvig ( 2021, p. 997 ) ^ christian ( 2020 ), p. 91. ^ christian ( 2020 ), p. 83. ^ verma ( 2021 ). ^ rothman ( 2020 ). ^ christian ( 2020 ), pp. 105 – 108. ^ christian ( 2020 ), pp. 108 – 112. ^ ropek, lucas ( 21 may 2024 ). "" new anthropic research sheds light on ai ' s ' black box ' "". gizmodo. archived from the original on 5 october 2024. retrieved 23"
q_043,hybrid,0.0,0.0,0.0,0.013513513513513514,the same cycle,"2, t + 1 ) { \ displaystyle f _ { \ theta _ { 2 } } : ( x _ { 1, t }, h _ { 2, t } ) \ mapsto ( x _ { 2, t }, h _ { 2, t + 1 } ) }.... layer n { \ displaystyle n } has hidden vector h n, t { \ displaystyle h _ { n, t } }, parameters θ n { \ displaystyle \ theta _ { n } }, and maps f θ n : ( x n − 1, t, h n, t ) ↦ ( x n, t, h n, t + 1 ) { \ displaystyle f _ { \ theta _ { n } } : ( x _ { n - 1, t }, h _ { n, t } ) \ mapsto ( x _ { n, t }, h _ { n, t + 1 } ) }. each layer operates as a stand - alone rnn, and each layer ' s output sequence is used as the input sequence to the layer above. there is no conceptual limit to the depth of stacked rnn. bidirectional main article : bidirectional recurrent neural networks bidirectional rnn a bidirectional rnn ( birnn ) is composed of two rnns, one processing the input sequence in one direction, and another in the opposite direction. abstractly,"
q_044,hybrid,0.0,0.0,0.0,0.0,op internship program,"natural language generation. in 2022, a chatbot based on gpt - 3, chatgpt, became unexpectedly popular, triggering a boom around large language models. since 2020, transformers have been applied in modalities beyond text, including the vision transformer, speech recognition, robotics, and multimodal. the vision transformer, in turn, stimulated new developments in convolutional neural networks. image and video generators like dall - e ( 2021 ), stable diffusion 3 ( 2024 ), and sora ( 2024 ), use transformers to analyse input data ( like text prompts ) by breaking it down into "" tokens "" and then calculating the relevance between each token using self - attention, which helps the model understand the context and relationships within the data. training methods for stabilizing training the plain transformer architecture had difficulty in converging. in the original paper, the authors recommended using learning rate warmup. that is, the learning rate should linearly scale up from 0 to maximal value for the first part of the training ( usually recommended to be 2 % of the total number of training steps ), before decaying again. a 2020 paper found that using layer normalization before ( instead of after ) multihead attention and feedforward layers stabilizes training, not requiring learning rate warmup. this is the "" pre - ln transformer "" and is more commonly used, compared to the original "" post - ln transformer"
q_045,hybrid,0.0,0.0,0.0,0.0,israel,"##3 - 12 - 29. ^ amodei, dario ; olah, chris ; steinhardt, jacob ; christiano, paul ; schulman, john ; mane, dan ( 2016 - 06 - 21 ). "" concrete problems in ai safety "". arxiv : 1606. 06565 [ cs. ai ]. ^ lyons, jessica ( 2025 - 09 - 26 ). "" prompt injection – and a $ 5 domain – trick salesforce agentforce into leaking sales "". the register. retrieved 2025 - 09 - 26. ^ carlini, nicholas ; tramer, florian ; wallace, eric ( 2021 - 08 - 11 ). "" extracting training data from large language models "" ( pdf ). usenix association. retrieved 2025 - 10 - 02. ^ zhao, yao ; zhang, yun ; sun, yong ( 2023 - 06 - 07 ). "" the debate over understanding in ai ' s large language models "". proceedings of the national academy of sciences. 120 ( 13 ) e2215907120. arxiv : 2306. 05499. bibcode : 2023pnas.. 12015907m. doi : 10. 1073 / pnas. 2215907120. pmc 10068812. pmid 36943882. ^ buolamwini, joy ; gebru, timnit ( 2018 - 01 - 01 )"
q_046,hybrid,0.0,0.0,0.0,0.0,trim,"} of type ( x t, h t ) ↦ ( y t, h t + 1 ) { \ displaystyle ( x _ { t }, h _ { t } ) \ mapsto ( y _ { t }, h _ { t + 1 } ) }, where x t { \ displaystyle x _ { t } } : input vector ; h t { \ displaystyle h _ { t } } : hidden vector ; y t { \ displaystyle y _ { t } } : output vector ; θ { \ displaystyle \ theta } : neural network parameters. in words, it is a neural network that maps an input x t { \ displaystyle x _ { t } } into an output y t { \ displaystyle y _ { t } }, with the hidden vector h t { \ displaystyle h _ { t } } playing the role of "" memory "", a partial record of all previous input - output pairs. at each step, it transforms input to an output, and modifies its "" memory "" to help it to better perform future processing. the illustration to the right may be misleading to many because practical neural network topologies are frequently organized in "" layers "" and the drawing gives that appearance. however, what appears to be layers are, in fact, different steps in time, "" unfolded "" to produce the appearance of layers. stacked rnn stacked rnn a stacked rnn, or deep rnn, is composed of multiple rnns"
q_047,hybrid,0.0,0.0,0.0,0.008298755186721992,"kemp, mitchell the pittsburgh survey",", however, underestimated the difficulty of the problem. in 1974, both the u. s. and british governments cut off exploratory research in response to the criticism of sir james lighthill and ongoing pressure from the u. s. congress to fund more productive projects. minsky and papert ' s book perceptrons was understood as proving that artificial neural networks would never be useful for solving real - world tasks, thus discrediting the approach altogether. the "" ai winter "", a period when obtaining funding for ai projects was difficult, followed. in the early 1980s, ai research was revived by the commercial success of expert systems, a form of ai program that simulated the knowledge and analytical skills of human experts. by 1985, the market for ai had reached over a billion dollars. at the same time, japan ' s fifth generation computer project inspired the u. s. and british governments to restore funding for academic research. however, beginning with the collapse of the lisp machine market in 1987, ai once again fell into disrepute, and a second, longer - lasting winter began. up to this point, most of ai ' s funding had gone to projects that used high - level symbols to represent mental objects like plans, goals, beliefs, and known facts. in the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition, and began to look"
q_048,hybrid,0.0,0.0,0.0,0.0,##gento,"346238. pmid 6953413. ^ hopfield, j. j. ( 1984 ). "" neurons with graded response have collective computational properties like those of two - state neurons "". proceedings of the national academy of sciences. 81 ( 10 ) : 3088 – 3092. bibcode : 1984pnas... 81. 3088h. doi : 10. 1073 / pnas. 81. 10. 3088. pmc 345226. pmid 6587342. ^ engel, a. ; broeck, c. van den ( 2001 ). statistical mechanics of learning. cambridge, uk ; new york, ny : cambridge university press. isbn 978 - 0 - 521 - 77307 - 2. ^ seung, h. s. ; sompolinsky, h. ; tishby, n. ( 1992 - 04 - 01 ). "" statistical mechanics of learning from examples "". physical review a. 45 ( 8 ) : 6056 – 6091. bibcode : 1992phrva.. 45. 6056s. doi : 10. 1103 / physreva. 45. 6056. pmid 9907706. ^ zhang, aston ; lipton, zachary ; li, mu ; smola, alexander j. ( 2024 ). "" 10. modern recurrent neural networks "". dive into deep learning. cambridge new york port melbourne new delhi singapore : cambridge"
q_049,hybrid,0.0,0.0,0.0,0.011904761904761906,a revelation,"from the original on 26 march 2023. retrieved 15 january 2023. ^ varshney, neeraj ; yao, wenlin ; zhang, hongming ; chen, jianshu ; yu, dong ( 2023 ). "" a stitch in time saves nine : detecting and mitigating hallucinations of llms by validating low - confidence generation "". arxiv : 2307. 03987 [ cs. cl ]. ^ lin, belle ( 2025 - 02 - 05 ). "" why amazon is betting on ' automated reasoning ' to reduce ai ' s hallucinations : the tech giant says an obscure field that combines ai and math can mitigate — but not completely eliminate — ai ' s propensity to provide wrong answers "". wall street journal. issn 0099 - 9660. ^ lakoff, george ( 1999 ). philosophy in the flesh : the embodied mind and its challenge to western philosophy ; appendix : the neural theory of language paradigm. new york basic books. pp. 569 – 583. isbn 978 - 0 - 465 - 05674 - 3. ^ evans, vyvyan. ( 2014 ). the language myth. cambridge university press. isbn 978 - 1 - 107 - 04396 - 1. ^ friston, karl j. ( 2022 ). active inference : the free energy principle in mind, brain, and behavior ; chapter 4 the generative models of active inference."
q_050,hybrid,0.0,0.0,0.0,0.022857142857142857," a b c d e f g h i j k l m n o p q r s t u v w x y z aa ab ac ad ae "" 33 on uc faculty win fellowships "". the san francisco examiner. san francisco, californ","/ pnas. 2215907120. pmc 10068812. pmid 36943882. ^ buolamwini, joy ; gebru, timnit ( 2018 - 01 - 01 ). "" gender shades : intersectional accuracy disparities in commercial gender classification "" ( pdf ). proceedings of machine learning research ( fat * ). retrieved 2025 - 10 - 02. ^ yang, kaiqi ( 2024 - 11 - 01 ). "" unpacking political bias in large language models : a cross - model comparison on u. s. politics "". arxiv : 2412. 16746 [ cs. cy ]. ^ strubell, emma ; ganesh, ananya ; mccallum, andrew ( 2019 - 07 - 28 ). "" energy and policy considerations for deep learning in nlp "" ( pdf ). acl anthology. retrieved 2025 - 10 - 02. ^ he, yuhao ; yang, li ; qian, chunlian ; li, tong ; su, zhengyuan ; zhang, qiang ; hou, xiangqing ( 2023 - 04 - 28 ). "" conversational agent interventions for mental health problems : systematic review and meta - analysis of randomized controlled trials "". journal of medical internet research. 25 e43862. doi : 10. 2196 / 43862. pmc 10182468. pmid 37115595."
q_051,hybrid,0.0,0.0,0.0,0.00865800865800866,ordinary mode of perception,"thousands of jobs in banking, financial planning, and pension advice in the process, but i ' m not sure it will unleash a new wave of [ e. g., sophisticated ] pension innovation. "" military main article : military applications of artificial intelligence various countries are deploying ai military applications. the main applications enhance command and control, communications, sensors, integration and interoperability. research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. ai technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed joint fires between networked combat vehicles, both human - operated and autonomous. ai has been used in military operations in iraq, syria, israel and ukraine. generative ai vincent van gogh in watercolour created by generative ai software these paragraphs are an excerpt from generative artificial intelligence. [ edit ] generative artificial intelligence, also known as generative ai or genai, is a subfield of artificial intelligence that uses generative models to generate text, images, videos, audio, software code or other forms of data. these models learn the underlying patterns and structures of their training data and use them to generate new data in response to input, which often takes the form of natural language prompts. the prevalence of generative ai tools has increased significantly since the ai boom in the 2020s. this boom was"
q_052,hybrid,0.0,0.0,0.0,0.0,reincarnation,"fuzzy logic assigns a "" degree of truth "" between 0 and 1. it can therefore handle propositions that are vague and partially true. non - monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning. other specialized versions of logic have been developed to describe many complex domains. probabilistic methods for uncertain reasoning a simple bayesian network, with the associated conditional probability tables many problems in ai ( including reasoning, planning, learning, perception, and robotics ) require the agent to operate with incomplete or uncertain information. ai researchers have devised a number of tools to solve these problems using methods from probability theory and economics. precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis, and information value theory. these tools include models such as markov decision processes, dynamic decision networks, game theory and mechanism design. bayesian networks are a tool that can be used for reasoning ( using the bayesian inference algorithm ), learning ( using the expectation – maximization algorithm ), planning ( using decision networks ) and perception ( using dynamic bayesian networks ). probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time ( e. g., hidden markov models or kalman filters ). expectation – maximization clustering of old faithful eruption data starts from a random guess but then successfully converge"
q_053,hybrid,0.0,0.0,0.0,0.0,encyclopdia britannica. vol. 20 ( 11th ed. ). cambridge university press. p. 251.,", paul ; chung, hyung won ; sutton, charles ; gehrmann, sebastian ; schuh, parker ; shi, kensen ; tsvyashchenko, sasha ; maynez, joshua ; rao, abhishek ( 2022 - 04 - 01 ). "" palm : scaling language modeling with pathways "". arxiv : 2204. 02311 [ cs. cl ]. ^ ainslie, joshua ; lee - thorp, james ; de jong, michiel ; zemlyanskiy, yury ; lebron, federico ; sanghai, sumit ( 2023 - 12 - 23 ), gqa : training generalized multi - query transformer models from multi - head checkpoints, arxiv : 2305. 13245 ^ a b deepseek - ai ; liu, aixin ; feng, bei ; wang, bin ; wang, bingxuan ; liu, bo ; zhao, chenggang ; dengr, chengqi ; ruan, chong ( 19 june 2024 ), deepseek - v2 : a strong, economical, and efficient mixture - of - experts language model, arxiv : 2405. 04434. ^ a b leviathan, yaniv ; kalman, matan ; matias, yossi ( 2023 - 05 - 18 ), fast inference from transformers via speculative decoding, arxiv : 2211. 17192 ^ fu, yao ( 202"
q_054,hybrid,0.0,0.0,0.0,0.0,"van horne , friedman , sy - david , honzik , radek , ternullo , claudio","( 1950 ). ^ solomonoff ( 1956 ). ^ unsupervised learning : russell & norvig ( 2021, pp. 653 ) ( definition ), russell & norvig ( 2021, pp. 738 – 740 ) ( cluster analysis ), russell & norvig ( 2021, pp. 846 – 860 ) ( word embedding ) ^ a b supervised learning : russell & norvig ( 2021, § 19. 2 ) ( definition ), russell & norvig ( 2021, chpt. 19 – 20 ) ( techniques ) ^ reinforcement learning : russell & norvig ( 2021, chpt. 22 ), luger & stubblefield ( 2004, pp. 442 – 449 ) ^ transfer learning : russell & norvig ( 2021, pp. 281 ), the economist ( 2016 ) ^ "" artificial intelligence ( ai ) : what is ai and how does it work? | built in "". builtin. com. retrieved 30 october 2023. ^ computational learning theory : russell & norvig ( 2021, pp. 672 – 674 ), jordan & mitchell ( 2015 ) ^ natural language processing ( nlp ) : russell & norvig ( 2021, chpt. 23 – 24 ), poole, mackworth & goebel ( 1998, pp. 91 – 104 ), luger & stubblefield ( 2004, pp. 591 – 632 ) ^ subproblems"
q_055,hybrid,0.0,0.0,0.0,0.02390438247011952,sullivan 's island for help in locating supplies and troops.,"the georgetown experiment in 1954 involved fully automatic translation of more than sixty russian sentences into english. the authors claimed that within three or five years, machine translation would be a solved problem. however, real progress was much slower, and after the alpac report in 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. little further research in machine translation was conducted in america ( though some research continued elsewhere, such as japan and europe ) until the late 1980s when the first statistical machine translation systems were developed. 1960s : some notably successful natural language processing systems developed in the 1960s were shrdlu, a natural language system working in restricted "" blocks worlds "" with restricted vocabularies, and eliza, a simulation of a rogerian psychotherapy, written by joseph weizenbaum between 1964 and 1966. despite using minimal information about human thought or emotion, eliza was able to produce interactions that appeared human - like. when the "" patient "" exceeded the very small knowledge base, eliza might provide a generic response, for example, responding to "" my head hurts "" with "" why do you say your head hurts? "". ross quillian ' s successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in a computer memory at the time. 1970s : during the 1970s, many programmers began to write "" conceptual ontologies "", which structured real - world information into computer - understandable data."
q_056,hybrid,0.0,0.0,0.0,0.018433179723502304,ordinary mode of perception,"data. researchers target concrete failure modes, including memorization and copyright leakage, security exploits such as prompt injection, algorithmic bias manifesting as stereotyping, dataset selection effects, and political skew, methods for reducing high energy and carbon costs of large - scale training, and measurable cognitive and mental health impacts of conversational agents on users, while engaging empirical and ethical uncertainty about claims of machine sentience, and applying mitigation measures such as dataset curation, input sanitization, model auditing, scalable oversight, and governance frameworks. cbrn and content misuse ai labs treat cbrn defense ( chemical, biological, radiological, and nuclear defense ) and similar topics as high - consequence misuse attempt to apply various techniques to reduce potential harms. some commenters expressed concern over accidental or deliberate creation of misinformation, or other forms of misuse. for example, the availability of large language models could reduce the skill - level required to commit bioterrorism ; biosecurity researcher kevin esvelt has suggested that llm creators should exclude from their training data papers on creating or enhancing pathogens. content filtering llm applications accessible to the public, like chatgpt or claude, typically incorporate safety measures designed to filter out harmful content. however, implementing these controls effectively has proven challenging. for instance, a 2023 study proposed a method for circumventing llm safety systems. in 2025, the american sunlight"
q_057,hybrid,0.0,0.0,0.0,0.0,wretched,"both of which scales as o ( n 2 ) { \ displaystyle o ( n ^ { 2 } ) } where n { \ displaystyle n } is the number of tokens in a sequence. reformer ( 2020 ) reduces the computational load from o ( n 2 ) { \ displaystyle o ( n ^ { 2 } ) } to o ( n ln n ) { \ displaystyle o ( n \ ln n ) } by using locality - sensitive hashing and reversible layers. sparse attention uses attention graphs that grows slower than o ( n 2 ) { \ displaystyle o ( n ^ { 2 } ) }. for example, bigbird ( 2020 ) uses random small - world networks which grows as o ( n ) { \ displaystyle o ( n ) }. ordinary transformers require a memory size that is quadratic in the size of the context window. attention - free transformers reduce this to a linear dependence while still retaining the advantages of a transformer by linking the key to the value. random feature attention random feature attention ( 2021 ) uses fourier random features : φ ( x ) = 1 d [ cos ⟨ w 1, x ⟩, sin ⟨ w 1, x ⟩, [UNK] cos ⟨ w d, x ⟩, sin ⟨ w d, x ⟩ ] t { \ displaystyle \ varphi ( x ) = { \ frac { 1 } { \ sqrt { d } } } [ \ cos \ langle w _"
q_058,hybrid,0.0,0.0,0.0,0.008032128514056226,ordinary mode of perception,"the ai boom. generative ai ' s ability to create and modify content has led to several unintended consequences and harms. ethical concerns have been raised about ai ' s long - term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology. goals the general problem of simulating ( or creating ) intelligence has been broken into subproblems. these consist of particular traits or capabilities that researchers expect an intelligent system to display. the traits described below have received the most attention and cover the scope of ai research. reasoning and problem - solving early researchers developed algorithms that imitated step - by - step reasoning that humans use when they solve puzzles or make logical deductions. by the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics. many of these algorithms are insufficient for solving large reasoning problems because they experience a "" combinatorial explosion "" : they become exponentially slower as the problems grow. even humans rarely use the step - by - step deduction that early ai research could model. they solve most of their problems using fast, intuitive judgments. accurate and efficient reasoning is an unsolved problem. knowledge representation an ontology represents knowledge as a set of concepts within a domain and the relationships between those concepts. knowledge representation and knowledge engineering allow ai programs to answer questions intelligently and make deductions about real - world facts. formal knowledge representations are used in content -"
q_059,hybrid,0.0,0.0,0.0,0.03550295857988165,"##htemeier , elizabeth ; achtemeier , paul j. ( 1962 ). the old testament roots of our faith. nashville : ab zarathustra , the red book, and "" visionary "" works. sonu shamdasani. cham, switzerland. isbn","##er & schmidhuber ( 2012 ). ^ russell & norvig ( 2021 ), p. 750. ^ a b c russell & norvig ( 2021 ), p. 17. ^ a b c d e f g russell & norvig ( 2021 ), p. 785. ^ a b schmidhuber ( 2022 ), sect. 5. ^ schmidhuber ( 2022 ), sect. 6. ^ a b c schmidhuber ( 2022 ), sect. 7. ^ schmidhuber ( 2022 ), sect. 8. ^ quoted in christian ( 2020, p. 22 ) ^ metz, cade ; weise, karen ( 5 may 2025 ). "" a. i. hallucinations are getting worse, even as new systems become more powerful "". the new york times. issn 0362 - 4331. retrieved 6 may 2025. ^ smith ( 2023 ). ^ "" explained : generative ai "". mit news | massachusetts institute of technology. 9 november 2023. ^ "" ai writing and content creation tools "". mit sloan teaching & learning technologies. archived from the original on 25 december 2023. retrieved 25 december 2023. ^ marmouyet ( 2023 ). ^ kobielus ( 2019 ). ^ thomason, james ( 21 may 2024 ). "" mojo rising : the resurgence of ai - first programming languages "". venturebeat."
q_060,hybrid,0.0,0.0,0.0,0.0,evelyn everett - green,"##elbach, sven ( 2022 ). "" pre - trained language models "". foundation models for natural language processing. artificial intelligence : foundations, theory, and algorithms. pp. 19 – 78. doi : 10. 1007 / 978 - 3 - 031 - 23190 - 2 _ 2. isbn 978 - 3 - 031 - 23190 - 2. ^ dodge, jesse ; sap, maarten ; marasovic, ana ; agnew, william ; ilharco, gabriel ; groeneveld, dirk ; mitchell, margaret ; gardner, matt ( 2021 ). "" documenting large webtext corpora : a case study on the colossal clean crawled corpus "" ( pdf ). emnlp. arxiv : 2104. 08758. doi : 10. 1145 / 3571730. ^ lee, katherine ; ippolito, daphne ; nystrom, andrew ; zhang, chiyuan ; eck, douglas ; callison - burch, chris ; carlini, nicholas ( may 2022 ). "" deduplicating training data makes language models better "" ( pdf ). proceedings of the 60th annual meeting of the association for computational linguistics ( volume 1 : long papers ). pp. 8424 – 8445. doi : 10. 18653 / v1 / 2022. acl - long. 577. ^ li, yuanzhi ; bubeck, sebastien ; eldan, ronen ; del giorno"
q_061,hybrid,0.0,0.0,0.0,0.0,chilean club fc san juan .,"norvig : "" stong ai – the assertion that machines that do so are actually thinking ( as opposed to simulating thinking ). "" references ^ a b c russell & norvig ( 2021 ), pp. 1 – 4. ^ ai set to exceed human brain power archived 19 february 2008 at the wayback machine cnn. com ( 26 july 2006 ) ^ kaplan, andreas ; haenlein, michael ( 2019 ). "" siri, siri, in my hand : who ' s the fairest in the land? on the interpretations, illustrations, and implications of artificial intelligence "". business horizons. 62 : 15 – 25. doi : 10. 1016 / j. bushor. 2018. 08. 004. [ the question of the source is a pastiche of : snow white ] ^ russell & norvig ( 2021, § 1. 2 ). ^ "" tech companies want to build artificial general intelligence. but who decides when agi is attained? "". ap news. 4 april 2024. retrieved 20 may 2025. ^ a b dartmouth workshop : russell & norvig ( 2021, p. 18 ), mccorduck ( 2004, pp. 111 – 136 ), nrc ( 1999, pp. 200 – 201 ) the proposal : mccarthy et al. ( 1955 ) ^ a b successful programs of the 1960s : mccorduck ( 2004, pp. 243 – 252 ), crevier ( 1993, pp. 52 –"
q_062,hybrid,0.0,0.0,0.0,0.00784313725490196,social cognition and social psychology,"ai companies. philosophy main article : philosophy of artificial intelligence philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. another major focus has been whether machines can be conscious, and the associated ethical implications. many other topics in philosophy are relevant to ai, such as epistemology and free will. for example, debates center on whether machines can genuinely understand meaning, whether they possess autonomous decision - making capabilities, and to what extent their actions can be considered intentional rather than merely the result of algorithmic processes. rapid advancements have intensified public discussions on the philosophy and ethics of ai. defining artificial intelligence see also : synthetic intelligence, intelligent agent, artificial mind, virtual intelligence, and dartmouth workshop alan turing wrote in 1950 "" i propose to consider the question ' can machines think '? "" he advised changing the question from whether a machine "" thinks "", to "" whether or not it is possible for machinery to show intelligent behaviour "". he devised the turing test, which measures the ability of a machine to simulate human conversation. since we can only observe the behavior of the machine, it does not matter if it is "" actually "" thinking or literally has a "" mind "". turing notes that we can not determine these things about other people but "" it is usual to have a polite convention that everyone thinks. "" the turing test can provide some evidence of intelligence, but it penalizes non - human intelligent behavior. russell and norvig agree with turing that intelligence must be defined in"
q_063,hybrid,0.0,0.0,0.0,0.0,swedenborg ' s journal of dreams ),"11 - 17. retrieved 2023 - 03 - 14. ^ fathallah, nadeen ; das, arunav ; de giorgis, stefano ; poltronieri, andrea ; haase, peter ; kovriguina, liubov ( 2024 - 05 - 26 ). neon - gpt : a large language model - powered pipeline for ontology learning ( pdf ). extended semantic web conference 2024. hersonissos, greece. ^ manning, christopher d. ( 2022 ). "" human language understanding & reasoning "". daedalus. 151 ( 2 ) : 127 – 138. doi : 10. 1162 / daed _ a _ 01905. s2cid 248377870. archived from the original on 2023 - 11 - 17. retrieved 2023 - 03 - 09. ^ kaplan, jared ; mccandlish, sam ; henighan, tom ; brown, tom b. ; chess, benjamin ; child, rewon ; gray, scott ; radford, alec ; wu, jeffrey ; amodei, dario ( 2020 ). "" scaling laws for neural language models "". arxiv : 2001. 08361 [ cs. lg ]. ^ vaswani, ashish ; shazeer, noam ; parmar, niki ; uszkoreit, jakob ; jones, llion ; gomez, aidan n ; kaiser, łukasz ; polosukhin"
q_064,hybrid,0.0,0.0,0.0,0.008368200836820083,ordinary mode of perception,"for artificial intelligence and cryptocurrency. the report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole japanese nation. prodigious power consumption by ai is responsible for the growth of fossil fuel use, and might delay closings of obsolete, carbon - emitting coal energy facilities. there is a feverish rise in the construction of data centers throughout the us, making large technology firms ( e. g., microsoft, meta, google, amazon ) into voracious consumers of electric power. projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. a chatgpt search involves the use of 10 times the electrical energy as a google search. the large firms are in haste to find power sources – from nuclear energy to geothermal to fusion. the tech firms argue that – in the long view – ai will be eventually kinder to the environment, but they need the energy now. ai makes the power grid more efficient and "" intelligent "", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms. a 2024 goldman sachs research paper, ai data centers and the coming us power demand surge, found "" us power demand ( is ) likely to experience growth not seen in a generation.... "" and forecasts that, by 2030, us data centers will consume 8 % of us power, as opposed to"
q_065,hybrid,0.0,0.0,0.0,0.0,b - mode i,"} } \ right ) v \ end { aligned } } } the following matrix is commonly used in decoder self - attention modules, called "" causal masking "" : m causal = [ 0 − ∞ − ∞ … − ∞ 0 0 − ∞ … − ∞ 0 0 0 … − ∞ [UNK] [UNK] [UNK] [UNK] [UNK] 0 0 0 … 0 ] { \ displaystyle m _ { \ text { causal } } = { \ begin { bmatrix } 0 & - \ infty & - \ infty & \ dots & - \ infty \ \ 0 & 0 & - \ infty & \ dots & - \ infty \ \ 0 & 0 & 0 & \ dots & - \ infty \ \ \ vdots & \ vdots & \ vdots & \ ddots & \ vdots \ \ 0 & 0 & 0 & \ dots & 0 \ end { bmatrix } } } in words, it means that each token can pay attention to itself, and every token before it, but not any after it. a non - masked attention module can be thought of as a masked attention module where the mask has all entries zero. as an example of an uncommon use of mask matrix, the xlnet considers all masks of the form p m causal p − 1 { \ displaystyle pm _ { \ text { causal } } p ^ { - 1 } }, where p { \ displaystyle p }"
q_066,hybrid,0.0,0.0,0.0,0.025974025974025976,"  cite book   : isbn / date incompatibility ( help )  eckhart, meister ( 1994 ). "" on detachment and possessing god "". selected writings. oliver davies. london : penguin books. p. 9. isbn 0 - 14 - 043343 - 0. oclc 31240752.","' s neuroscience to the birth of cybernetics "". the neuroscientist. 31 ( 1 ) : 14 – 30. doi : 10. 1177 / 10738584231179932. hdl : 10261 / 348372. issn 1073 - 8584. pmid 37403768. ^ ramon y cajal, santiago ( 1909 ). histologie du systeme nerveux de l ' homme & des vertebres. vol. ii. foyle special collections library king ' s college london. paris : a. maloine. p. 149. ^ de no, r. lorente ( 1933 - 08 - 01 ). "" vestibulo - ocular reflex arc "". archives of neurology and psychiatry. 30 ( 2 ) : 245. doi : 10. 1001 / archneurpsyc. 1933. 02240140009001. issn 0096 - 6754. ^ larriva - sahd, jorge a. ( 2014 - 12 - 03 ). "" some predictions of rafael lorente de no 80 years later "". frontiers in neuroanatomy. 8 : 147. doi : 10. 3389 / fnana. 2014. 00147. issn 1662 - 5129. pmc 4253658. pmid 25520630. ^ "" reverberating circuit "". oxford reference. retrieved 2024 - 07 -"
q_067,hybrid,0.0,0.0,0.0,0.0,virtuous,"). "" robot rights violate human rights, experts warn eu "". euronews. archived from the original on 19 september 2024. retrieved 23 february 2024. ^ the intelligence explosion and technological singularity : russell & norvig ( 2021, pp. 1004 – 1005 ), omohundro ( 2008 ), kurzweil ( 2005 ) i. j. good ' s "" intelligence explosion "" : good ( 1965 ) vernor vinge ' s "" singularity "" : vinge ( 1993 ) ^ russell & norvig ( 2021 ), p. 1005. ^ transhumanism : moravec ( 1988 ), kurzweil ( 2005 ), russell & norvig ( 2021, p. 1005 ) ^ ai as evolution : edward fredkin is quoted in mccorduck ( 2004, p. 401 ), butler ( 1863 ), dyson ( 1998 ) ^ ai in myth : mccorduck ( 2004, pp. 4 – 5 ) ^ mccorduck ( 2004 ), pp. 340 – 400. ^ buttazzo ( 2001 ). ^ anderson ( 2008 ). ^ mccauley ( 2007 ). ^ galvan ( 1997 ). textbooks luger, george ; stubblefield, william ( 2004 ). artificial intelligence : structures and strategies for complex problem solving ( 5th ed. ). benjamin / cummings. isbn 978 - 0 - 8053 - 4780 - 7. archived from the original"
q_068,hybrid,0.0,0.0,0.0,0.08247422680412372,"fcforum : networks for a r - evolution. what the internet has done for us, and what we can now do for it fcforum : networks for a r - evolution. what the internet has done for us, and what we can now do for it mayo fuster morell personal website : http : / / www. onlinecreation. info xnet : http : / /","much of what people know is not represented as "" facts "" or "" statements "" that they could express verbally ). there is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for ai applications. planning and decision - making an "" agent "" is anything that perceives and takes actions in the world. a rational agent has goals or preferences and takes actions to make them happen. in automated planning, the agent has a specific goal. in automated decision - making, the agent has preferences — there are some situations it would prefer to be in, and some situations it is trying to avoid. the decision - making agent assigns a number to each situation ( called the "" utility "" ) that measures how much the agent prefers it. for each possible action, it can calculate the "" expected utility "" : the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. it can then choose the action with the maximum expected utility. in classical planning, the agent knows exactly what the effect of any action will be. in most real - world problems, however, the agent may not be certain about the situation they are in ( it is "" unknown "" or "" unobservable "" ) and it may not know for certain what will happen after each possible action ( it is not "" deterministic "" ). it must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked. in some problems"
q_069,hybrid,0.0,0.0,0.0,0.008230452674897118,ordinary mode of perception,"such as llama 2, mistral or stable diffusion, have been made open - weight, meaning that their architecture and trained parameters ( the "" weights "" ) are publicly available. open - weight models can be freely fine - tuned, which allows companies to specialize them with their own data and for their own use - case. open - weight models are useful for research and innovation but can also be misused. since they can be fine - tuned, any built - in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. some researchers warn that future ai models may develop dangerous capabilities ( such as the potential to drastically facilitate bioterrorism ) and that once released on the internet, they cannot be deleted everywhere if needed. they recommend pre - release audits and cost - benefit analyses. frameworks artificial intelligence projects can be guided by ethical considerations during the design, development, and implementation of an ai system. an ai framework such as the care and act framework, developed by the alan turing institute and based on the sum values, outlines four main ethical dimensions, defined as follows : respect the dignity of individual people connect with other people sincerely, openly, and inclusively care for the wellbeing of everyone protect social values, justice, and the public interest other developments in ethical frameworks include those decided upon during the asilomar conference, the montreal declaration for responsible ai, and the ieee ' s ethics of autonomous systems initiative, among others ; however"
q_070,hybrid,0.0,0.0,0.0,0.07920792079207921,"the co - captain of the flight said in an interview with hstv, a video service of helsingin sanomat that he thought of the incident again after malaysia airlines flight 17 had been shot down in ukraine on 17 july 2014",". reuters. 2025 - 06 - 25. retrieved 2025 - 06 - 26. ^ "" meta scores victory in ai copyright case "". wired. 2025 - 06 - 25. retrieved 2025 - 06 - 26. ^ "" openai defeats news outlets ' copyright lawsuit over ai training for now "". reuters. 2024 - 11 - 07. retrieved 2024 - 11 - 08. ^ "" openai erases evidence in training data lawsuit "". the verge. 2024 - 11 - 21. retrieved 2024 - 11 - 22. ^ peng, zhencan ; wang, zhizhi ; deng, dong ( 13 june 2023 ). "" near - duplicate sequence search at scale for large language model memorization evaluation "" ( pdf ). proceedings of the acm on management of data. 1 ( 2 ) : 1 – 18. doi : 10. 1145 / 3589324. s2cid 259213212. archived ( pdf ) from the original on 2024 - 08 - 27. retrieved 2024 - 01 - 20. citing lee et al 2022. ^ peng, wang & deng 2023, p. 8. ^ stephen council ( 1 dec 2023 ). "" how googlers cracked an sf rival ' s tech model with a single word "". sfgate. archived from the original on 16 december 2023. ^ "" prepare for truly useful large language models "". nature biomedical engineering. 7 ( 2 ) : 85"
q_071,hybrid,0.0,0.0,0.0,0.0,flight 148 was improved,"y } } _ { 2 }, \ dots, { \ hat { y } } _ { l } ) }. the problem is that if the model makes a mistake early on, say at y ^ 2 { \ displaystyle { \ hat { y } } _ { 2 } }, then subsequent tokens are likely to also be mistakes. this makes it inefficient for the model to obtain a learning signal, since the model would mostly learn to shift y ^ 2 { \ displaystyle { \ hat { y } } _ { 2 } } towards y 2 { \ displaystyle y _ { 2 } }, but not the others. teacher forcing makes it so that the decoder uses the correct output sequence for generating the next entry in the sequence. so for example, it would see ( y 1, …, y k ) { \ displaystyle ( y _ { 1 }, \ dots, y _ { k } ) } in order to generate y ^ k + 1 { \ displaystyle { \ hat { y } } _ { k + 1 } }. gradient descent main articles : gradient descent and vanishing gradient problem gradient descent is a first - order iterative optimization algorithm for finding the minimum of a function. in neural networks, it can be used to minimize the error term by changing each weight in proportion to the derivative of the error with respect to that weight, provided the non - linear activation functions are differentiable. the standard method for training rn"
q_072,hybrid,0.0,0.0,0.0,0.0,wretched,"agreeability observed across multi - turn interactions and productized assistants. continued sycophancy has led to the observation of getting "" 1 - shotted "", denoting instances where conversational interaction with a large language model produces a lasting change in a user ' s beliefs or decisions, similar to the negative effects of psychedelics, and controlled experiments show that short llm dialogues can generate measurable opinion and confidence shifts comparable to human interlocutors. empirical analyses attribute part of the effect to human preference signals and preference models that reward convincingly written agreeable responses, and subsequent work has extended evaluation to multi - turn benchmarks and proposed interventions such as synthetic - data finetuning, adversarial evaluation, targeted preference - model reweighting, and multi - turn sycophancy benchmarks to measure persistence and regression risk. industry responses have combined research interventions with product controls, for example google and other labs publishing synthetic - data and fine - tuning interventions and openai rolling back an overly agreeable gpt - 4o update while publicly describing changes to feedback collection, personalization controls, and evaluation procedures to reduce regression risk and improve long - term alignment with user - level safety objectives. mainstream culture has reflected anxieties about this dynamic where south park satirized overreliance on chatgpt and the tendency of assistants to flatter user beliefs in season 27 episode "" sickofancy "", and continued the themes across the following season, which commentators interpreted as a critique of"
q_073,hybrid,0.0,0.0,0.0,0.0,blackrock microsystems official website cyber,"hp labs describes a system of cortical computing with memristive nanodevices. the memristors ( memory resistors ) are implemented by thin film materials in which the resistance is electrically tuned via the transport of ions or oxygen vacancies within the film. darpa ' s synapse project has funded ibm research and hp labs, in collaboration with the boston university department of cognitive and neural systems ( cns ), to develop neuromorphic architectures that may be based on memristive systems. memristive networks are a particular type of physical neural network that have very similar properties to ( little - ) hopfield networks, as they have continuous dynamics, a limited memory capacity and natural relaxation via the minimization of a function which is asymptotic to the ising model. in this sense, the dynamics of a memristive circuit have the advantage compared to a resistor - capacitor network to have a more interesting non - linear behavior. from this point of view, engineering analog memristive networks account for a peculiar type of neuromorphic engineering in which the device behavior depends on the circuit wiring or topology. the evolution of these networks can be studied analytically using variations of the caravelli - traversa - di ventra equation. continuous - time a continuous - time recurrent neural network ( ctrnn ) uses a system of ordinary differential equations to model the effects on a neuron of the incoming inputs"
q_074,hybrid,0.0,0.0,0.0,0.0,bac,"one of which is their ambiguity. several works use ai to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. this appears in karel capek ' s r. u. r., the films a. i. artificial intelligence and ex machina, as well as the novel do androids dream of electric sheep?, by philip k. dick. dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence. see also artificial consciousness – field in cognitive science artificial intelligence and elections – impact of ai on political elections artificial intelligence content detection – software to detect ai - generated content artificial intelligence in wikimedia projects – use of artificial intelligence to develop wikipedia and other wikimedia projects association for the advancement of artificial intelligence ( aaai ) behavior selection algorithm – algorithm that selects actions for intelligent agents business process automation – automation of business processes case - based reasoning – process of solving new problems based on the solutions of similar past problems computational intelligence – ability of a computer to learn a specific task from data or experimental observation darwin eu – a european union initiative coordinated by the european medicines agency ( ema ) to generate and utilize real world evidence ( rwe ) to support the evaluation and supervision of medicines across the eu digital immortality – hypothetical concept of storing a personality in digital form emergent algorithm – algorithm exhibiting emergent behavior female gendering of ai technologies – gender biases in"
q_075,hybrid,0.0,0.0,0.0,0.0," tupelocetus palmeri , whale skull | charleston museum "".",", johannes ; horvitz, eric ; kamar, ece ; lee, peter ; lee, yin tat ; li, yuanzhi ; lundberg, scott ; nori, harsha ; palangi, hamid ; ribeiro, marco tulio ; zhang, yi ( 2023 ). "" machine culture "". nature human behaviour. 7 ( 11 ) : 1855 – 1868. arxiv : 2303. 12712. doi : 10. 1038 / s41562 - 023 - 01742 - 2. pmid 37985914. ^ "" anthropic ceo dario amodei pens a smart look at our ai future "". fast company. october 17, 2024. ^ "" chatgpt is more like an ' alien intelligence ' than a human brain, says futurist "". zdnet. 2023. archived from the original on 12 june 2023. retrieved 12 june 2023. ^ a b newport, cal ( 13 april 2023 ). "" what kind of mind does chatgpt have? "". the new yorker. archived from the original on 12 june 2023. retrieved 12 june 2023. ^ roose, kevin ( 30 may 2023 ). "" why an octopus - like creature has come to symbolize the state of a. i. "" the new york times. archived from the original on 30 may 2023. retrieved 12 june 2023. ^ "" the a to"
q_076,hybrid,0.0,0.0,0.0,0.008368200836820083,ordinary mode of perception,"math benchmark problems. alternatively, dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such as alphatensor, alphageometry, alphaproof and alphaevolve all from google deepmind, llemma from eleutherai or julius. when natural language is used to describe mathematical problems, converters can transform such prompts into a formal language such as lean to define mathematical tasks. the experimental model gemini deep think accepts natural language prompts directly and achieved gold medal results in the international math olympiad of 2025. some models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics. topological deep learning integrates various topological approaches. finance finance is one of the fastest growing sectors where applied ai tools are being deployed : from retail online banking to investment advice and insurance, where automated "" robot advisers "" have been in use for some years. according to nicolas firzli, director of the world pensions & investments forum, it may be too early to see the emergence of highly innovative ai - informed financial products and services. he argues that "" the deployment of ai tools will simply further automatise things : destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but i ' m not sure it will unleash a new wave of [ e. g., sophisticated ] pension innovation. "" military main article : military applications of"
q_077,hybrid,0.0,0.0,0.0,0.0, ingrid johnsen haas - msepp - psepp - psepp - msepp - msepp - msepp - msepp - msepp - msepp - msepp - msepp - msepp - m,"- video, language translation, more "". venturebeat. 2022 - 11 - 02. retrieved 2022 - 11 - 09. ^ vincent, james ( 2022 - 09 - 29 ). "" meta ' s new text - to - video ai generator is like dall - e for video "". the verge. retrieved 2022 - 11 - 09. ^ "" previous shared tasks | conll "". www. conll. org. retrieved 2021 - 01 - 11. ^ "" cognition "". lexico. oxford university press and dictionary. com. archived from the original on july 15, 2020. retrieved 6 may 2020. ^ "" ask the cognitive scientist "". american federation of teachers. 8 august 2014. cognitive science is an interdisciplinary field of researchers from linguistics, psychology, neuroscience, philosophy, computer science, and anthropology that seek to understand the mind. ^ robinson, peter ( 2008 ). handbook of cognitive linguistics and second language acquisition. routledge. pp. 3 – 8. isbn 978 - 0 - 805 - 85352 - 0. ^ lakoff, george ( 1999 ). philosophy in the flesh : the embodied mind and its challenge to western philosophy ; appendix : the neural theory of language paradigm. new york basic books. pp. 569 – 583. isbn 978 - 0 - 465 - 05674 - 3. ^ strauss, claudia ( 1999 ). a cognitive theory of cultural meaning. cambridge university press. pp. 156 – 164. isbn"
q_078,hybrid,0.0,0.0,0.0,0.009132420091324202,ordinary mode of perception,"main article : prompt engineering in 2020, openai researchers demonstrated that their new model gpt - 3 could understand what format to use given a few rounds of q and a ( or other type of task ) in the input data as example, thanks in part due to the rlhf technique. this technique, called few - shot prompting, allows llms to be adapted to any task without requiring fine - tuning. also in 2022, it was found that the base gpt - 3 model can generate an instruction based on user input. the generated instruction along with user input is then used as input to another instance of the model under a "" instruction : [... ], input : [... ], output : "" format. the other instance is able to complete the output and often produces the correct answer in doing so. the ability to "" self - instruct "" makes llms able to bootstrap themselves toward a correct answer. dialogue processing ( chatbot ) an llm can be turned into a chatbot by specializing it for conversation. user input is prefixed with a marker such as "" q : "" or "" user : "" and the llm is asked to predict the output after a fixed "" a : "" or "" assistant : "". this type of model became commercially available in 2022 with chatgpt, a sibling model of instructgpt fine - tuned to accept and produce dialog - formatted text based on gpt - 3."
q_079,hybrid,0.0,0.0,0.0,0.035398230088495575,is a relic of the early 20th century,"analysis, coreference ; see natural language understanding below ). semantic role labelling ( see also implicit semantic role labelling below ) given a single sentence, identify and disambiguate semantic predicates ( e. g., verbal frames ), then identify and classify the frame elements ( semantic roles ). discourse ( semantics beyond individual sentences ) coreference resolution given a sentence or larger chunk of text, determine which words ( "" mentions "" ) refer to the same objects ( "" entities "" ). anaphora resolution is a specific example of this task, and is specifically concerned with matching up pronouns with the nouns or names to which they refer. the more general task of coreference resolution also includes identifying so - called "" bridging relationships "" involving referring expressions. for example, in a sentence such as "" he entered john ' s house through the front door "", "" the front door "" is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of john ' s house ( rather than of some other structure that might also be referred to ). discourse analysis this rubric includes several related tasks. one task is discourse parsing, i. e., identifying the discourse structure of a connected text, i. e. the nature of the discourse relationships between sentences ( e. g. elaboration, explanation, contrast ). another possible task is recognizing and classifying the speech acts in a chunk of"
q_080,hybrid,0.0,0.0,0.0,0.0091324200913242,infinite,"learning for the recognition of sequences can also be implemented by a more biological - based model which uses the silencing mechanism exhibited in neurons with a relatively high frequency spiking activity. additional stored states and the storage under direct control by the network can be added to both infinite - impulse and finite - impulse networks. another network or graph can also replace the storage if that incorporates time delays or has feedback loops. such controlled states are referred to as gated states or gated memory and are part of long short - term memory networks ( lstms ) and gated recurrent units. this is also called feedback neural network ( fnn ). libraries modern libraries provide runtime - optimized implementations of the above functionality or allow to speed up the slow loop by just - in - time compilation. apache singa caffe : created by the berkeley vision and learning center ( bvlc ). it supports both cpu and gpu. developed in c + +, and has python and matlab wrappers. chainer : fully in python, production support for cpu, gpu, distributed training. deeplearning4j : deep learning in java and scala on multi - gpu - enabled spark. flux : includes interfaces for rnns, including grus and lstms, written in julia. keras : high - level api, providing a wrapper to many other deep learning libraries. microsoft cognitive toolkit mxnet : an open - source deep learning framework used to train"
q_081,hybrid,0.0,0.0,0.0,0.04511278195488721,zombie plague zone mutant thought about adding bumps on the skin that modeling performed directly on the face of extra - zombie - victim with toilet paper,"##out heuristic. when a programmatic world model is not available, an llm can also be prompted with a description of the environment to act as world model. for open - ended exploration, an llm can be used to score observations for their "" interestingness "", which can be used as a reward signal to guide a normal ( non - llm ) reinforcement learning agent. alternatively, it can propose increasingly difficult tasks for curriculum learning. instead of outputting individual actions, an llm planner can also construct "" skills "", or functions for complex action sequences. the skills can be stored and later invoked, allowing increasing levels of abstraction in planning. multiple agents with memory can interact socially. reasoning llms are conventionally trained to generate an output without generating intermediate steps. as a result, their performance tends to be subpar on complex questions requiring ( at least in humans ) intermediate steps of thought. early research demonstrated that inserting intermediate "" scratchpad "" computations could improve performance on such tasks. later methods overcame this deficiency more systematically by breaking tasks into smaller steps for the llm, either manually or automatically. chaining prompt chaining was introduced in 2022. in this method, a user manually breaks a complex problem down into several steps. in each step, the llm receives as input a prompt telling it what to do and some results from preceding steps. the result from one step is then reused in a next step, until a final answer is reached. the ability of an"
q_082,hybrid,0.0,0.0,0.0,0.012658227848101267,"a biography of warren beatty and desert eyes , a biography of warren beatty and speculative fiction , a biography of nicole kidman , a biography of kieran hickey , a documentary filmmaker , and a biography of sarah green","2102. 12092 ^ yu, jiahui ; xu, yuanzhong ; koh, jing yu ; luong, thang ; baid, gunjan ; wang, zirui ; vasudevan, vijay ; ku, alexander ; yang, yinfei ( 2022 - 06 - 21 ), scaling autoregressive models for content - rich text - to - image generation, arxiv : 2206. 10789 ^ kariampuzha, william ; alyea, gioconda ; qu, sue ; sanjak, jaleal ; mathe, ewy ; sid, eric ; chatelaine, haley ; yadaw, arjun ; xu, yanji ; zhu, qian ( 2023 ). "" precision information extraction for rare disease epidemiology at scale "". journal of translational medicine. 21 ( 1 ) : 157. doi : 10. 1186 / s12967 - 023 - 04011 - y. pmc 9972634. pmid 36855134. further reading alexander rush, the annotated transformer archived 2021 - 09 - 22 at the wayback machine, harvard nlp group, 3 april 2018 phuong, mary ; hutter, marcus ( 2022 ). "" formal algorithms for transformers "". arxiv : 2207. 09238 [ cs. lg ]. ferrando, javier ; sarti,"
q_083,hybrid,0.0,0.0,0.0,0.0,w,"s. ). ^ nilsson ( 1983 ), p. 10. ^ haugeland ( 1985 ), pp. 112 – 117. ^ physical symbol system hypothesis : newell & simon ( 1976, p. 116 ) historical significance : mccorduck ( 2004, p. 153 ), russell & norvig ( 2021, p. 19 ) ^ moravec ' s paradox : moravec ( 1988, pp. 15 – 16 ), minsky ( 1986, p. 29 ), pinker ( 2007, pp. 190 – 191 ) ^ dreyfus ' critique of ai : dreyfus ( 1972 ), dreyfus & dreyfus ( 1986 ) historical significance and philosophical implications : crevier ( 1993, pp. 120 – 132 ), mccorduck ( 2004, pp. 211 – 239 ), russell & norvig ( 2021, pp. 981 – 982 ), fearn ( 2007, chpt. 3 ) ^ crevier ( 1993 ), p. 125. ^ langley ( 2011 ). ^ katz ( 2012 ). ^ neats vs. scruffies, the historic debate : mccorduck ( 2004, pp. 421 – 424, 486 – 489 ), crevier ( 1993, p. 168 ), nilsson ( 1983, pp. 10 – 11 ), russell & norvig ( 2021, p. 24 ) a classic example of the "" scruffy "" approach to"
q_084,hybrid,0.0,0.0,0.0,0.0,e2xx,"cost of training the model, in flops. n { \ displaystyle n } is the number of parameters in the model. d { \ displaystyle d } is the number of tokens in the training set. l { \ displaystyle l } is the average negative log - likelihood loss per token ( nats / token ), achieved by the trained llm on the test dataset. and the statistical hyper - parameters are c 0 = 6 { \ displaystyle c _ { 0 } = 6 }, meaning that it costs 6 flops per parameter to train on one token. note that training cost is much higher than inference cost, where it costs 1 to 2 flops per parameter to infer on one token. α = 0. 34, β = 0. 28, a = 406. 4, b = 410. 7, l 0 = 1. 69 { \ displaystyle \ alpha = 0. 34, \ beta = 0. 28, a = 406. 4, b = 410. 7, l _ { 0 } = 1. 69 } emergent abilities at point ( s ) referred to as breaks, the lines change their slopes, appearing on a linear - log plot as a series of linear segments connected by arcs. performance of bigger models on various tasks, when plotted on a log - log scale, appears as a linear extrapolation of performance achieved by smaller models. however, this linearity may be punctuated by "" break ( s ) """
q_085,hybrid,0.0,0.0,0.0,0.026845637583892613,"farm , he was able to draw a few scenes from the farm .",". doi : 10. 1103 / revmodphys. 39. 883. ^ glauber, roy j. ( february 1963 ). "" roy j. glauber "" time - dependent statistics of the ising model "" "". journal of mathematical physics. 4 ( 2 ) : 294 – 307. doi : 10. 1063 / 1. 1703954. retrieved 2021 - 03 - 21. ^ sherrington, david ; kirkpatrick, scott ( 1975 - 12 - 29 ). "" solvable model of a spin - glass "". physical review letters. 35 ( 26 ) : 1792 – 1796. bibcode : 1975phrvl.. 35. 1792s. doi : 10. 1103 / physrevlett. 35. 1792. issn 0031 - 9007. ^ hopfield, j. j. ( 1982 ). "" neural networks and physical systems with emergent collective computational abilities "". proceedings of the national academy of sciences. 79 ( 8 ) : 2554 – 2558. bibcode : 1982pnas... 79. 2554h. doi : 10. 1073 / pnas. 79. 8. 2554. pmc 346238. pmid 6953413. ^ hopfield, j. j. ( 1984 ). "" neurons with graded response have collective computational properties like those of two - state neurons "". proceedings of the national academy of sciences. 81"
q_086,hybrid,0.0,0.0,0.0,0.0,wretched,"of after ) multihead attention and feedforward layers stabilizes training, not requiring learning rate warmup. this is the "" pre - ln transformer "" and is more commonly used, compared to the original "" post - ln transformer "". pretrain - finetune transformers typically are first pretrained by self - supervised learning on a large generic dataset, followed by supervised fine - tuning on a small task - specific dataset. the pretrain dataset is typically an unlabeled large corpus, such as the pile. tasks for pretraining and fine - tuning commonly include : language modeling next - sentence prediction question answering reading comprehension sentiment analysis paraphrasing the t5 transformer report documents a large number of natural language pretraining tasks. some examples are : restoring or repairing incomplete or corrupted text. for example, the input, "" thank you ~ ~ me to your party ~ ~ week "", might generate the output, "" thank you for inviting me to your party last week "". translation between natural languages ( machine translation ) judging the pragmatic acceptability of natural language. for example, the following sentence might be judged "" not acceptable "", because even though it is syntactically well - formed, it is improbable in ordinary human usage : the course is jumping well. note that while each of these tasks is trivial or obvious for human native speakers of the language ( or languages ), they have typically proved challenging for"
q_087,hybrid,0.0,0.0,0.0,0.0,te,"280. doi : 10. 1016 / j. techfore. 2016. 08. 019. "" from not working to neural networking "". the economist. 2016. archived from the original on 31 december 2016. retrieved 26 april 2018. galvan, jill ( 1 january 1997 ). "" entering the posthuman collective in philip k. dick ' s "" do androids dream of electric sheep? "" "". science fiction studies. 24 ( 3 ) : 413 – 429. doi : 10. 1525 / sfs. 24. 3. 0413. jstor 4240644. geist, edward moore ( 9 august 2015 ). "" is artificial intelligence really an existential threat to humanity? "". bulletin of the atomic scientists. archived from the original on 30 october 2015. retrieved 30 october 2015. gibbs, samuel ( 27 october 2014 ). "" elon musk : artificial intelligence is our biggest existential threat "". the guardian. archived from the original on 30 october 2015. retrieved 30 october 2015. goffrey, andrew ( 2008 ). "" algorithm "". in fuller, matthew ( ed. ). software studies : a lexicon. cambridge, mass. : mit press. pp. 15 – 20. isbn 978 - 1 - 4356 - 4787 - 9. goldman, sharon ( 14 september 2022 ). "" 10 years later, deep learning ' revolution ' rages on, say ai pioneers hinton, lecu"
q_088,hybrid,0.0,0.0,0.0,0.02631578947368421,virgin islands as a state of virgin islands,"##yesha ; nambiar, vaishnavi ( 2024 ). "" role of artificial intelligence in the prevention of online child sexual abuse : a systematic review of literature "". journal of applied security research. 19 ( 4 ) : 586 – 627. doi : 10. 1080 / 19361610. 2024. 2331885. ^ razi, afsaneh ; kim, seunghyun ; alsoubai, ashwaq ; stringhini, gianluca ; solorio, thamar ; de choudhury, munmun ; wisniewski, pamela j. ( 13 october 2021 ). "" a human - centered systematic literature review of the computational approaches for online sexual risk detection "". proceedings of the acm on human - computer interaction. 5 ( cscw2 ) : 1 – 38. doi : 10. 1145 / 3479609. ^ ransbotham, sam ; kiron, david ; gerbert, philipp ; reeves, martin ( 6 september 2017 ). "" reshaping business with artificial intelligence "". mit sloan management review. archived from the original on 13 february 2024. ^ sun, yuran ; zhao, xilei ; lovreglio, ruggiero ; kuligowski, erica ( 2024 ). "" ai for large - scale evacuation modeling : promises and challenges "". interpretable machine learning for the analysis, design, assessment, and informed decision making"
q_089,hybrid,0.0,0.0,0.0,0.039999999999999994,"2010 buick lacrosse cxs was the first north american car to incorporate a front suspension design, marketed as hiper strut, designed to improve ride","quickly became "" ubiquitous "". though the original transformer has both encoder and decoder blocks, bert is an encoder - only model. academic and research usage of bert began to decline in 2023, following rapid improvements in the abilities of decoder - only models ( such as gpt ) to solve tasks via prompting. although decoder - only gpt - 1 was introduced in 2018, it was gpt - 2 in 2019 that caught widespread attention because openai claimed to have initially deemed it too powerful to release publicly, out of fear of malicious use. gpt - 3 in 2020 went a step further and as of 2025 is available only via api with no offering of downloading the model to execute locally. but it was the 2022 consumer - facing chatbot chatgpt that received extensive media coverage and public attention. the 2023 gpt - 4 was praised for its increased accuracy and as a "" holy grail "" for its multimodal capabilities. openai did not reveal the high - level architecture and the number of parameters of gpt - 4. the release of chatgpt led to an uptick in llm usage across several research subfields of computer science, including robotics, software engineering, and societal impact work. in 2024 openai released the reasoning model openai o1, which generates long chains of thought before returning a final answer. many llms with parameter counts comparable to those of openai ' s gpt series have been"
q_090,hybrid,0.0,0.0,0.0,0.013986013986013986,israeli scientists and engineers.,"; polosukhin, illia ( 2017 ). "" attention is all you need "". advances in neural information processing systems. 30. curran associates, inc. ^ oord, aaron van den ; kalchbrenner, nal ; kavukcuoglu, koray ( 2016 - 06 - 11 ). "" pixel recurrent neural networks "". proceedings of the 33rd international conference on machine learning. pmlr : 1747 – 1756. ^ a b cruse, holk ; neural networks as cybernetic systems, 2nd and revised edition ^ elman, jeffrey l. ( 1990 ). "" finding structure in time "". cognitive science. 14 ( 2 ) : 179 – 211. doi : 10. 1016 / 0364 - 0213 ( 90 ) 90002 - e. ^ jordan, michael i. ( 1997 - 01 - 01 ). "" serial order : a parallel distributed processing approach "". neural - network models of cognition — biobehavioral foundations. advances in psychology. vol. 121. pp. 471 – 495. doi : 10. 1016 / s0166 - 4115 ( 97 ) 80111 - 2. isbn 978 - 0 - 444 - 81931 - 4. s2cid 15375627. ^ gers, felix a. ; schraudolph, nicol n. ; schmidhuber, jurgen ( 2002 ). "" learning precise timing with lstm rec"
q_091,hybrid,0.0,0.0,0.0,0.014492753623188404,", and a miniature camera .","optimal large language models "". neurips : 30016 – 30030. isbn 978 - 1 - 7138 - 7108 - 8. ^ a b caballero, ethan ; gupta, kshitij ; rish, irina ; krueger, david ( 2022 ). "" broken neural scaling laws "". arxiv : 2210. 14891 [ cs. lg ]. ^ a b wei, jason ; tay, yi ; bommasani, rishi ; raffel, colin ; zoph, barret ; borgeaud, sebastian ; yogatama, dani ; bosma, maarten ; zhou, denny ; metzler, donald ; chi, ed h. ; hashimoto, tatsunori ; vinyals, oriol ; liang, percy ; dean, jeff ; fedus, william ( 31 august 2022 ). "" emergent abilities of large language models "". transactions on machine learning research. issn 2835 - 8856. archived from the original on 22 march 2023. retrieved 19 march 2023. ^ "" 137 emergent abilities of large language models "". jason wei. retrieved 2023 - 06 - 24. ^ bowman, samuel r. ( 2024 ). "" eight things to know about large language models "". critical ai. 2 ( 2 ). doi : 10. 1215 / 2834703x - 11556011. ^ hahn, michael ; goyal"
q_092,hybrid,0.0,0.0,0.0,0.0,),"). "" sequence to sequence learning with neural networks "" ( pdf ). electronic proceedings of the neural information processing systems conference. 27 : 5346. arxiv : 1409. 3215. bibcode : 2014arxiv1409. 3215s. ^ jozefowicz, rafal ; vinyals, oriol ; schuster, mike ; shazeer, noam ; wu, yonghui ( 2016 - 02 - 07 ). "" exploring the limits of language modeling "". arxiv : 1602. 02410 [ cs. cl ]. ^ gillick, dan ; brunk, cliff ; vinyals, oriol ; subramanya, amarnag ( 2015 - 11 - 30 ). "" multilingual language processing from bytes "". arxiv : 1512. 00103 [ cs. cl ]. ^ vinyals, oriol ; toshev, alexander ; bengio, samy ; erhan, dumitru ( 2014 - 11 - 17 ). "" show and tell : a neural image caption generator "". arxiv : 1411. 4555 [ cs. cv ]. ^ cho, kyunghyun ; van merrienboer, bart ; gulcehre, caglar ; bahdanau, dzmitry ; bougares, fethi ; schwenk, holger ; bengio, yoshua ( 2014 - 06"
q_093,hybrid,0.0,0.0,0.0,0.03508771929824561,nebraska lincoln researcher perceives the political attitudes of a nebraska lincoln researcher.,": discourse parsing, 2019 : semantic parsing ). increasing interest in multilinguality, and, potentially, multimodality ( english since 1999 ; spanish, dutch since 2002 ; german since 2003 ; bulgarian, danish, japanese, portuguese, slovenian, swedish, turkish since 2006 ; basque, catalan, chinese, greek, hungarian, italian, turkish since 2007 ; czech since 2009 ; arabic since 2012 ; 2017 : 40 + languages ; 2018 : 60 + / 100 + languages ) elimination of symbolic representations ( rule - based over supervised towards weakly supervised methods, representation learning and end - to - end systems ) cognition most higher - level nlp applications involve aspects that emulate intelligent behavior and apparent comprehension of natural language. more broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behavior represents one of the developmental trajectories of nlp ( see trends among conll shared tasks above ). cognition refers to "" the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses. "" cognitive science is the interdisciplinary, scientific study of the mind and its processes. cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. especially during the age of symbolic nlp, the area of computational linguistics maintained strong ties with cognitive studies. as an example, george lakoff offers a methodology to build natural language processing ( nlp ) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, with two defining"
q_094,hybrid,0.0,0.0,0.0,0.0,,""". transactions of the association for computational linguistics. 8 : 842 – 866. arxiv : 2002. 12327. doi : 10. 1162 / tacl _ a _ 00349. s2cid 211532403. archived from the original on 2022 - 04 - 03. retrieved 2024 - 01 - 21. ^ a b movva, rajiv ; balachandar, sidhika ; peng, kenny ; agostini, gabriel ; garg, nikhil ; pierson, emma ( 2024 ). "" topics, authors, and institutions in large language model research : trends from 17k arxiv papers "". proceedings of the 2024 conference of the north american chapter of the association for computational linguistics : human language technologies ( volume 1 : long papers ). pp. 1223 – 1243. arxiv : 2307. 10700. doi : 10. 18653 / v1 / 2024. naacl - long. 67. retrieved 2024 - 12 - 08. ^ hern, alex ( 14 february 2019 ). "" new ai fake text generator may be too dangerous to release, say creators "". the guardian. archived from the original on 14 february 2019. retrieved 20 january 2024. ^ "" chatgpt a year on : 3 ways the ai chatbot has completely changed the world in 12 months "". euronews. november 30, 2023. archived from the original"
q_095,hybrid,0.0,0.0,0.0,0.0,rave,"decoder transformer, then taking just the encoder. they are also referred to as "" all - to - all "" or "" bert - like "". a "" decoder - only "" transformer is not literally decoder - only, since without an encoder, the cross - attention mechanism has nothing to attend to. thus, the decoder layers in a decoder - only transformer is composed of just two sublayers : the causally masked self - attention, and the feedforward network. this is usually used for text generation and instruction following. the models in the gpt series and chinchilla series are decoder - only. they are also referred to as "" autoregressive "" or "" causal "". an "" encoder – decoder "" transformer is generally the same as the original transformer, with 2 sublayers per encoder layer and 3 sublayers per decoder layer, etc. they might have minor architectural improvements, such as alternative activation functions, changing the location of normalization, etc. this is also usually used for text generation and instruction following. the models in the t5 series are encoder – decoder. a "" prefixlm "" ( prefix language model ) is a decoder - only architecture, but with prefix masking, which is different from causal masking. specifically, it has mask of the form m prefixlm = [ 0 − ∞ 0 m causal ] { \ displays"
q_096,hybrid,0.0,0.0,0.0,0.015151515151515152,be the universal universal,"##ctional recurrent neural networks bidirectional rnn a bidirectional rnn ( birnn ) is composed of two rnns, one processing the input sequence in one direction, and another in the opposite direction. abstractly, it is structured as follows : the forward rnn processes in one direction : f θ ( x 0, h 0 ) = ( y 0, h 1 ), f θ ( x 1, h 1 ) = ( y 1, h 2 ), … { \ displaystyle f _ { \ theta } ( x _ { 0 }, h _ { 0 } ) = ( y _ { 0 }, h _ { 1 } ), f _ { \ theta } ( x _ { 1 }, h _ { 1 } ) = ( y _ { 1 }, h _ { 2 } ), \ dots } the backward rnn processes in the opposite direction : f θ ′ ′ ( x n, h n ′ ) = ( y n ′, h n − 1 ′ ), f θ ′ ′ ( x n − 1, h n − 1 ′ ) = ( y n − 1 ′, h n − 2 ′ ), … { \ displaystyle f ' _ { \ theta ' } ( x _ { n }, h _ { n } ' ) = ( y ' _ { n }, h _ { n - 1 } ' ), f ' _ { \ theta ' } ( x _"
q_097,hybrid,0.0,0.0,0.0,0.0,roger ackroyd,"##har ; mishra, shailesh ; endres, christoph ; holz, thorsten ; fritz, mario ( 2023 - 02 - 01 ). "" not what you ' ve signed up for : compromising real - world llm - integrated applications with indirect prompt injection "". proceedings of the 16th acm workshop on artificial intelligence and security. pp. 79 – 90. doi : 10. 1145 / 3605764. 3623985. isbn 979 - 8 - 4007 - 0260 - 0. ^ edwards, benj ( 2024 - 01 - 15 ). "" ai poisoning could turn models into destructive "" sleeper agents, "" says anthropic "". ars technica. retrieved 2025 - 07 - 19. ^ "" u. s. judge approves $ 1. 5 billion anthropic copyright settlement with authors "". reuters. 2025 - 09 - 25. retrieved 2025 - 09 - 26. ^ "" anthropic reaches $ 1. 5b settlement with authors over ai copyright claims "". associated press. 2025 - 09 - 25. retrieved 2025 - 09 - 26. ^ "" meta fends off authors ' u. s. copyright lawsuit over ai "". reuters. 2025 - 06 - 25. retrieved 2025 - 06 - 26. ^ "" meta scores victory in ai copyright case "". wired. 2025 - 06 - 25. retrieved 2025 - 06 - 26. ^ "" openai"
q_098,hybrid,0.0,0.0,0.0,0.008968609865470852,ordinary mode of perception,"##er architecture, introduced in 2017, replaced recurrence with self - attention, allowing efficient parallelization, longer context handling, and scalable training on unprecedented data volumes. this innovation enabled models like gpt, bert, and their successors, which demonstrated emergent behaviors at scale, such as few - shot learning and compositional reasoning. reinforcement learning, particularly policy gradient algorithms, has been adapted to fine - tune llms for desired behaviors beyond raw next - token prediction. reinforcement learning from human feedback ( rlhf ) applies these methods to optimize a policy, the llm ' s output distribution, against reward signals derived from human or automated preference judgments. this has been critical for aligning model outputs with user expectations, improving factuality, reducing harmful responses, and enhancing task performance. benchmark evaluations for llms have evolved from narrow linguistic assessments toward comprehensive, multi - task evaluations measuring reasoning, factual accuracy, alignment, and safety. hill climbing, iteratively optimizing models against benchmarks, has emerged as a dominant strategy, producing rapid incremental performance gains but raising concerns of overfitting to benchmarks rather than achieving genuine generalization or robust capability improvements. history the number of publications about large language models by year grouped by publication types. the training compute of notable large models in flops vs publication date over the period 2010 – 2024. for overall notable models ( top left ), frontier models ( top right ), top language models ( bottom left"
q_099,hybrid,0.0,0.0,0.0,0.0,mississauga,"s that are y s "" ). deductive reasoning in logic is the process of proving a new statement ( conclusion ) from other statements that are given and assumed to be true ( the premises ). proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules. given a problem and a set of premises, problem - solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. in the case of horn clauses, problem - solving search can be performed by reasoning forwards from the premises or backwards from the problem. in the more general case of the clausal form of first - order logic, resolution is a single, axiom - free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved. inference in both horn clause logic and first - order logic is undecidable, and therefore intractable. however, backward reasoning with horn clauses, which underpins computation in the logic programming language prolog, is turing complete. moreover, its efficiency is competitive with computation in other symbolic programming languages. fuzzy logic assigns a "" degree of truth "" between 0 and 1. it can therefore handle propositions that are vague and partially true. non - monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning."
