[
  {
    "id": "q_000",
    "question": "What is the name of the generative model that was used to generate fictitious wikipedia articles?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Transformer_(machine_learning)",
    "ground_truth_context": "introduction of a multi - head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence. its parallelizability was an important factor to its widespread use in large neural networks. ai boom era as early as spring 2017, even before the \" attention is all you need \" preprint was published, one of the co - authors applied the \" decoder - only \" variation of the architecture to generate fictitious wikipedia articles. transformer architecture is now used alongside many generative models that contribute to the ongoing ai boom. in language modelling, elmo ( 2018 ) was a bi - directional lstm that produces contextualized word embeddings, improving upon the line of research from bag of words and word2vec. it was followed by bert ( 2018 ), an encoder - only transformer model. in october 2019, google started using bert to process search queries. in 2020, google translate replaced the previous rnn - encoder \u2013 rnn - decoder model by a transformer - encoder \u2013 rnn - decoder model. starting in 2018, the openai gpt series of decoder - only transformers became state of the art in natural language generation. in 2022, a chatbot based on gpt - 3, chatgpt, became unexpectedly popular, triggering a boom around large language models. since 2020, transformers have been applied in modalities beyond text, including the",
    "chunk_id": "doc_fixed_000_chunk_06",
    "type": "factual"
  },
  {
    "id": "q_001",
    "question": "What is the most recent publication of the authors of \" self - attention with relative position representations \"?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Transformer_(machine_learning)",
    "ground_truth_context": "cs. cl ]. ^ press, ofir ; smith, noah a. ; lewis, mike ( 2021 - 08 - 01 ). \" train short, test long : attention with linear biases enables input length extrapolation \". arxiv : 2108. 12409 [ cs. cl ]. ^ shaw, peter ; uszkoreit, jakob ; vaswani, ashish ( 2018 ). \" self - attention with relative position representations \". arxiv : 1803. 02155 [ cs. cl ]. ^ ke, guolin ; he, di ; liu, tie - yan ( 2021 - 03 - 15 ), rethinking positional encoding in language pre - training, arxiv : 2006. 15595 ^ kwon, woosuk ; li, zhuohan ; zhuang, siyuan ; sheng, ying ; zheng, lianmin ; yu, cody hao ; gonzalez, joseph ; zhang, hao ; stoica, ion ( 2023 - 10 - 23 ). \" efficient memory management for large language model serving with pagedattention \". proceedings of the 29th symposium on operating systems principles. sosp ' 23. new york, ny, usa : association for computing machinery. pp. 611 \u2013 626. arxiv : 2309. 06180. doi : 10. 1145 / 3600006. 3613165. isbn 979 - 8 -",
    "chunk_id": "doc_fixed_000_chunk_91",
    "type": "inferential"
  },
  {
    "id": "q_002",
    "question": "What is the narrator 's opinion of the current advances in neural network structures ?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
    "ground_truth_context": "- term memory based deep recurrent neural networks for large vocabulary speech recognition \". arxiv : 1410. 4281 [ cs. cl ]. ^ dupond, samuel ( 2019 ). \" a thorough review on the current advance of neural network structures \". annual reviews in control. 14 : 200 \u2013 230. ^ abiodun, oludare isaac ; jantan, aman ; omolara, abiodun esther ; dada, kemi victoria ; mohamed, nachaat abdelatif ; arshad, humaira ( 2018 - 11 - 01 ). \" state - of - the - art in artificial neural network applications : a survey \". heliyon. 4 ( 11 ) e00938. bibcode : 2018heliy... 400938a. doi : 10. 1016 / j. heliyon. 2018. e00938. issn 2405 - 8440. pmc 6260436. pmid 30519653. ^ espinosa - sanchez, juan manuel ; gomez - marin, alex ; de castro, fernando ( 2023 - 07 - 05 ). \" the importance of cajal ' s and lorente de no ' s neuroscience to the birth of cybernetics \". the neuroscientist. 31 ( 1 ) : 14 \u2013 30. doi : 10. 1177 / 10738584231179932. hdl : 1026",
    "chunk_id": "doc_fixed_004_chunk_37",
    "type": "inferential"
  },
  {
    "id": "q_003",
    "question": "What is the date when the first ai chatbot was released?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Large_language_model",
    "ground_truth_context": "on 14 february 2019. retrieved 20 january 2024. ^ \" chatgpt a year on : 3 ways the ai chatbot has completely changed the world in 12 months \". euronews. november 30, 2023. archived from the original on january 14, 2024. retrieved january 20, 2024. ^ heaven, will ( march 14, 2023 ). \" gpt - 4 is bigger and better than chatgpt \u2014 but openai won ' t say why \". mit technology review. archived from the original on march 17, 2023. retrieved january 20, 2024. ^ metz, cade ( september 12, 2024 ). \" openai unveils new chatgpt that can reason through math and science \". the new york times. retrieved september 12, 2024. ^ \" parameters in notable artificial intelligence systems \". ourworldindata. org. november 30, 2023. retrieved january 20, 2024. ^ sharma, shubham ( 2025 - 01 - 20 ). \" open - source deepseek - r1 uses pure reinforcement learning to match openai o1 \u2014 at 95 % less cost \". venturebeat. retrieved 2025 - 01 - 26. ^ \" llama - mesh \". research. nvidia. com. 2024. retrieved 2025 - 10 - 30. ^ zia, dr tehseen ( 2024 - 01 - 08 ). \" unveiling of large multimo",
    "chunk_id": "doc_fixed_001_chunk_62",
    "type": "inferential"
  },
  {
    "id": "q_004",
    "question": "What is the main idea of the text?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
    "ground_truth_context": "deep learning for human action recognition \". in salah, albert ali ; lepri, bruno ( eds. ). human behavior unterstanding. lecture notes in computer science. vol. 7065. amsterdam, netherlands : springer. pp. 29 \u2013 39. doi : 10. 1007 / 978 - 3 - 642 - 25446 - 8 _ 4. isbn 978 - 3 - 642 - 25445 - 1. ^ hochreiter, sepp ; heusel, martin ; obermayer, klaus ( 2007 ). \" fast model - based protein homology detection without alignment \". bioinformatics. 23 ( 14 ) : 1728 \u2013 1736. doi : 10. 1093 / bioinformatics / btm247. pmid 17488755. ^ thireou, trias ; reczko, martin ( july 2007 ). \" bidirectional long short - term memory networks for predicting the subcellular localization of eukaryotic proteins \". ieee / acm transactions on computational biology and bioinformatics. 4 ( 3 ) : 441 \u2013 446. bibcode : 2007itcbb... 4.. 441t. doi : 10. 1109 / tcbb. 2007. 1015. pmid 17666763. s2cid 11787259. ^ tax, niek ; verenich, ilya ; la rosa, marcello ; dumas,",
    "chunk_id": "doc_fixed_004_chunk_76",
    "type": "inferential"
  },
  {
    "id": "q_005",
    "question": "What is the most likely future use of the term \" an attention free transformer \"?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Transformer_(machine_learning)",
    "ground_truth_context": "transformers for longer sequences with sparse attention methods \". google ai blog. 25 march 2021. archived from the original on 2021 - 09 - 18. retrieved 2021 - 05 - 28. ^ zhai, shuangfei ; talbott, walter ; srivastava, nitish ; huang, chen ; goh, hanlin ; zhang, ruixiang ; susskind, josh ( 2021 - 09 - 21 ). \" an attention free transformer \". arxiv : 2105. 14103 [ cs. lg ]. ^ peng, hao ; pappas, nikolaos ; yogatama, dani ; schwartz, roy ; smith, noah a. ; kong, lingpeng ( 2021 - 03 - 19 ). \" random feature attention \". arxiv : 2103. 02143 [ cs. cl ]. ^ choromanski, krzysztof ; likhosherstov, valerii ; dohan, david ; song, xingyou ; gane, andreea ; sarlos, tamas ; hawkins, peter ; davis, jared ; belanger, david ; colwell, lucy ; weller, adrian ( 2020 - 09 - 30 ). \" masked language modeling for proteins via linearly scalable long - context transformers \". arxiv : 2006. 03555 [ cs. lg ]. ^ lu, kevin ; grover, aditya ; abbeel, pieter ;",
    "chunk_id": "doc_fixed_000_chunk_98",
    "type": "inferential"
  },
  {
    "id": "q_006",
    "question": "What is the narrator's name?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
    "ground_truth_context": ", fabio lorenzo ; di ventra, massimiliano ( 2017 ). \" the complex dynamics of memristive circuits : analytical results and universal slow relaxation \". physical review e. 95 ( 2 ) 022140. arxiv : 1608. 08651. bibcode : 2017phrve.. 95b2140c. doi : 10. 1103 / physreve. 95. 022140. pmid 28297937. s2cid 6758362. ^ harvey, inman ; husbands, phil ; cliff, dave ( 1994 ), \" seeing the light : artificial evolution, real vision \", 3rd international conference on simulation of adaptive behavior : from animals to animats 3, pp. 392 \u2013 401 ^ quinn, matt ( 2001 ). \" evolving communication without dedicated communication channels \". advances in artificial life : 6th european conference, ecal 2001. pp. 357 \u2013 366. doi : 10. 1007 / 3 - 540 - 44811 - x _ 38. isbn 978 - 3 - 540 - 42567 - 0. ^ beer, randall d. ( 1997 ). \" the dynamics of adaptive behavior : a research program \". robotics and autonomous systems. 20 ( 2 \u2013 4 ) : 257 \u2013 289. doi : 10. 1016 / s0921 - 8890 ( 96 ) 00063 - 2. ^ sherstinsky, alex ( 2018 - 12 -",
    "chunk_id": "doc_fixed_004_chunk_67",
    "type": "factual"
  },
  {
    "id": "q_007",
    "question": "What is the last book mentioned ?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "ground_truth_context": "; mackworth, alan ( 2023 ). artificial intelligence, foundations of computational agents ( 3rd ed. ). cambridge university press. doi : 10. 1017 / 9781009258227. isbn 978 - 1 - 0092 - 5819 - 7. ^ russell, stuart ; norvig, peter ( 2020 ). artificial intelligence : a modern approach ( 4th ed. ). pearson. isbn 978 - 0 - 1346 - 1099 - 3. ^ \" why agents are the next frontier of generative ai \". mckinsey digital. 24 july 2024. archived from the original on 3 october 2024. retrieved 10 august 2024. ^ \" introducing copilot search in bing \". blogs. bing. com. 4 april 2025. ^ peters, jay ( 14 march 2023 ). \" the bing ai bot has been secretly running gpt - 4 \". the verge. retrieved 31 august 2025. ^ \" security for microsoft 365 copilot \". learn. microsoft. com. ^ o ' flaherty, kate ( 21 may 2025 ). \" google ai overviews \u2014 everything you need to know \". forbes. ^ \" generative ai in search : let google do the searching for you \". google. 14 may 2024. ^ figueiredo, mayara costa ; ankrah, elizabeth ; powell, jacquelyn e. ; epstein, daniel a. ; chen, yunan",
    "chunk_id": "doc_fixed_003_chunk_109",
    "type": "inferential"
  },
  {
    "id": "q_008",
    "question": "What is the word index?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "ground_truth_context": "able to recognize the wide variety of input as being identical to each other in terms of its textual equivalent. speech segmentation given a sound clip of a person or people speaking, separate it into words. a subtask of speech recognition and typically grouped with it. text - to - speech given a text, transform those units and produce a spoken representation. text - to - speech can be used to aid the visually impaired. word segmentation ( tokenization ) tokenization is a text - processing technique that divides text into individual words or word fragments. this technique results in two key components : a word index and tokenized text. the word index is a list that maps unique words to specific numerical identifiers, and the tokenized text replaces each word with its corresponding numerical token. these numerical tokens are then used in various deep learning methods. for a language like english, this is fairly trivial, since words are usually separated by spaces. however, some written languages like chinese, japanese and thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language. sometimes this process is also used in cases like bag of words ( bow ) creation in data mining. morphological analysis lemmatization of basque words lemmatization the task of removing inflectional endings only and to return the base dictionary form of a word which is also known as a lemma. lemmatization",
    "chunk_id": "doc_fixed_002_chunk_09",
    "type": "factual"
  },
  {
    "id": "q_009",
    "question": "What is the ieee 's nt a b ?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Transformer_(machine_learning)",
    "ground_truth_context": "; azulay, osher ; sintov, avishai ( february 2023 ). \" learning to throw with a handful of samples using decision transformers \". ieee robotics and automation letters. 8 ( 2 ) : 576 \u2013 583. bibcode : 2023iral.... 8.. 576m. doi : 10. 1109 / lra. 2022. 3229266. issn 2377 - 3766. ^ a b ruoss, anian ; deletang, gregoire ; medapati, sourabh ; grau - moya, jordi ; wenliang, li ; catt, elliot ; reid, john ; genewein, tim ( 2024 - 02 - 07 ). \" grandmaster - level chess without search \". arxiv : 2402. 04494v1 [ cs. lg ]. ^ a b wolf, thomas ; debut, lysandre ; sanh, victor ; chaumond, julien ; delangue, clement ; moi, anthony ; cistac, pierric ; rault, tim ; louf, remi ; funtowicz, morgan ; davison, joe ; shleifer, sam ; von platen, patrick ; ma, clara ; jernite, yacine ; plu, julien ; xu, canwen ; le scao, teven ; gugger, sylvain ; drame, maria",
    "chunk_id": "doc_fixed_000_chunk_74",
    "type": "inferential"
  },
  {
    "id": "q_010",
    "question": "What is the term for the process of learning a mathematical concept?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Large_language_model",
    "ground_truth_context": "the resulting models were reverse - engineered, and it turned out they used discrete fourier transform. the training of the model also highlighted a phenomenon called grokking, in which the model initially memorizes the training set ( overfitting ), and later suddenly learns to actually perform the calculation. understanding and intelligence see also : philosophy of artificial intelligence and artificial consciousness nlp researchers were evenly split when asked, in a 2022 survey, whether ( untuned ) llms \" could ( ever ) understand natural language in some nontrivial sense \". proponents of \" llm understanding \" believe that some llm abilities, such as mathematical reasoning, imply an ability to \" understand \" certain concepts. a microsoft team argued in 2023 that gpt - 4 \" can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more \" and that gpt - 4 \" could reasonably be viewed as an early ( yet still incomplete ) version of an artificial general intelligence system \" : \" can one reasonably say that a system that passes exams for software engineering candidates is not really intelligent? \" ilya sutskever argues that predicting the next word sometimes involves reasoning and deep insights, for example if the llm has to predict the name of the criminal in an unknown detective novel after processing the entire story leading up to the revelation. some researchers characterize llms as \" alien intelligence \". for example, conjecture ceo connor leahy considers untuned llms to be",
    "chunk_id": "doc_fixed_001_chunk_31",
    "type": "factual"
  },
  {
    "id": "q_011",
    "question": "What was the name of the sf rival 's tech model?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Large_language_model",
    "ground_truth_context": "googlers cracked an sf rival ' s tech model with a single word \". sfgate. archived from the original on 16 december 2023. ^ \" prepare for truly useful large language models \". nature biomedical engineering. 7 ( 2 ) : 85 \u2013 86. 7 march 2023. doi : 10. 1038 / s41551 - 023 - 01012 - 6. pmid 36882584. s2cid 257403466. ^ brinkmann, levin ; baumann, fabian ; bonnefon, jean - francois ; derex, maxime ; muller, thomas f. ; nussberger, anne - marie ; czaplicka, agnieszka ; acerbi, alberto ; griffiths, thomas l. ; henrich, joseph ; leibo, joel z. ; mcelreath, richard ; oudeyer, pierre - yves ; stray, jonathan ; rahwan, iyad ( 2023 - 11 - 20 ). \" machine culture \". nature human behaviour. 7 ( 11 ) : 1855 \u2013 1868. arxiv : 2311. 11388. doi : 10. 1038 / s41562 - 023 - 01742 - 2. issn 2397 - 3374. pmid 37985914. ^ niederhoffer, kate ; kellerman, gabriella rosen ; lee, angela ; liebscher, alex ; rapuano, kristina",
    "chunk_id": "doc_fixed_001_chunk_112",
    "type": "factual"
  },
  {
    "id": "q_012",
    "question": "What is the purpose of the text?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "ground_truth_context": "the area of computational linguistics maintained strong ties with cognitive studies. as an example, george lakoff offers a methodology to build natural language processing ( nlp ) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, with two defining aspects : apply the theory of conceptual metaphor, explained by lakoff as \" the understanding of one idea, in terms of another \" which provides an idea of the intent of the author. for example, consider the english word big. when used in a comparison ( \" that is a big tree \" ), the author ' s intent is to imply that the tree is physically large relative to other trees or the authors experience. when used metaphorically ( \" tomorrow is a big day \" ), the author ' s intent to imply importance. the intent behind other usages, like in \" she is a big person \", will remain somewhat ambiguous to a person and a cognitive nlp algorithm alike without additional information. assign relative measures of meaning to a word, phrase, sentence or piece of text based on the information presented before and after the piece of text being analyzed, e. g., by means of a probabilistic context - free grammar ( pcfg ). the mathematical equation for such algorithms is presented in us patent 9269353 : r m m ( t o k e n n ) = p m m ( t o k e n n ) \u00d7 1 2 d ( [UNK] i = \u2212 d d ( ( p",
    "chunk_id": "doc_fixed_002_chunk_22",
    "type": "inferential"
  },
  {
    "id": "q_013",
    "question": "What is the rnn 's rnn code?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
    "ground_truth_context": "^ zhang, aston ; lipton, zachary ; li, mu ; smola, alexander j. ( 2024 ). \" 10. modern recurrent neural networks \". dive into deep learning. cambridge new york port melbourne new delhi singapore : cambridge university press. isbn 978 - 1 - 009 - 38943 - 3. ^ rumelhart, david e. ; hinton, geoffrey e. ; williams, ronald j. ( october 1986 ). \" learning representations by back - propagating errors \". nature. 323 ( 6088 ) : 533 \u2013 536. bibcode : 1986natur. 323.. 533r. doi : 10. 1038 / 323533a0. issn 1476 - 4687. ^ a b schmidhuber, jurgen ( 1993 ). habilitation thesis : system modeling and optimization ( pdf ). page 150 ff demonstrates credit assignment across the equivalent of 1, 200 layers in an unfolded rnn. ^ sepp hochreiter ; jurgen schmidhuber ( 21 august 1995 ), long short term memory, wikidata q98967430 ^ a b hochreiter, sepp ; schmidhuber, jurgen ( 1997 - 11 - 01 ). \" long short - term memory \". neural computation. 9 ( 8 ) : 1735 \u2013 1780. doi : 10. 1162 / neco. 1997. 9. 8. 1735. pmid",
    "chunk_id": "doc_fixed_004_chunk_45",
    "type": "factual"
  },
  {
    "id": "q_014",
    "question": "What is the term for the ai that is used to describe the ai that is used to describe the ai that is used to describe the ai that is used to describe the ai that is used to describe",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "ground_truth_context": "explicit symbolic knowledge. although his arguments had been ridiculed and ignored when they were first presented, eventually, ai research came to agree with him. the issue is not resolved : sub - symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. critics such as noam chomsky argue continuing research into symbolic ai will still be necessary to attain general intelligence, in part because sub - symbolic ai is a move away from explainable ai : it can be difficult or impossible to understand why a modern statistical ai program made a particular decision. the emerging field of neuro - symbolic artificial intelligence attempts to bridge the two approaches. neat vs. scruffy main article : neats and scruffies \" neats \" hope that intelligent behavior is described using simple, elegant principles ( such as logic, optimization, or neural networks ). \" scruffies \" expect that it necessarily requires solving a large number of unrelated problems. neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. this issue was actively discussed in the 1970s and 1980s, but eventually was seen as irrelevant. modern ai has elements of both. soft vs. hard computing main article : soft computing finding a provably correct or optimal solution is intractable for many important problems. soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision,",
    "chunk_id": "doc_fixed_003_chunk_65",
    "type": "factual"
  },
  {
    "id": "q_015",
    "question": "What is the most common profession in the United States?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "ground_truth_context": "from paralegals to fast food cooks, while job demand is likely to increase for care - related professions ranging from personal healthcare to the clergy. in july 2025, ford ceo jim farley predicted that \" artificial intelligence is going to replace literally half of all white - collar workers in the u. s. \" from the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by joseph weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value - based judgement. existential risk main article : existential risk from artificial intelligence recent public debates in artificial intelligence have increasingly focused on its broader societal and ethical implications. it has been argued ai will become so powerful that humanity may irreversibly lose control of it. this could, as physicist stephen hawking stated, \" spell the end of the human race \". this scenario has been common in science fiction, when a computer or robot suddenly develops a human - like \" self - awareness \" ( or \" sentience \" or \" consciousness \" ) and becomes a malevolent character. these sci - fi scenarios are misleading in several ways. first, ai does not require human - like sentience to be an existential risk. modern ai programs are given specific goals and use learning and intelligence to achieve them. philosopher nick bostrom argued that if one gives almost any goal",
    "chunk_id": "doc_fixed_003_chunk_45",
    "type": "factual"
  },
  {
    "id": "q_016",
    "question": "What is the difference between the word embedding vectors and the word embedding matrix?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Transformer_(machine_learning)",
    "ground_truth_context": "information : word embedding each integer token identifier is converted into an embedding vector via a lookup table. equivalently stated, it multiplies a one - hot representation of the token identifier by an embedding matrix m { \\ displaystyle m }. for example, if the input token ' s identifier is 3 { \\ displaystyle 3 }, then the one - hot representation is [ 0, 0, 0, 1, 0, 0, \u2026 ] { \\ displaystyle [ 0, 0, 0, 1, 0, 0, \\ dots ] }, and its embedding vector is e m b e d ( 3 ) = [ 0, 0, 0, 1, 0, 0, \u2026 ] m { \\ displaystyle \\ mathrm { embed } ( 3 ) = [ 0, 0, 0, 1, 0, 0, \\ dots ] m } the token embedding vectors are added to their respective positional encoding vectors ( see below ), producing the sequence of input vectors. the dimension of an embedding vector is called hidden size or embedding size and written as d emb { \\ displaystyle d _ { \\ text { emb } } }. this size is written as d model { \\ displaystyle d _ { \\ text { model } } } in the original transformer paper. un - embedding an un - embedding layer is almost the reverse of",
    "chunk_id": "doc_fixed_000_chunk_15",
    "type": "inferential"
  },
  {
    "id": "q_017",
    "question": "What is the main idea of the passage?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Large_language_model",
    "ground_truth_context": "training data for llms. this produces large volumes of traffic which has led to denial of service issues with many websites. the situation has been described as \" a ddos on the entire internet \" and in some cases scrapers make up the majority of traffic to a site. ai web crawlers may bypass the methods that are usually used to block web scrapers, such as robots. txt files, blocking user - agents and filtering suspicious traffic. website operators have resorted to novel methods such as ai tarpits, but some fear that tarpits will only worsen the burden on servers. mental health clinical and mental health contexts present emerging applications alongside significant safety concerns. research and social media posts suggest that some individuals are using llms to seek therapy or mental health support. in early 2025, a survey by sentio university found that nearly half ( 48. 7 % ) of 499 u. s. adults with ongoing mental health conditions who had used llms reported turning to them for therapy or emotional support, including help with anxiety, depression, loneliness, and similar concerns. llms can produce hallucinations \u2014 plausible but incorrect statements \u2014 which may mislead users in sensitive mental health contexts. research also shows that llms may express stigma or inappropriate agreement with maladaptive thoughts, reflecting limitations in replicating the judgment and relational skills of human therapists. evaluations of crisis scenarios indicate that some llms lack effective safety protocols, such as assessing suicide risk or making appropriate",
    "chunk_id": "doc_fixed_001_chunk_51",
    "type": "inferential"
  },
  {
    "id": "q_018",
    "question": "What is the normal distribution of a fractogram?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Transformer_(machine_learning)",
    "ground_truth_context": "d, x \u27e9, sin \u27e8 w d, x \u27e9 ] t { \\ displaystyle \\ varphi ( x ) = { \\ frac { 1 } { \\ sqrt { d } } } [ \\ cos \\ langle w _ { 1 }, x \\ rangle, \\ sin \\ langle w _ { 1 }, x \\ rangle, \\ cdots \\ cos \\ langle w _ { d }, x \\ rangle, \\ sin \\ langle w _ { d }, x \\ rangle ] ^ { t } } where w 1,..., w d { \\ displaystyle w _ { 1 },..., w _ { d } } are independent samples from the normal distribution n ( 0, \u03c3 2 i ) { \\ displaystyle n ( 0, \\ sigma ^ { 2 } i ) }. this choice of parameters satisfy e [ \u27e8 \u03c6 ( x ), \u03c6 ( y ) \u27e9 ] = e \u2212 \u2016 x \u2212 y \u2016 2 2 \u03c3 2 { \\ displaystyle \\ mathbb { e } [ \\ langle \\ varphi ( x ), \\ varphi ( y ) \\ rangle ] = e ^ { - { \\ frac { \\ | x - y \\ | ^ { 2 } } { 2 \\ sigma ^ { 2 } } } } }, or e \u27e8 x, y \u27e9 / \u03c3 2 = e [ \u27e8 e \u2016 x \u2016 2 / 2 \u03c3 2 \u03c6",
    "chunk_id": "doc_fixed_000_chunk_64",
    "type": "factual"
  },
  {
    "id": "q_019",
    "question": "What is the name of the book that was published in the year 2024?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "ground_truth_context": "commitments from companies \". reuters. 21 may 2024. retrieved 23 may 2024. ^ \" frontier ai safety commitments, ai seoul summit 2024 \". gov. uk. 21 may 2024. archived from the original on 23 may 2024. retrieved 23 may 2024. ^ a b buntz, brian ( 3 november 2024 ). \" quality vs. quantity : us and china chart different paths in global ai patent race in 2024 / geographical breakdown of ai patents in 2024 \". research & development world. r & d world. archived from the original on 9 december 2024. ^ a b russell & norvig 2021, p. 9. ^ a b c copeland, j., ed. ( 2004 ). the essential turing : the ideas that gave birth to the computer age. oxford, england : clarendon press. isbn 0 - 1982 - 5079 - 7. ^ \" google books ngram \". archived from the original on 5 october 2024. retrieved 5 october 2024. ^ ai ' s immediate precursors : mccorduck ( 2004, pp. 51 \u2013 107 ), crevier ( 1993, pp. 27 \u2013 32 ), russell & norvig ( 2021, pp. 8 \u2013 17 ), moravec ( 1988, p. 3 ) ^ a b turing ' s original publication of the turing test in \" computing machinery and intelligence \" : turing ( 1950 ) historical influence and philosophical implications : haugeland",
    "chunk_id": "doc_fixed_003_chunk_134",
    "type": "factual"
  },
  {
    "id": "q_020",
    "question": "What is the most likely author to write a book about the tool integration problem ?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Large_language_model",
    "ground_truth_context": "; kuttler, heinrich ; lewis, mike ; yih, wen - tau ; rocktaschel, tim ; riedel, sebastian ; kiela, douwe ( 2020 ). \" retrieval - augmented generation for knowledge - intensive nlp tasks \". advances in neural information processing systems. 33. curran associates, inc. : 9459 \u2013 9474. arxiv : 2005. 11401. archived from the original on 2023 - 06 - 12. retrieved 2023 - 06 - 12. ^ dickson, ben ( 2025 - 04 - 02 ). \" the tool integration problem that ' s holding back enterprise ai ( and how cotools solves it ) \". venturebeat. retrieved 2025 - 05 - 26. ^ liang, yaobo ; wu, chenfei ; song, ting ; wu, wenshan ; xia, yan ; liu, yu ; ou, yang ; lu, shuai ; ji, lei ; mao, shaoguang ; wang, yun ; shou, linjun ; gong, ming ; duan, nan ( 2024 ). \" taskmatrix. ai : completing tasks by connecting foundation models with millions of apis \". science. 3 0063. doi : 10. 34133 / icomputing. 0063. ^ patil, shishir g. ; zhang, tianjun ; wang, xin ; gonzalez, joseph e. ( 2023 - 05 - 01",
    "chunk_id": "doc_fixed_001_chunk_73",
    "type": "inferential"
  },
  {
    "id": "q_021",
    "question": "What is the pseudocode for a standard encoder - decoder transformer?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Transformer_(machine_learning)",
    "ground_truth_context": "be easier to train, requiring no warm - up, leading to faster convergence. pseudocode the following is the pseudocode for a standard pre - ln encoder \u2013 decoder transformer, adapted from formal algorithms for transformers input : encoder input t _ e decoder input t _ d output : array of probability distributions, with shape ( decoder vocabulary size x length ( decoder output sequence ) ) / * encoder * / z _ e \u2190 encoder. tokenizer ( t _ e ) for each t in 1 : length ( z _ e ) do z _ e [ t ] \u2190 encoder. embedding ( z _ e [ t ] ) + encoder. positional _ embedding ( t ) for each l in 1 : length ( encoder. layers ) do layer \u2190 encoder. layers [ l ] / * first sublayer * / z _ e _ copy \u2190 copy ( z _ e ) for each t in 1 : length ( z _ e ) do z _ e [ t ] \u2190 layer. layer _ norm ( z _ e [ t ] ) z _ e \u2190 layer. multihead _ attention ( z _ e, z _ e, z _ e ) for each t in 1 : length ( z _ e ) do z _ e [ t ] \u2190 z _ e [ t ] + z _ e _ copy [ t ] / * second sublayer * / z _ e",
    "chunk_id": "doc_fixed_000_chunk_42",
    "type": "factual"
  },
  {
    "id": "q_022",
    "question": "What is the main idea of the passage?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Large_language_model",
    "ground_truth_context": "predict the name of the criminal in an unknown detective novel after processing the entire story leading up to the revelation. some researchers characterize llms as \" alien intelligence \". for example, conjecture ceo connor leahy considers untuned llms to be like inscrutable alien \" shoggoths \", and believes that rlhf tuning creates a \" smiling facade \" obscuring the inner workings of the llm : \" if you don ' t push it too far, the smiley face stays on. but then you give it [ an unexpected ] prompt, and suddenly you see this massive underbelly of insanity, of weird thought processes and clearly non - human understanding. \" in contrast, some skeptics of llm understanding believe that existing llms are \" simply remixing and recombining existing writing \", a phenomenon known as stochastic parrot, or they point to the deficits existing llms continue to have in prediction skills, reasoning skills, agency, and explainability. for example, gpt - 4 has natural deficits in planning and in real - time learning. generative llms have been observed to confidently assert claims of fact which do not seem to be justified by their training data, a phenomenon which has been termed \" hallucination \". specifically, hallucinations in the context of llms correspond to the generation of text or responses that seem syntactically sound, fluent, and natural but are factually incorrect, nonsen",
    "chunk_id": "doc_fixed_001_chunk_32",
    "type": "inferential"
  },
  {
    "id": "q_023",
    "question": "What is the dtic code for the book \" perceptual generalization over transformation groups \"?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
    "ground_truth_context": ": 10. 1353 / pbm. 2000. 0001. issn 1529 - 8795. pmid 10804585. ^ renshaw, birdsey ( 1946 - 05 - 01 ). \" central effects of centripetal impulses in axons of spinal ventral roots \". journal of neurophysiology. 9 ( 3 ) : 191 \u2013 204. doi : 10. 1152 / jn. 1946. 9. 3. 191. issn 0022 - 3077. pmid 21028162. ^ a b grossberg, stephen ( 2013 - 02 - 22 ). \" recurrent neural networks \". scholarpedia. 8 ( 2 ) : 1888. bibcode : 2013schpj... 8. 1888g. doi : 10. 4249 / scholarpedia. 1888. issn 1941 - 6016. ^ a b c rosenblatt, frank ( 1961 - 03 - 15 ). dtic ad0256582 : principles of neurodynamics. perceptrons and the theory of brain mechanisms. defense technical information center. ^ f. rosenblatt, \" perceptual generalization over transformation groups \", pp. 63 - - 100 in self - organizing systems : proceedings of an inter - disciplinary conference, 5 and 6 may 1959. edited by marshall c. yovitz and scott cameron. london, new york, [ etc. ], pergamon press, 1960",
    "chunk_id": "doc_fixed_004_chunk_40",
    "type": "factual"
  },
  {
    "id": "q_024",
    "question": "What is the main idea of the text?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "ground_truth_context": "time ) skepticism by most participants. until then, neural learning was basically rejected because of its lack of statistical interpretability. until 2015, deep learning had evolved into the major framework of nlp. [ link is broken, try http : / / web. stanford. edu / class / cs224n / ] ^ segev, elad ( 2022 ). semantic network analysis in social sciences. london : routledge. isbn 978 - 0 - 367 - 63652 - 4. archived from the original on 5 december 2021. retrieved 5 december 2021. ^ yi, chucai ; tian, yingli ( 2012 ), \" assistive text reading from complex background for blind persons \", camera - based document analysis and recognition, lecture notes in computer science, vol. 7139, springer berlin heidelberg, pp. 15 \u2013 28, citeseerx 10. 1. 1. 668. 869, doi : 10. 1007 / 978 - 3 - 642 - 29364 - 1 _ 2, isbn 978 - 3 - 642 - 29363 - 4 { { citation } } : cs1 maint : work parameter with isbn ( link ) ^ a b \" natural language processing ( nlp ) - a complete guide \". www. deeplearning. ai. 2023 - 01 - 11. retrieved 2024 - 05 - 05. ^ \" geeksforgeeks. ( n. d. ). tokenization in natural language",
    "chunk_id": "doc_fixed_002_chunk_31",
    "type": "inferential"
  },
  {
    "id": "q_025",
    "question": "What is the best title of the text ?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Large_language_model",
    "ground_truth_context": "##opomorphism foundation models list of large language models list of chatbots language model benchmark reinforcement learning small language model references ^ a b c bommasani, rishi ; hudson, drew a. ; adeli, ehsan ; altman, russ ; arora, simran ; von arx, matthew ; bernstein, michael s. ; bohg, jeannette ; bosselut, antoine ; brunskill, emma ( 2021 ). \" on the opportunities and risks of foundation models \". arxiv : 2108. 07258 [ cs. lg ]. ^ a b brown, tom b. ; mann, benjamin ; ryder, nick ; subbiah, melanie ; kaplan, jared ; dhariwal, prafulla ; neelakantan, arvind ; shyam, pranav ; sastry, girish ; askell, amanda ( 2020 ). \" language models are few - shot learners \". arxiv : 2005. 14165 [ cs. cl ]. ^ a b c brown, tom b. ; mann, benjamin ; ryder, nick ; subbiah, melanie ; kaplan, jared ; dhariwal, prafulla ; neelakantan, arvind ; shyam, pranav ; sastry, girish ; askell, amanda ; agarwal, sandhini ; herbert - voss, ariel ; krueger, gretchen ; henighan, tom ; child",
    "chunk_id": "doc_fixed_001_chunk_53",
    "type": "inferential"
  },
  {
    "id": "q_026",
    "question": "What was the first development of chatterbots?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "ground_truth_context": "of research were continued, e. g., the development of chatterbots with racter and jabberwacky. an important development ( that eventually led to the statistical turn in the 1990s ) was the rising importance of quantitative evaluation in this period. statistical nlp ( 1990s \u2013 present ) up until the 1980s, most natural language processing systems were based on complex sets of hand - written rules. starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. this shift was influenced by increasing computational power ( see moore ' s law ) and a decline in the dominance of chomskyan linguistic theories... ( e. g. transformational grammar ), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine - learning approach to language processing. 1990s : many of the notable early successes in statistical methods in nlp occurred in the field of machine translation, due especially to work at ibm research, such as ibm alignment models. these systems were able to take advantage of existing multilingual textual corpora that had been produced by the parliament of canada and the european union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. however, many systems relied on corpora that were specifically developed for the tasks they were designed to perform. this reliance has been a major limitation to their broader effectiveness and continues to affect similar systems. consequently,",
    "chunk_id": "doc_fixed_002_chunk_03",
    "type": "factual"
  },
  {
    "id": "q_027",
    "question": "What is the most recent publication of the authors of \" ai for large - scale evacuation modeling : promises and challenges \" ?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "ground_truth_context": "; lovreglio, ruggiero ; kuligowski, erica ( 2024 ). \" ai for large - scale evacuation modeling : promises and challenges \". interpretable machine learning for the analysis, design, assessment, and informed decision making for civil infrastructure. pp. 185 \u2013 204. doi : 10. 1016 / b978 - 0 - 12 - 824073 - 1. 00014 - 9. isbn 978 - 0 - 12 - 824073 - 1. ^ gomaa, islam ; adelzadeh, masoud ; gwynne, steven ; spencer, bruce ; ko, yoon ; benichou, noureddine ; ma, chunyun ; elsagan, nour ; duong, dana ; zalok, ehab ; kinateder, max ( 1 november 2021 ). \" a framework for intelligent fire detection and evacuation system \". fire technology. 57 ( 6 ) : 3179 \u2013 3185. doi : 10. 1007 / s10694 - 021 - 01157 - 3. ^ zhao, xilei ; lovreglio, ruggiero ; nilsson, daniel ( 1 may 2020 ). \" modelling and interpreting pre - evacuation decision - making using machine learning \". automation in construction. 113 103140. doi : 10. 1016 / j. autcon. 2020. 103140. hdl : 10179 / 17315. ^ \" india ' s",
    "chunk_id": "doc_fixed_003_chunk_113",
    "type": "inferential"
  },
  {
    "id": "q_028",
    "question": "What is the main idea of the passage?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "ground_truth_context": "study. in the late 2010s and early 2020s, agi companies began to deliver programs that created enormous interest. in 2015, alphago, developed by deepmind, beat the world champion go player. the program taught only the game ' s rules and developed a strategy by itself. gpt - 3 is a large language model that was released in 2020 by openai and is capable of generating high - quality human - like text. chatgpt, launched on 30 november 2022, became the fastest - growing consumer software application in history, gaining over 100 million users in two months. it marked what is widely regarded as ai ' s breakout year, bringing it into the public consciousness. these programs, and others, inspired an aggressive ai boom, where large companies began investing billions of dollars in ai research. according to ai impacts, about us $ 50 billion annually was invested in \" ai \" around 2022 in the u. s. alone and about 20 % of the new u. s. computer science phd graduates have specialized in \" ai \". about 800, 000 \" ai \" - related u. s. job openings existed in 2022. according to pitchbook research, 22 % of newly funded startups in 2024 claimed to be ai companies. philosophy main article : philosophy of artificial intelligence philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. another major focus has been whether machines can be conscious, and the associated ethical implications. many other topics in",
    "chunk_id": "doc_fixed_003_chunk_59",
    "type": "inferential"
  },
  {
    "id": "q_029",
    "question": "What is the name of the magazine that was published in 2024 ?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "ground_truth_context": "##g. 2. 2. 30247. 50087. ^ iraqi, amjad ( 3 april 2024 ). \" ' lavender ' : the ai machine directing israel ' s bombing spree in gaza \". + 972 magazine. archived from the original on 10 october 2024. retrieved 6 april 2024. ^ davies, harry ; mckernan, bethan ; sabbagh, dan ( 1 december 2023 ). \" ' the gospel ' : how israel uses ai to select bombing targets in gaza \". the guardian. archived from the original on 6 december 2023. retrieved 4 december 2023. ^ marti, j werner ( 10 august 2024 ). \" drohnen haben den krieg in der ukraine revolutioniert, doch sie sind empfindlich auf storsender \u2013 deshalb sollen sie jetzt autonom operieren \". neue zurcher zeitung ( in german ). archived from the original on 10 august 2024. retrieved 10 august 2024. ^ banh, leonardo ; strobel, gero ( 2023 ). \" generative artificial intelligence \". electronic markets. 33 ( 1 ) 63. doi : 10. 1007 / s12525 - 023 - 00680 - 1. ^ pasick, adam ( 27 march 2023 ). \" artificial intelligence glossary : neural networks and other terms explained \". the new york times",
    "chunk_id": "doc_fixed_003_chunk_105",
    "type": "factual"
  },
  {
    "id": "q_030",
    "question": "What is the difference between the two categories of fairness?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "ground_truth_context": "in areas where there is hope that the future will be better than the past. it is descriptive rather than prescriptive. bias and unfairness may go undetected because the developers are overwhelmingly white and male : among ai engineers, about 4 % are black and 20 % are women. there are various conflicting definitions and mathematical models of fairness. these notions depend on ethical assumptions, and are influenced by beliefs about society. one broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. representational fairness tries to ensure that ai systems do not reinforce negative stereotypes or render certain groups invisible. procedural fairness focuses on the decision process rather than the outcome. the most relevant notions of fairness may depend on the context, notably the type of ai application and the stakeholders. the subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. having access to sensitive attributes such as race or gender is also considered by many ai ethicists to be necessary in order to compensate for biases, but it may conflict with anti - discrimination laws. at the 2022 acm conference on fairness, accountability, and transparency a paper reported that a clip \u2011 based ( contrastive language - image pre - training ) robotic system reproduced harmful gender \u2011 and race \u2011 linked stereotypes in a simulated manipulation task. the authors recommended robot \u2011 learning methods which physically manifest such harms be \" paused, reworked, or even wound down",
    "chunk_id": "doc_fixed_003_chunk_39",
    "type": "factual"
  },
  {
    "id": "q_031",
    "question": "What is the smc code for the nakano kaoru book?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
    "ground_truth_context": "- 100 in self - organizing systems : proceedings of an inter - disciplinary conference, 5 and 6 may 1959. edited by marshall c. yovitz and scott cameron. london, new york, [ etc. ], pergamon press, 1960. ix, 322 p. ^ nakano, kaoru ( 1971 ). \" learning process in a model of associative memory \". pattern recognition and machine learning. pp. 172 \u2013 186. doi : 10. 1007 / 978 - 1 - 4615 - 7566 - 5 _ 15. isbn 978 - 1 - 4615 - 7568 - 9. ^ nakano, kaoru ( 1972 ). \" associatron - a model of associative memory \". ieee transactions on systems, man, and cybernetics. smc - 2 ( 3 ) : 380 \u2013 388. bibcode : 1972itsmc... 2.. 380n. doi : 10. 1109 / tsmc. 1972. 4309133. ^ amari, shun - ichi ( 1972 ). \" learning patterns and pattern sequences by self - organizing nets of threshold elements \". ieee transactions. c ( 21 ) : 1197 \u2013 1206. ^ little, w. a. ( 1974 ). \" the existence of persistent states in the brain \". mathematical biosciences. 19 ( 1 \u2013 2 ) : 101 \u2013 120. doi : 10. 1016",
    "chunk_id": "doc_fixed_004_chunk_41",
    "type": "factual"
  },
  {
    "id": "q_032",
    "question": "What is the most recent publication of the nlp approaches to computational argumentation ?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "ground_truth_context": ". unice. fr. archived from the original on 2021 - 04 - 18. retrieved 2021 - 03 - 09. ^ \" nlp approaches to computational argumentation \u2013 acl 2016, berlin \". retrieved 2021 - 03 - 09. ^ administration. \" centre for language technology ( clt ) \". macquarie university. retrieved 2021 - 01 - 11. ^ \" shared task : grammatical error correction \". www. comp. nus. edu. sg. retrieved 2021 - 01 - 11. ^ \" shared task : grammatical error correction \". www. comp. nus. edu. sg. retrieved 2021 - 01 - 11. ^ duan, yucong ; cruz, christophe ( 2011 ). \" formalizing semantic of natural language through conceptualization from existence \". international journal of innovation, management and technology. 2 ( 1 ) : 37 \u2013 42. archived from the original on 2011 - 10 - 09. ^ \" u b u w e b : : racter \". www. ubu. com. retrieved 2020 - 08 - 17. ^ writer, beta ( 2019 ). lithium - ion batteries. doi : 10. 1007 / 978 - 3 - 030 - 16800 - 1. isbn 978 - 3 - 030 - 16799 - 8. s2cid 155818532. ^ \" document understanding ai on google cloud ( cloud next ' 19 ) \u2013 youtube \". www. youtube. com. 11 april 2019",
    "chunk_id": "doc_fixed_002_chunk_34",
    "type": "inferential"
  },
  {
    "id": "q_033",
    "question": "What is the basic construction of a model?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Large_language_model",
    "ground_truth_context": "##set. this basic construction can be applied with more sophistication to improve the model. the image encoder may be frozen to improve stability. this type of method, where embeddings from multiple modalities are fused and the predictor is trained on the combined embeddings, is called early fusion. another method, called intermediate fusion, involves each modality being first processed independently to obtain modality - specific representations ; then these intermediate representations are fused together. in general, cross - attention is used for integrating information from different modalities. as an example, the flamingo model uses cross - attention layers to inject visual information into its pre - trained language model. non - natural languages llms can handle programming languages similarly to how they handle natural languages. no special change in token handling is needed as code, like human language, is represented as plain text. llms can generate code based on problems or instructions written in natural language. they can also describe code in natural language or translate it into other programming languages. they were originally used as a code completion tool, but advances have moved them towards automatic programming. services such as github copilot offer llms specifically trained, fine - tuned, or prompted for programming. in computational biology, transformer - base architectures, such as dna llms, have also proven useful in analyzing biological sequences : protein, dna, and rna. with proteins they appear able to capture a degree of \" grammar \"",
    "chunk_id": "doc_fixed_001_chunk_24",
    "type": "factual"
  },
  {
    "id": "q_034",
    "question": "What is the main idea of the passage?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "ground_truth_context": "be researching battlefield robots. ai tools make it easier for authoritarian governments to efficiently control their citizens in several ways. face and voice recognition allow widespread surveillance. machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. recommendation systems can precisely target propaganda and misinformation for maximum effect. deepfakes and generative ai aid in producing misinformation. advanced ai can make authoritarian centralized decision - making more competitive than liberal and decentralized systems such as markets. it lowers the cost and difficulty of digital warfare and advanced spyware. all these technologies have been available since 2020 or earlier \u2014 ai facial recognition systems are already being used for mass surveillance in china. there are many other ways in which ai is expected to help bad actors, some of which can not be foreseen. for example, machine - learning ai is able to design tens of thousands of toxic molecules in a matter of hours. technological unemployment main articles : workplace impact of artificial intelligence and technological unemployment economists have frequently highlighted the risks of redundancies from ai, and speculated about unemployment if there is no adequate social policy for full employment. in the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \" we ' re in uncharted territory \" with ai. a survey of economists showed disagreement about whether the increasing use of robots and ai will cause a substantial increase in long - term unemployment, but they generally agree that it could be a net benefit if",
    "chunk_id": "doc_fixed_003_chunk_43",
    "type": "inferential"
  },
  {
    "id": "q_035",
    "question": "What is the most recent publication of the authors ?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Large_language_model",
    "ground_truth_context": ", accountability, and transparency. pp. 599 \u2013 627. arxiv : 2504. 18412. doi : 10. 1145 / 3715275. 3732039. isbn 979 - 8 - 4007 - 1482 - 5. ^ grabb, declan ; lamparth, max ; vasan, nina ( 2024 - 08 - 14 ). \" risks from language models for automated mental healthcare : ethics and structure for implementation \". arxiv : 2406. 11852 [ cs. cy ]. ^ mcbain, ryan k. ; cantor, jonathan h. ; zhang, li ang ; baker, olesya ; zhang, fang ; halbisen, alyssa ; kofner, aaron ; breslau, joshua ; stein, bradley ; mehrotra, ateev ; yu, hao ( 2025 - 03 - 05 ). \" competency of large language models in evaluating appropriate responses to suicidal ideation : comparative study \". journal of medical internet research. 27 ( 1 ) e67891. doi : 10. 2196 / 67891. pmc 11928068. pmid 40053817. ^ li, fei - fei ; etchemendy, john ( 2024 - 05 - 22 ). \" no, today ' s ai isn ' t sentient. here ' s how we know \". time. retrieved 2024 - 05 - 22. ^",
    "chunk_id": "doc_fixed_001_chunk_116",
    "type": "inferential"
  },
  {
    "id": "q_036",
    "question": "What is the difference between a f and a t?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Transformer_(machine_learning)",
    "ground_truth_context": "} } f ( t ) = ( e i t / r k ) k = 0, 1, \u2026, d 2 \u2212 1 { \\ displaystyle f ( t ) = \\ left ( e ^ { it / r ^ { k } } \\ right ) _ { k = 0, 1, \\ ldots, { \\ frac { d } { 2 } } - 1 } } where r = n 2 / d { \\ displaystyle r = n ^ { 2 / d } }. the main reason for using this positional encoding function is that using it, shifts are linear transformations : f ( t + \u03b4 t ) = d i a g ( f ( \u03b4 t ) ) f ( t ) { \\ displaystyle f ( t + \\ delta t ) = \\ mathrm { diag } ( f ( \\ delta t ) ) f ( t ) } where \u03b4 t \u2208 r { \\ displaystyle \\ delta t \\ in \\ mathbb { r } } is the distance one wishes to shift. this allows the transformer to take any encoded position, and find the encoding of the position n - steps - ahead or n - steps - behind, by a matrix multiplication. by taking a linear sum, any convolution can also be implemented as linear transformations : [UNK] j c j f ( t + \u03b4 t j ) = ( [UNK] j c j d i a g ( f ( \u03b4 t j ) ) ) f ( t ) { \\ displaystyle",
    "chunk_id": "doc_fixed_000_chunk_19",
    "type": "factual"
  },
  {
    "id": "q_037",
    "question": "What was the first computer-readable data format?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "ground_truth_context": "of only twenty words, because that was all that would fit in a computer memory at the time. 1970s : during the 1970s, many programmers began to write \" conceptual ontologies \", which structured real - world information into computer - understandable data. examples are margie ( schank, 1975 ), sam ( cullingford, 1978 ), pam ( wilensky, 1978 ), talespin ( meehan, 1976 ), qualm ( lehnert, 1977 ), politics ( carbonell, 1979 ), and plot units ( lehnert 1981 ). during this time, the first chatterbots were written ( e. g., parry ). 1980s : the 1980s and early 1990s mark the heyday of symbolic methods in nlp. focus areas of the time included research on rule - based parsing ( e. g., the development of hpsg as a computational operationalization of generative grammar ), morphology ( e. g., two - level morphology ), semantics ( e. g., lesk algorithm ), reference ( e. g., within centering theory ) and other areas of natural language understanding ( e. g., in the rhetorical structure theory ). other lines of research were continued, e. g., the development of chatterbots with racter and jabberwacky. an important development ( that eventually led to the statistical turn in the 1990s ) was the rising importance of quantitative evaluation in this period.",
    "chunk_id": "doc_fixed_002_chunk_02",
    "type": "factual"
  },
  {
    "id": "q_038",
    "question": "What is the ieee code for the book ?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
    "ground_truth_context": ". cv ]. ^ campolucci, paolo ; uncini, aurelio ; piazza, francesco ; rao, bhaskar d. ( 1999 ). \" on - line learning algorithms for locally recurrent neural networks \". ieee transactions on neural networks. 10 ( 2 ) : 253 \u2013 271. bibcode : 1999itnn... 10.. 253c. citeseerx 10. 1. 1. 33. 7550. doi : 10. 1109 / 72. 750549. pmid 18252525. ^ wan, eric a. ; beaufays, francoise ( 1996 ). \" diagrammatic derivation of gradient algorithms for neural networks \". neural computation. 8 : 182 \u2013 201. doi : 10. 1162 / neco. 1996. 8. 1. 182. s2cid 15512077. ^ a b campolucci, paolo ; uncini, aurelio ; piazza, francesco ( 2000 ). \" a signal - flow - graph approach to on - line gradient calculation \". neural computation. 12 ( 8 ) : 1901 \u2013 1927. citeseerx 10. 1. 1. 212. 5406. doi : 10. 1162 / 089976600300015196. pmid 10953244. s2cid 15090951. ^ graves, alex ; fernandez, santiago ; gomez, faustino j. ( 2006 ). \" connectionist temporal classification : label",
    "chunk_id": "doc_fixed_004_chunk_61",
    "type": "inferential"
  },
  {
    "id": "q_039",
    "question": "What is the most recent work by a researcher?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
    "ground_truth_context": "- zheng ; lee, yee - chun ( 1992 ). \" learning and extracting finite state automata with second - order recurrent neural networks \" ( pdf ). neural computation. 4 ( 3 ) : 393 \u2013 405. doi : 10. 1162 / neco. 1992. 4. 3. 393. s2cid 19666035. ^ omlin, christian w. ; giles, c. lee ( 1996 ). \" constructing deterministic finite - state automata in recurrent neural networks \". journal of the acm. 45 ( 6 ) : 937 \u2013 972. citeseerx 10. 1. 1. 32. 2364. doi : 10. 1145 / 235809. 235811. s2cid 228941. ^ paine, rainer w. ; tani, jun ( 2005 - 09 - 01 ). \" how hierarchical control self - organizes in artificial adaptive systems \". adaptive behavior. 13 ( 3 ) : 211 \u2013 225. doi : 10. 1177 / 105971230501300303. s2cid 9932565. ^ a b \" burns, benureau, tani ( 2018 ) a bergson - inspired adaptive time constant for the multiple timescales recurrent neural network model. jnns \". ^ barkan, oren ; benchimol, jonathan ; caspi, itamar ; cohen, eliya ;",
    "chunk_id": "doc_fixed_004_chunk_64",
    "type": "inferential"
  },
  {
    "id": "q_040",
    "question": "What is the value matrix w v  displaystyle w  v",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Transformer_(machine_learning)",
    "ground_truth_context": "^ { k } }, and the value weights w v { \\ displaystyle w ^ { v } }. the module takes three sequences, a query sequence, a key sequence, and a value sequence. the query sequence is a sequence of length \u2113 seq, query { \\ displaystyle \\ ell _ { \\ text { seq, query } } }, and each entry is a vector of dimension d emb, query { \\ displaystyle d _ { \\ text { emb, query } } }. similarly for the key and value sequences. for each vector x i, query { \\ displaystyle x _ { i, { \\ text { query } } } } in the query sequence, it is multiplied by a matrix w q { \\ displaystyle w ^ { q } } to produce a query vector q i = x i, query w q { \\ displaystyle q _ { i } = x _ { i, { \\ text { query } } } w ^ { q } }. the matrix of all query vectors is the query matrix : q = x query w q { \\ displaystyle q = x _ { \\ text { query } } w ^ { q } } similarly, we construct the key matrix k = x key w k { \\ displaystyle k = x _ { \\ text { key } } w ^ { k } } and the value matrix v = x value w v { \\ displaystyle v = x _ { \\ text {",
    "chunk_id": "doc_fixed_000_chunk_24",
    "type": "factual"
  },
  {
    "id": "q_041",
    "question": "What is the smallest gpt - 2 model?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Transformer_(machine_learning)",
    "ground_truth_context": ", in the smallest gpt - 2 model, there are only self - attention mechanisms. it has the following dimensions : d emb = 768, n head = 12, d head = 64 { \\ displaystyle d _ { \\ text { emb } } = 768, n _ { \\ text { head } } = 12, d _ { \\ text { head } } = 64 } since 12 \u00d7 64 = 768 { \\ displaystyle 12 \\ times 64 = 768 }, its output projection matrix w o \u2208 r ( 12 \u00d7 64 ) \u00d7 768 { \\ displaystyle w ^ { o } \\ in \\ mathbb { r } ^ { ( 12 \\ times 64 ) \\ times 768 } } is a square matrix. masked attention the transformer architecture is constructed to calculate output tokens iteratively. assuming t = 0 { \\ displaystyle t = 0 } refers to the calculation of the first output token i = 0 { \\ displaystyle i = 0 }, for step t > 0 { \\ displaystyle t > 0 }, the output token i = 0 { \\ displaystyle i = 0 } shall remain constant. this ensures properties of the model similar to autoregressive models. therefore, at every time step t { \\ displaystyle t }, the calculation for all outputs i { \\ displaystyle i } should not have access to tokens at position j { \\ displaystyle j } for j > = i { \\",
    "chunk_id": "doc_fixed_000_chunk_32",
    "type": "factual"
  },
  {
    "id": "q_042",
    "question": "What is the date of the conference?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "ground_truth_context": "acm conference on fairness, accountability, and transparency ( facct ' 22 ). seoul, south korea : association for computing machinery. doi : 10. 1145 / 3531146. 3533138. ^ for accessible summaries, see the georgia tech release and sciencedaily coverage of the study ' s findings. \" flawed ai makes robots racist, sexist \". georgia tech research news. 23 june 2022. ^ \" robots turn racist and sexist with flawed ai, study finds \". sciencedaily. 21 june 2022. ^ sample ( 2017 ). ^ \" black box ai \". 16 june 2023. archived from the original on 15 june 2024. retrieved 5 october 2024. ^ christian ( 2020 ), p. 110. ^ christian ( 2020 ), pp. 88 \u2013 91. ^ christian ( 2020, p. 83 ) ; russell & norvig ( 2021, p. 997 ) ^ christian ( 2020 ), p. 91. ^ christian ( 2020 ), p. 83. ^ verma ( 2021 ). ^ rothman ( 2020 ). ^ christian ( 2020 ), pp. 105 \u2013 108. ^ christian ( 2020 ), pp. 108 \u2013 112. ^ ropek, lucas ( 21 may 2024 ). \" new anthropic research sheds light on ai ' s ' black box ' \". gizmodo. archived from the original on 5 october 2024. retrieved 23",
    "chunk_id": "doc_fixed_003_chunk_124",
    "type": "inferential"
  },
  {
    "id": "q_043",
    "question": "What is the difference between a bidirectional rnn and a bidirectional rnn?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
    "ground_truth_context": "2, t + 1 ) { \\ displaystyle f _ { \\ theta _ { 2 } } : ( x _ { 1, t }, h _ { 2, t } ) \\ mapsto ( x _ { 2, t }, h _ { 2, t + 1 } ) }.... layer n { \\ displaystyle n } has hidden vector h n, t { \\ displaystyle h _ { n, t } }, parameters \u03b8 n { \\ displaystyle \\ theta _ { n } }, and maps f \u03b8 n : ( x n \u2212 1, t, h n, t ) \u21a6 ( x n, t, h n, t + 1 ) { \\ displaystyle f _ { \\ theta _ { n } } : ( x _ { n - 1, t }, h _ { n, t } ) \\ mapsto ( x _ { n, t }, h _ { n, t + 1 } ) }. each layer operates as a stand - alone rnn, and each layer ' s output sequence is used as the input sequence to the layer above. there is no conceptual limit to the depth of stacked rnn. bidirectional main article : bidirectional recurrent neural networks bidirectional rnn a bidirectional rnn ( birnn ) is composed of two rnns, one processing the input sequence in one direction, and another in the opposite direction. abstractly,",
    "chunk_id": "doc_fixed_004_chunk_08",
    "type": "factual"
  },
  {
    "id": "q_044",
    "question": "What is the name of the first gpt-based chatbot?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Transformer_(machine_learning)",
    "ground_truth_context": "natural language generation. in 2022, a chatbot based on gpt - 3, chatgpt, became unexpectedly popular, triggering a boom around large language models. since 2020, transformers have been applied in modalities beyond text, including the vision transformer, speech recognition, robotics, and multimodal. the vision transformer, in turn, stimulated new developments in convolutional neural networks. image and video generators like dall - e ( 2021 ), stable diffusion 3 ( 2024 ), and sora ( 2024 ), use transformers to analyse input data ( like text prompts ) by breaking it down into \" tokens \" and then calculating the relevance between each token using self - attention, which helps the model understand the context and relationships within the data. training methods for stabilizing training the plain transformer architecture had difficulty in converging. in the original paper, the authors recommended using learning rate warmup. that is, the learning rate should linearly scale up from 0 to maximal value for the first part of the training ( usually recommended to be 2 % of the total number of training steps ), before decaying again. a 2020 paper found that using layer normalization before ( instead of after ) multihead attention and feedforward layers stabilizes training, not requiring learning rate warmup. this is the \" pre - ln transformer \" and is more commonly used, compared to the original \" post - ln transformer",
    "chunk_id": "doc_fixed_000_chunk_07",
    "type": "factual"
  },
  {
    "id": "q_045",
    "question": "What is the name of the book that amodei wrote about?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Large_language_model",
    "ground_truth_context": "##3 - 12 - 29. ^ amodei, dario ; olah, chris ; steinhardt, jacob ; christiano, paul ; schulman, john ; mane, dan ( 2016 - 06 - 21 ). \" concrete problems in ai safety \". arxiv : 1606. 06565 [ cs. ai ]. ^ lyons, jessica ( 2025 - 09 - 26 ). \" prompt injection \u2013 and a $ 5 domain \u2013 trick salesforce agentforce into leaking sales \". the register. retrieved 2025 - 09 - 26. ^ carlini, nicholas ; tramer, florian ; wallace, eric ( 2021 - 08 - 11 ). \" extracting training data from large language models \" ( pdf ). usenix association. retrieved 2025 - 10 - 02. ^ zhao, yao ; zhang, yun ; sun, yong ( 2023 - 06 - 07 ). \" the debate over understanding in ai ' s large language models \". proceedings of the national academy of sciences. 120 ( 13 ) e2215907120. arxiv : 2306. 05499. bibcode : 2023pnas.. 12015907m. doi : 10. 1073 / pnas. 2215907120. pmc 10068812. pmid 36943882. ^ buolamwini, joy ; gebru, timnit ( 2018 - 01 - 01 )",
    "chunk_id": "doc_fixed_001_chunk_104",
    "type": "factual"
  },
  {
    "id": "q_046",
    "question": "What is the most likely reason that the display style is misleading?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
    "ground_truth_context": "} of type ( x t, h t ) \u21a6 ( y t, h t + 1 ) { \\ displaystyle ( x _ { t }, h _ { t } ) \\ mapsto ( y _ { t }, h _ { t + 1 } ) }, where x t { \\ displaystyle x _ { t } } : input vector ; h t { \\ displaystyle h _ { t } } : hidden vector ; y t { \\ displaystyle y _ { t } } : output vector ; \u03b8 { \\ displaystyle \\ theta } : neural network parameters. in words, it is a neural network that maps an input x t { \\ displaystyle x _ { t } } into an output y t { \\ displaystyle y _ { t } }, with the hidden vector h t { \\ displaystyle h _ { t } } playing the role of \" memory \", a partial record of all previous input - output pairs. at each step, it transforms input to an output, and modifies its \" memory \" to help it to better perform future processing. the illustration to the right may be misleading to many because practical neural network topologies are frequently organized in \" layers \" and the drawing gives that appearance. however, what appears to be layers are, in fact, different steps in time, \" unfolded \" to produce the appearance of layers. stacked rnn stacked rnn a stacked rnn, or deep rnn, is composed of multiple rnns",
    "chunk_id": "doc_fixed_004_chunk_06",
    "type": "inferential"
  },
  {
    "id": "q_047",
    "question": "What was the first book to be published by the u. s. and british governments?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "ground_truth_context": ", however, underestimated the difficulty of the problem. in 1974, both the u. s. and british governments cut off exploratory research in response to the criticism of sir james lighthill and ongoing pressure from the u. s. congress to fund more productive projects. minsky and papert ' s book perceptrons was understood as proving that artificial neural networks would never be useful for solving real - world tasks, thus discrediting the approach altogether. the \" ai winter \", a period when obtaining funding for ai projects was difficult, followed. in the early 1980s, ai research was revived by the commercial success of expert systems, a form of ai program that simulated the knowledge and analytical skills of human experts. by 1985, the market for ai had reached over a billion dollars. at the same time, japan ' s fifth generation computer project inspired the u. s. and british governments to restore funding for academic research. however, beginning with the collapse of the lisp machine market in 1987, ai once again fell into disrepute, and a second, longer - lasting winter began. up to this point, most of ai ' s funding had gone to projects that used high - level symbols to represent mental objects like plans, goals, beliefs, and known facts. in the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition, and began to look",
    "chunk_id": "doc_fixed_003_chunk_56",
    "type": "factual"
  },
  {
    "id": "q_048",
    "question": "What is the narrator's name?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
    "ground_truth_context": "346238. pmid 6953413. ^ hopfield, j. j. ( 1984 ). \" neurons with graded response have collective computational properties like those of two - state neurons \". proceedings of the national academy of sciences. 81 ( 10 ) : 3088 \u2013 3092. bibcode : 1984pnas... 81. 3088h. doi : 10. 1073 / pnas. 81. 10. 3088. pmc 345226. pmid 6587342. ^ engel, a. ; broeck, c. van den ( 2001 ). statistical mechanics of learning. cambridge, uk ; new york, ny : cambridge university press. isbn 978 - 0 - 521 - 77307 - 2. ^ seung, h. s. ; sompolinsky, h. ; tishby, n. ( 1992 - 04 - 01 ). \" statistical mechanics of learning from examples \". physical review a. 45 ( 8 ) : 6056 \u2013 6091. bibcode : 1992phrva.. 45. 6056s. doi : 10. 1103 / physreva. 45. 6056. pmid 9907706. ^ zhang, aston ; lipton, zachary ; li, mu ; smola, alexander j. ( 2024 ). \" 10. modern recurrent neural networks \". dive into deep learning. cambridge new york port melbourne new delhi singapore : cambridge",
    "chunk_id": "doc_fixed_004_chunk_44",
    "type": "factual"
  },
  {
    "id": "q_049",
    "question": "What is the name of the book that 's based on the ' ai ' s hallucinations ' ?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Large_language_model",
    "ground_truth_context": "from the original on 26 march 2023. retrieved 15 january 2023. ^ varshney, neeraj ; yao, wenlin ; zhang, hongming ; chen, jianshu ; yu, dong ( 2023 ). \" a stitch in time saves nine : detecting and mitigating hallucinations of llms by validating low - confidence generation \". arxiv : 2307. 03987 [ cs. cl ]. ^ lin, belle ( 2025 - 02 - 05 ). \" why amazon is betting on ' automated reasoning ' to reduce ai ' s hallucinations : the tech giant says an obscure field that combines ai and math can mitigate \u2014 but not completely eliminate \u2014 ai ' s propensity to provide wrong answers \". wall street journal. issn 0099 - 9660. ^ lakoff, george ( 1999 ). philosophy in the flesh : the embodied mind and its challenge to western philosophy ; appendix : the neural theory of language paradigm. new york basic books. pp. 569 \u2013 583. isbn 978 - 0 - 465 - 05674 - 3. ^ evans, vyvyan. ( 2014 ). the language myth. cambridge university press. isbn 978 - 1 - 107 - 04396 - 1. ^ friston, karl j. ( 2022 ). active inference : the free energy principle in mind, brain, and behavior ; chapter 4 the generative models of active inference.",
    "chunk_id": "doc_fixed_001_chunk_93",
    "type": "factual"
  },
  {
    "id": "q_050",
    "question": "What is the most recent publication of the authors of the text ?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Large_language_model",
    "ground_truth_context": "/ pnas. 2215907120. pmc 10068812. pmid 36943882. ^ buolamwini, joy ; gebru, timnit ( 2018 - 01 - 01 ). \" gender shades : intersectional accuracy disparities in commercial gender classification \" ( pdf ). proceedings of machine learning research ( fat * ). retrieved 2025 - 10 - 02. ^ yang, kaiqi ( 2024 - 11 - 01 ). \" unpacking political bias in large language models : a cross - model comparison on u. s. politics \". arxiv : 2412. 16746 [ cs. cy ]. ^ strubell, emma ; ganesh, ananya ; mccallum, andrew ( 2019 - 07 - 28 ). \" energy and policy considerations for deep learning in nlp \" ( pdf ). acl anthology. retrieved 2025 - 10 - 02. ^ he, yuhao ; yang, li ; qian, chunlian ; li, tong ; su, zhengyuan ; zhang, qiang ; hou, xiangqing ( 2023 - 04 - 28 ). \" conversational agent interventions for mental health problems : systematic review and meta - analysis of randomized controlled trials \". journal of medical internet research. 25 e43862. doi : 10. 2196 / 43862. pmc 10182468. pmid 37115595.",
    "chunk_id": "doc_fixed_001_chunk_105",
    "type": "inferential"
  },
  {
    "id": "q_051",
    "question": "What is the main idea of the passage?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "ground_truth_context": "thousands of jobs in banking, financial planning, and pension advice in the process, but i ' m not sure it will unleash a new wave of [ e. g., sophisticated ] pension innovation. \" military main article : military applications of artificial intelligence various countries are deploying ai military applications. the main applications enhance command and control, communications, sensors, integration and interoperability. research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. ai technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed joint fires between networked combat vehicles, both human - operated and autonomous. ai has been used in military operations in iraq, syria, israel and ukraine. generative ai vincent van gogh in watercolour created by generative ai software these paragraphs are an excerpt from generative artificial intelligence. [ edit ] generative artificial intelligence, also known as generative ai or genai, is a subfield of artificial intelligence that uses generative models to generate text, images, videos, audio, software code or other forms of data. these models learn the underlying patterns and structures of their training data and use them to generate new data in response to input, which often takes the form of natural language prompts. the prevalence of generative ai tools has increased significantly since the ai boom in the 2020s. this boom was",
    "chunk_id": "doc_fixed_003_chunk_23",
    "type": "inferential"
  },
  {
    "id": "q_052",
    "question": "What is the degree of truth between 0 and 1 in fuzzy logic?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "ground_truth_context": "fuzzy logic assigns a \" degree of truth \" between 0 and 1. it can therefore handle propositions that are vague and partially true. non - monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning. other specialized versions of logic have been developed to describe many complex domains. probabilistic methods for uncertain reasoning a simple bayesian network, with the associated conditional probability tables many problems in ai ( including reasoning, planning, learning, perception, and robotics ) require the agent to operate with incomplete or uncertain information. ai researchers have devised a number of tools to solve these problems using methods from probability theory and economics. precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis, and information value theory. these tools include models such as markov decision processes, dynamic decision networks, game theory and mechanism design. bayesian networks are a tool that can be used for reasoning ( using the bayesian inference algorithm ), learning ( using the expectation \u2013 maximization algorithm ), planning ( using decision networks ) and perception ( using dynamic bayesian networks ). probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time ( e. g., hidden markov models or kalman filters ). expectation \u2013 maximization clustering of old faithful eruption data starts from a random guess but then successfully converge",
    "chunk_id": "doc_fixed_003_chunk_12",
    "type": "factual"
  },
  {
    "id": "q_053",
    "question": "What is the name of the person who is the author of the text ?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Transformer_(machine_learning)",
    "ground_truth_context": ", paul ; chung, hyung won ; sutton, charles ; gehrmann, sebastian ; schuh, parker ; shi, kensen ; tsvyashchenko, sasha ; maynez, joshua ; rao, abhishek ( 2022 - 04 - 01 ). \" palm : scaling language modeling with pathways \". arxiv : 2204. 02311 [ cs. cl ]. ^ ainslie, joshua ; lee - thorp, james ; de jong, michiel ; zemlyanskiy, yury ; lebron, federico ; sanghai, sumit ( 2023 - 12 - 23 ), gqa : training generalized multi - query transformer models from multi - head checkpoints, arxiv : 2305. 13245 ^ a b deepseek - ai ; liu, aixin ; feng, bei ; wang, bin ; wang, bingxuan ; liu, bo ; zhao, chenggang ; dengr, chengqi ; ruan, chong ( 19 june 2024 ), deepseek - v2 : a strong, economical, and efficient mixture - of - experts language model, arxiv : 2405. 04434. ^ a b leviathan, yaniv ; kalman, matan ; matias, yossi ( 2023 - 05 - 18 ), fast inference from transformers via speculative decoding, arxiv : 2211. 17192 ^ fu, yao ( 202",
    "chunk_id": "doc_fixed_000_chunk_94",
    "type": "inferential"
  },
  {
    "id": "q_054",
    "question": "What is the term for the supervised learning theory?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "ground_truth_context": "( 1950 ). ^ solomonoff ( 1956 ). ^ unsupervised learning : russell & norvig ( 2021, pp. 653 ) ( definition ), russell & norvig ( 2021, pp. 738 \u2013 740 ) ( cluster analysis ), russell & norvig ( 2021, pp. 846 \u2013 860 ) ( word embedding ) ^ a b supervised learning : russell & norvig ( 2021, \u00a7 19. 2 ) ( definition ), russell & norvig ( 2021, chpt. 19 \u2013 20 ) ( techniques ) ^ reinforcement learning : russell & norvig ( 2021, chpt. 22 ), luger & stubblefield ( 2004, pp. 442 \u2013 449 ) ^ transfer learning : russell & norvig ( 2021, pp. 281 ), the economist ( 2016 ) ^ \" artificial intelligence ( ai ) : what is ai and how does it work? | built in \". builtin. com. retrieved 30 october 2023. ^ computational learning theory : russell & norvig ( 2021, pp. 672 \u2013 674 ), jordan & mitchell ( 2015 ) ^ natural language processing ( nlp ) : russell & norvig ( 2021, chpt. 23 \u2013 24 ), poole, mackworth & goebel ( 1998, pp. 91 \u2013 104 ), luger & stubblefield ( 2004, pp. 591 \u2013 632 ) ^ subproblems",
    "chunk_id": "doc_fixed_003_chunk_86",
    "type": "factual"
  },
  {
    "id": "q_055",
    "question": "What was the result of the georgetown experiment?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "ground_truth_context": "the georgetown experiment in 1954 involved fully automatic translation of more than sixty russian sentences into english. the authors claimed that within three or five years, machine translation would be a solved problem. however, real progress was much slower, and after the alpac report in 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. little further research in machine translation was conducted in america ( though some research continued elsewhere, such as japan and europe ) until the late 1980s when the first statistical machine translation systems were developed. 1960s : some notably successful natural language processing systems developed in the 1960s were shrdlu, a natural language system working in restricted \" blocks worlds \" with restricted vocabularies, and eliza, a simulation of a rogerian psychotherapy, written by joseph weizenbaum between 1964 and 1966. despite using minimal information about human thought or emotion, eliza was able to produce interactions that appeared human - like. when the \" patient \" exceeded the very small knowledge base, eliza might provide a generic response, for example, responding to \" my head hurts \" with \" why do you say your head hurts? \". ross quillian ' s successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in a computer memory at the time. 1970s : during the 1970s, many programmers began to write \" conceptual ontologies \", which structured real - world information into computer - understandable data.",
    "chunk_id": "doc_fixed_002_chunk_01",
    "type": "inferential"
  },
  {
    "id": "q_056",
    "question": "What is the main idea of the passage?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Large_language_model",
    "ground_truth_context": "data. researchers target concrete failure modes, including memorization and copyright leakage, security exploits such as prompt injection, algorithmic bias manifesting as stereotyping, dataset selection effects, and political skew, methods for reducing high energy and carbon costs of large - scale training, and measurable cognitive and mental health impacts of conversational agents on users, while engaging empirical and ethical uncertainty about claims of machine sentience, and applying mitigation measures such as dataset curation, input sanitization, model auditing, scalable oversight, and governance frameworks. cbrn and content misuse ai labs treat cbrn defense ( chemical, biological, radiological, and nuclear defense ) and similar topics as high - consequence misuse attempt to apply various techniques to reduce potential harms. some commenters expressed concern over accidental or deliberate creation of misinformation, or other forms of misuse. for example, the availability of large language models could reduce the skill - level required to commit bioterrorism ; biosecurity researcher kevin esvelt has suggested that llm creators should exclude from their training data papers on creating or enhancing pathogens. content filtering llm applications accessible to the public, like chatgpt or claude, typically incorporate safety measures designed to filter out harmful content. however, implementing these controls effectively has proven challenging. for instance, a 2023 study proposed a method for circumventing llm safety systems. in 2025, the american sunlight",
    "chunk_id": "doc_fixed_001_chunk_44",
    "type": "inferential"
  },
  {
    "id": "q_057",
    "question": "What is the main idea of the text?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Transformer_(machine_learning)",
    "ground_truth_context": "both of which scales as o ( n 2 ) { \\ displaystyle o ( n ^ { 2 } ) } where n { \\ displaystyle n } is the number of tokens in a sequence. reformer ( 2020 ) reduces the computational load from o ( n 2 ) { \\ displaystyle o ( n ^ { 2 } ) } to o ( n ln n ) { \\ displaystyle o ( n \\ ln n ) } by using locality - sensitive hashing and reversible layers. sparse attention uses attention graphs that grows slower than o ( n 2 ) { \\ displaystyle o ( n ^ { 2 } ) }. for example, bigbird ( 2020 ) uses random small - world networks which grows as o ( n ) { \\ displaystyle o ( n ) }. ordinary transformers require a memory size that is quadratic in the size of the context window. attention - free transformers reduce this to a linear dependence while still retaining the advantages of a transformer by linking the key to the value. random feature attention random feature attention ( 2021 ) uses fourier random features : \u03c6 ( x ) = 1 d [ cos \u27e8 w 1, x \u27e9, sin \u27e8 w 1, x \u27e9, [UNK] cos \u27e8 w d, x \u27e9, sin \u27e8 w d, x \u27e9 ] t { \\ displaystyle \\ varphi ( x ) = { \\ frac { 1 } { \\ sqrt { d } } } [ \\ cos \\ langle w _",
    "chunk_id": "doc_fixed_000_chunk_63",
    "type": "inferential"
  },
  {
    "id": "q_058",
    "question": "What is the main idea of the passage?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "ground_truth_context": "the ai boom. generative ai ' s ability to create and modify content has led to several unintended consequences and harms. ethical concerns have been raised about ai ' s long - term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology. goals the general problem of simulating ( or creating ) intelligence has been broken into subproblems. these consist of particular traits or capabilities that researchers expect an intelligent system to display. the traits described below have received the most attention and cover the scope of ai research. reasoning and problem - solving early researchers developed algorithms that imitated step - by - step reasoning that humans use when they solve puzzles or make logical deductions. by the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics. many of these algorithms are insufficient for solving large reasoning problems because they experience a \" combinatorial explosion \" : they become exponentially slower as the problems grow. even humans rarely use the step - by - step deduction that early ai research could model. they solve most of their problems using fast, intuitive judgments. accurate and efficient reasoning is an unsolved problem. knowledge representation an ontology represents knowledge as a set of concepts within a domain and the relationships between those concepts. knowledge representation and knowledge engineering allow ai programs to answer questions intelligently and make deductions about real - world facts. formal knowledge representations are used in content -",
    "chunk_id": "doc_fixed_003_chunk_02",
    "type": "inferential"
  },
  {
    "id": "q_059",
    "question": "What is the name of the book that ##er & schmidhuber wrote?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "ground_truth_context": "##er & schmidhuber ( 2012 ). ^ russell & norvig ( 2021 ), p. 750. ^ a b c russell & norvig ( 2021 ), p. 17. ^ a b c d e f g russell & norvig ( 2021 ), p. 785. ^ a b schmidhuber ( 2022 ), sect. 5. ^ schmidhuber ( 2022 ), sect. 6. ^ a b c schmidhuber ( 2022 ), sect. 7. ^ schmidhuber ( 2022 ), sect. 8. ^ quoted in christian ( 2020, p. 22 ) ^ metz, cade ; weise, karen ( 5 may 2025 ). \" a. i. hallucinations are getting worse, even as new systems become more powerful \". the new york times. issn 0362 - 4331. retrieved 6 may 2025. ^ smith ( 2023 ). ^ \" explained : generative ai \". mit news | massachusetts institute of technology. 9 november 2023. ^ \" ai writing and content creation tools \". mit sloan teaching & learning technologies. archived from the original on 25 december 2023. retrieved 25 december 2023. ^ marmouyet ( 2023 ). ^ kobielus ( 2019 ). ^ thomason, james ( 21 may 2024 ). \" mojo rising : the resurgence of ai - first programming languages \". venturebeat.",
    "chunk_id": "doc_fixed_003_chunk_96",
    "type": "factual"
  },
  {
    "id": "q_060",
    "question": "What is the name of the book that focuses on the foundations of language processing?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Large_language_model",
    "ground_truth_context": "##elbach, sven ( 2022 ). \" pre - trained language models \". foundation models for natural language processing. artificial intelligence : foundations, theory, and algorithms. pp. 19 \u2013 78. doi : 10. 1007 / 978 - 3 - 031 - 23190 - 2 _ 2. isbn 978 - 3 - 031 - 23190 - 2. ^ dodge, jesse ; sap, maarten ; marasovic, ana ; agnew, william ; ilharco, gabriel ; groeneveld, dirk ; mitchell, margaret ; gardner, matt ( 2021 ). \" documenting large webtext corpora : a case study on the colossal clean crawled corpus \" ( pdf ). emnlp. arxiv : 2104. 08758. doi : 10. 1145 / 3571730. ^ lee, katherine ; ippolito, daphne ; nystrom, andrew ; zhang, chiyuan ; eck, douglas ; callison - burch, chris ; carlini, nicholas ( may 2022 ). \" deduplicating training data makes language models better \" ( pdf ). proceedings of the 60th annual meeting of the association for computational linguistics ( volume 1 : long papers ). pp. 8424 \u2013 8445. doi : 10. 18653 / v1 / 2022. acl - long. 577. ^ li, yuanzhi ; bubeck, sebastien ; eldan, ronen ; del giorno",
    "chunk_id": "doc_fixed_001_chunk_66",
    "type": "factual"
  },
  {
    "id": "q_061",
    "question": "What is the name of the book that mccarthy et al. wrote in 1955?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "ground_truth_context": "norvig : \" stong ai \u2013 the assertion that machines that do so are actually thinking ( as opposed to simulating thinking ). \" references ^ a b c russell & norvig ( 2021 ), pp. 1 \u2013 4. ^ ai set to exceed human brain power archived 19 february 2008 at the wayback machine cnn. com ( 26 july 2006 ) ^ kaplan, andreas ; haenlein, michael ( 2019 ). \" siri, siri, in my hand : who ' s the fairest in the land? on the interpretations, illustrations, and implications of artificial intelligence \". business horizons. 62 : 15 \u2013 25. doi : 10. 1016 / j. bushor. 2018. 08. 004. [ the question of the source is a pastiche of : snow white ] ^ russell & norvig ( 2021, \u00a7 1. 2 ). ^ \" tech companies want to build artificial general intelligence. but who decides when agi is attained? \". ap news. 4 april 2024. retrieved 20 may 2025. ^ a b dartmouth workshop : russell & norvig ( 2021, p. 18 ), mccorduck ( 2004, pp. 111 \u2013 136 ), nrc ( 1999, pp. 200 \u2013 201 ) the proposal : mccarthy et al. ( 1955 ) ^ a b successful programs of the 1960s : mccorduck ( 2004, pp. 243 \u2013 252 ), crevier ( 1993, pp. 52 \u2013",
    "chunk_id": "doc_fixed_003_chunk_79",
    "type": "factual"
  },
  {
    "id": "q_062",
    "question": "What is the main topic of philosophical debates about artificial intelligence?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "ground_truth_context": "ai companies. philosophy main article : philosophy of artificial intelligence philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. another major focus has been whether machines can be conscious, and the associated ethical implications. many other topics in philosophy are relevant to ai, such as epistemology and free will. for example, debates center on whether machines can genuinely understand meaning, whether they possess autonomous decision - making capabilities, and to what extent their actions can be considered intentional rather than merely the result of algorithmic processes. rapid advancements have intensified public discussions on the philosophy and ethics of ai. defining artificial intelligence see also : synthetic intelligence, intelligent agent, artificial mind, virtual intelligence, and dartmouth workshop alan turing wrote in 1950 \" i propose to consider the question ' can machines think '? \" he advised changing the question from whether a machine \" thinks \", to \" whether or not it is possible for machinery to show intelligent behaviour \". he devised the turing test, which measures the ability of a machine to simulate human conversation. since we can only observe the behavior of the machine, it does not matter if it is \" actually \" thinking or literally has a \" mind \". turing notes that we can not determine these things about other people but \" it is usual to have a polite convention that everyone thinks. \" the turing test can provide some evidence of intelligence, but it penalizes non - human intelligent behavior. russell and norvig agree with turing that intelligence must be defined in",
    "chunk_id": "doc_fixed_003_chunk_60",
    "type": "factual"
  },
  {
    "id": "q_063",
    "question": "What is the name of the book that was published in 2024?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Large_language_model",
    "ground_truth_context": "11 - 17. retrieved 2023 - 03 - 14. ^ fathallah, nadeen ; das, arunav ; de giorgis, stefano ; poltronieri, andrea ; haase, peter ; kovriguina, liubov ( 2024 - 05 - 26 ). neon - gpt : a large language model - powered pipeline for ontology learning ( pdf ). extended semantic web conference 2024. hersonissos, greece. ^ manning, christopher d. ( 2022 ). \" human language understanding & reasoning \". daedalus. 151 ( 2 ) : 127 \u2013 138. doi : 10. 1162 / daed _ a _ 01905. s2cid 248377870. archived from the original on 2023 - 11 - 17. retrieved 2023 - 03 - 09. ^ kaplan, jared ; mccandlish, sam ; henighan, tom ; brown, tom b. ; chess, benjamin ; child, rewon ; gray, scott ; radford, alec ; wu, jeffrey ; amodei, dario ( 2020 ). \" scaling laws for neural language models \". arxiv : 2001. 08361 [ cs. lg ]. ^ vaswani, ashish ; shazeer, noam ; parmar, niki ; uszkoreit, jakob ; jones, llion ; gomez, aidan n ; kaiser, \u0142ukasz ; polosukhin",
    "chunk_id": "doc_fixed_001_chunk_55",
    "type": "factual"
  },
  {
    "id": "q_064",
    "question": "What is the main idea of the passage?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "ground_truth_context": "for artificial intelligence and cryptocurrency. the report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole japanese nation. prodigious power consumption by ai is responsible for the growth of fossil fuel use, and might delay closings of obsolete, carbon - emitting coal energy facilities. there is a feverish rise in the construction of data centers throughout the us, making large technology firms ( e. g., microsoft, meta, google, amazon ) into voracious consumers of electric power. projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. a chatgpt search involves the use of 10 times the electrical energy as a google search. the large firms are in haste to find power sources \u2013 from nuclear energy to geothermal to fusion. the tech firms argue that \u2013 in the long view \u2013 ai will be eventually kinder to the environment, but they need the energy now. ai makes the power grid more efficient and \" intelligent \", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms. a 2024 goldman sachs research paper, ai data centers and the coming us power demand surge, found \" us power demand ( is ) likely to experience growth not seen in a generation.... \" and forecasts that, by 2030, us data centers will consume 8 % of us power, as opposed to",
    "chunk_id": "doc_fixed_003_chunk_31",
    "type": "inferential"
  },
  {
    "id": "q_065",
    "question": "What is the most common way to mask causal?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Transformer_(machine_learning)",
    "ground_truth_context": "} } \\ right ) v \\ end { aligned } } } the following matrix is commonly used in decoder self - attention modules, called \" causal masking \" : m causal = [ 0 \u2212 \u221e \u2212 \u221e \u2026 \u2212 \u221e 0 0 \u2212 \u221e \u2026 \u2212 \u221e 0 0 0 \u2026 \u2212 \u221e [UNK] [UNK] [UNK] [UNK] [UNK] 0 0 0 \u2026 0 ] { \\ displaystyle m _ { \\ text { causal } } = { \\ begin { bmatrix } 0 & - \\ infty & - \\ infty & \\ dots & - \\ infty \\ \\ 0 & 0 & - \\ infty & \\ dots & - \\ infty \\ \\ 0 & 0 & 0 & \\ dots & - \\ infty \\ \\ \\ vdots & \\ vdots & \\ vdots & \\ ddots & \\ vdots \\ \\ 0 & 0 & 0 & \\ dots & 0 \\ end { bmatrix } } } in words, it means that each token can pay attention to itself, and every token before it, but not any after it. a non - masked attention module can be thought of as a masked attention module where the mask has all entries zero. as an example of an uncommon use of mask matrix, the xlnet considers all masks of the form p m causal p \u2212 1 { \\ displaystyle pm _ { \\ text { causal } } p ^ { - 1 } }, where p { \\ displaystyle p }",
    "chunk_id": "doc_fixed_000_chunk_34",
    "type": "inferential"
  },
  {
    "id": "q_066",
    "question": "What is the most recent work by lorente?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
    "ground_truth_context": "' s neuroscience to the birth of cybernetics \". the neuroscientist. 31 ( 1 ) : 14 \u2013 30. doi : 10. 1177 / 10738584231179932. hdl : 10261 / 348372. issn 1073 - 8584. pmid 37403768. ^ ramon y cajal, santiago ( 1909 ). histologie du systeme nerveux de l ' homme & des vertebres. vol. ii. foyle special collections library king ' s college london. paris : a. maloine. p. 149. ^ de no, r. lorente ( 1933 - 08 - 01 ). \" vestibulo - ocular reflex arc \". archives of neurology and psychiatry. 30 ( 2 ) : 245. doi : 10. 1001 / archneurpsyc. 1933. 02240140009001. issn 0096 - 6754. ^ larriva - sahd, jorge a. ( 2014 - 12 - 03 ). \" some predictions of rafael lorente de no 80 years later \". frontiers in neuroanatomy. 8 : 147. doi : 10. 3389 / fnana. 2014. 00147. issn 1662 - 5129. pmc 4253658. pmid 25520630. ^ \" reverberating circuit \". oxford reference. retrieved 2024 - 07 -",
    "chunk_id": "doc_fixed_004_chunk_38",
    "type": "inferential"
  },
  {
    "id": "q_067",
    "question": "What is the name of the book that cites ai as evolution?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "ground_truth_context": "). \" robot rights violate human rights, experts warn eu \". euronews. archived from the original on 19 september 2024. retrieved 23 february 2024. ^ the intelligence explosion and technological singularity : russell & norvig ( 2021, pp. 1004 \u2013 1005 ), omohundro ( 2008 ), kurzweil ( 2005 ) i. j. good ' s \" intelligence explosion \" : good ( 1965 ) vernor vinge ' s \" singularity \" : vinge ( 1993 ) ^ russell & norvig ( 2021 ), p. 1005. ^ transhumanism : moravec ( 1988 ), kurzweil ( 2005 ), russell & norvig ( 2021, p. 1005 ) ^ ai as evolution : edward fredkin is quoted in mccorduck ( 2004, p. 401 ), butler ( 1863 ), dyson ( 1998 ) ^ ai in myth : mccorduck ( 2004, pp. 4 \u2013 5 ) ^ mccorduck ( 2004 ), pp. 340 \u2013 400. ^ buttazzo ( 2001 ). ^ anderson ( 2008 ). ^ mccauley ( 2007 ). ^ galvan ( 1997 ). textbooks luger, george ; stubblefield, william ( 2004 ). artificial intelligence : structures and strategies for complex problem solving ( 5th ed. ). benjamin / cummings. isbn 978 - 0 - 8053 - 4780 - 7. archived from the original",
    "chunk_id": "doc_fixed_003_chunk_144",
    "type": "factual"
  },
  {
    "id": "q_068",
    "question": "What is the problem of knowledge acquisition?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "ground_truth_context": "much of what people know is not represented as \" facts \" or \" statements \" that they could express verbally ). there is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for ai applications. planning and decision - making an \" agent \" is anything that perceives and takes actions in the world. a rational agent has goals or preferences and takes actions to make them happen. in automated planning, the agent has a specific goal. in automated decision - making, the agent has preferences \u2014 there are some situations it would prefer to be in, and some situations it is trying to avoid. the decision - making agent assigns a number to each situation ( called the \" utility \" ) that measures how much the agent prefers it. for each possible action, it can calculate the \" expected utility \" : the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. it can then choose the action with the maximum expected utility. in classical planning, the agent knows exactly what the effect of any action will be. in most real - world problems, however, the agent may not be certain about the situation they are in ( it is \" unknown \" or \" unobservable \" ) and it may not know for certain what will happen after each possible action ( it is not \" deterministic \" ). it must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked. in some problems",
    "chunk_id": "doc_fixed_003_chunk_04",
    "type": "inferential"
  },
  {
    "id": "q_069",
    "question": "What is the main idea of the passage?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "ground_truth_context": "such as llama 2, mistral or stable diffusion, have been made open - weight, meaning that their architecture and trained parameters ( the \" weights \" ) are publicly available. open - weight models can be freely fine - tuned, which allows companies to specialize them with their own data and for their own use - case. open - weight models are useful for research and innovation but can also be misused. since they can be fine - tuned, any built - in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. some researchers warn that future ai models may develop dangerous capabilities ( such as the potential to drastically facilitate bioterrorism ) and that once released on the internet, they cannot be deleted everywhere if needed. they recommend pre - release audits and cost - benefit analyses. frameworks artificial intelligence projects can be guided by ethical considerations during the design, development, and implementation of an ai system. an ai framework such as the care and act framework, developed by the alan turing institute and based on the sum values, outlines four main ethical dimensions, defined as follows : respect the dignity of individual people connect with other people sincerely, openly, and inclusively care for the wellbeing of everyone protect social values, justice, and the public interest other developments in ethical frameworks include those decided upon during the asilomar conference, the montreal declaration for responsible ai, and the ieee ' s ethics of autonomous systems initiative, among others ; however",
    "chunk_id": "doc_fixed_003_chunk_50",
    "type": "inferential"
  },
  {
    "id": "q_070",
    "question": "What is the most recent news article about the openai case?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Large_language_model",
    "ground_truth_context": ". reuters. 2025 - 06 - 25. retrieved 2025 - 06 - 26. ^ \" meta scores victory in ai copyright case \". wired. 2025 - 06 - 25. retrieved 2025 - 06 - 26. ^ \" openai defeats news outlets ' copyright lawsuit over ai training for now \". reuters. 2024 - 11 - 07. retrieved 2024 - 11 - 08. ^ \" openai erases evidence in training data lawsuit \". the verge. 2024 - 11 - 21. retrieved 2024 - 11 - 22. ^ peng, zhencan ; wang, zhizhi ; deng, dong ( 13 june 2023 ). \" near - duplicate sequence search at scale for large language model memorization evaluation \" ( pdf ). proceedings of the acm on management of data. 1 ( 2 ) : 1 \u2013 18. doi : 10. 1145 / 3589324. s2cid 259213212. archived ( pdf ) from the original on 2024 - 08 - 27. retrieved 2024 - 01 - 20. citing lee et al 2022. ^ peng, wang & deng 2023, p. 8. ^ stephen council ( 1 dec 2023 ). \" how googlers cracked an sf rival ' s tech model with a single word \". sfgate. archived from the original on 16 december 2023. ^ \" prepare for truly useful large language models \". nature biomedical engineering. 7 ( 2 ) : 85",
    "chunk_id": "doc_fixed_001_chunk_111",
    "type": "inferential"
  },
  {
    "id": "q_071",
    "question": "What is the minimum error term for a function?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
    "ground_truth_context": "y } } _ { 2 }, \\ dots, { \\ hat { y } } _ { l } ) }. the problem is that if the model makes a mistake early on, say at y ^ 2 { \\ displaystyle { \\ hat { y } } _ { 2 } }, then subsequent tokens are likely to also be mistakes. this makes it inefficient for the model to obtain a learning signal, since the model would mostly learn to shift y ^ 2 { \\ displaystyle { \\ hat { y } } _ { 2 } } towards y 2 { \\ displaystyle y _ { 2 } }, but not the others. teacher forcing makes it so that the decoder uses the correct output sequence for generating the next entry in the sequence. so for example, it would see ( y 1, \u2026, y k ) { \\ displaystyle ( y _ { 1 }, \\ dots, y _ { k } ) } in order to generate y ^ k + 1 { \\ displaystyle { \\ hat { y } } _ { k + 1 } }. gradient descent main articles : gradient descent and vanishing gradient problem gradient descent is a first - order iterative optimization algorithm for finding the minimum of a function. in neural networks, it can be used to minimize the error term by changing each weight in proportion to the derivative of the error with respect to that weight, provided the non - linear activation functions are differentiable. the standard method for training rn",
    "chunk_id": "doc_fixed_004_chunk_21",
    "type": "factual"
  },
  {
    "id": "q_072",
    "question": "What is the main idea of the text?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Large_language_model",
    "ground_truth_context": "agreeability observed across multi - turn interactions and productized assistants. continued sycophancy has led to the observation of getting \" 1 - shotted \", denoting instances where conversational interaction with a large language model produces a lasting change in a user ' s beliefs or decisions, similar to the negative effects of psychedelics, and controlled experiments show that short llm dialogues can generate measurable opinion and confidence shifts comparable to human interlocutors. empirical analyses attribute part of the effect to human preference signals and preference models that reward convincingly written agreeable responses, and subsequent work has extended evaluation to multi - turn benchmarks and proposed interventions such as synthetic - data finetuning, adversarial evaluation, targeted preference - model reweighting, and multi - turn sycophancy benchmarks to measure persistence and regression risk. industry responses have combined research interventions with product controls, for example google and other labs publishing synthetic - data and fine - tuning interventions and openai rolling back an overly agreeable gpt - 4o update while publicly describing changes to feedback collection, personalization controls, and evaluation procedures to reduce regression risk and improve long - term alignment with user - level safety objectives. mainstream culture has reflected anxieties about this dynamic where south park satirized overreliance on chatgpt and the tendency of assistants to flatter user beliefs in season 27 episode \" sickofancy \", and continued the themes across the following season, which commentators interpreted as a critique of",
    "chunk_id": "doc_fixed_001_chunk_46",
    "type": "inferential"
  },
  {
    "id": "q_073",
    "question": "What is the term for a system of cortical computing with memristive nanodevices?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
    "ground_truth_context": "hp labs describes a system of cortical computing with memristive nanodevices. the memristors ( memory resistors ) are implemented by thin film materials in which the resistance is electrically tuned via the transport of ions or oxygen vacancies within the film. darpa ' s synapse project has funded ibm research and hp labs, in collaboration with the boston university department of cognitive and neural systems ( cns ), to develop neuromorphic architectures that may be based on memristive systems. memristive networks are a particular type of physical neural network that have very similar properties to ( little - ) hopfield networks, as they have continuous dynamics, a limited memory capacity and natural relaxation via the minimization of a function which is asymptotic to the ising model. in this sense, the dynamics of a memristive circuit have the advantage compared to a resistor - capacitor network to have a more interesting non - linear behavior. from this point of view, engineering analog memristive networks account for a peculiar type of neuromorphic engineering in which the device behavior depends on the circuit wiring or topology. the evolution of these networks can be studied analytically using variations of the caravelli - traversa - di ventra equation. continuous - time a continuous - time recurrent neural network ( ctrnn ) uses a system of ordinary differential equations to model the effects on a neuron of the incoming inputs",
    "chunk_id": "doc_fixed_004_chunk_30",
    "type": "factual"
  },
  {
    "id": "q_074",
    "question": "What is the ambiguity of ai?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "ground_truth_context": "one of which is their ambiguity. several works use ai to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. this appears in karel capek ' s r. u. r., the films a. i. artificial intelligence and ex machina, as well as the novel do androids dream of electric sheep?, by philip k. dick. dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence. see also artificial consciousness \u2013 field in cognitive science artificial intelligence and elections \u2013 impact of ai on political elections artificial intelligence content detection \u2013 software to detect ai - generated content artificial intelligence in wikimedia projects \u2013 use of artificial intelligence to develop wikipedia and other wikimedia projects association for the advancement of artificial intelligence ( aaai ) behavior selection algorithm \u2013 algorithm that selects actions for intelligent agents business process automation \u2013 automation of business processes case - based reasoning \u2013 process of solving new problems based on the solutions of similar past problems computational intelligence \u2013 ability of a computer to learn a specific task from data or experimental observation darwin eu \u2013 a european union initiative coordinated by the european medicines agency ( ema ) to generate and utilize real world evidence ( rwe ) to support the evaluation and supervision of medicines across the eu digital immortality \u2013 hypothetical concept of storing a personality in digital form emergent algorithm \u2013 algorithm exhibiting emergent behavior female gendering of ai technologies \u2013 gender biases in",
    "chunk_id": "doc_fixed_003_chunk_72",
    "type": "factual"
  },
  {
    "id": "q_075",
    "question": "What is the name of the earliest surviving octopus?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Large_language_model",
    "ground_truth_context": ", johannes ; horvitz, eric ; kamar, ece ; lee, peter ; lee, yin tat ; li, yuanzhi ; lundberg, scott ; nori, harsha ; palangi, hamid ; ribeiro, marco tulio ; zhang, yi ( 2023 ). \" machine culture \". nature human behaviour. 7 ( 11 ) : 1855 \u2013 1868. arxiv : 2303. 12712. doi : 10. 1038 / s41562 - 023 - 01742 - 2. pmid 37985914. ^ \" anthropic ceo dario amodei pens a smart look at our ai future \". fast company. october 17, 2024. ^ \" chatgpt is more like an ' alien intelligence ' than a human brain, says futurist \". zdnet. 2023. archived from the original on 12 june 2023. retrieved 12 june 2023. ^ a b newport, cal ( 13 april 2023 ). \" what kind of mind does chatgpt have? \". the new yorker. archived from the original on 12 june 2023. retrieved 12 june 2023. ^ roose, kevin ( 30 may 2023 ). \" why an octopus - like creature has come to symbolize the state of a. i. \" the new york times. archived from the original on 30 may 2023. retrieved 12 june 2023. ^ \" the a to",
    "chunk_id": "doc_fixed_001_chunk_91",
    "type": "factual"
  },
  {
    "id": "q_076",
    "question": "What is the main idea of the passage?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "ground_truth_context": "math benchmark problems. alternatively, dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such as alphatensor, alphageometry, alphaproof and alphaevolve all from google deepmind, llemma from eleutherai or julius. when natural language is used to describe mathematical problems, converters can transform such prompts into a formal language such as lean to define mathematical tasks. the experimental model gemini deep think accepts natural language prompts directly and achieved gold medal results in the international math olympiad of 2025. some models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics. topological deep learning integrates various topological approaches. finance finance is one of the fastest growing sectors where applied ai tools are being deployed : from retail online banking to investment advice and insurance, where automated \" robot advisers \" have been in use for some years. according to nicolas firzli, director of the world pensions & investments forum, it may be too early to see the emergence of highly innovative ai - informed financial products and services. he argues that \" the deployment of ai tools will simply further automatise things : destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but i ' m not sure it will unleash a new wave of [ e. g., sophisticated ] pension innovation. \" military main article : military applications of",
    "chunk_id": "doc_fixed_003_chunk_22",
    "type": "inferential"
  },
  {
    "id": "q_077",
    "question": "What is the most recent book that has been published in the field of cognitive science?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "ground_truth_context": "- video, language translation, more \". venturebeat. 2022 - 11 - 02. retrieved 2022 - 11 - 09. ^ vincent, james ( 2022 - 09 - 29 ). \" meta ' s new text - to - video ai generator is like dall - e for video \". the verge. retrieved 2022 - 11 - 09. ^ \" previous shared tasks | conll \". www. conll. org. retrieved 2021 - 01 - 11. ^ \" cognition \". lexico. oxford university press and dictionary. com. archived from the original on july 15, 2020. retrieved 6 may 2020. ^ \" ask the cognitive scientist \". american federation of teachers. 8 august 2014. cognitive science is an interdisciplinary field of researchers from linguistics, psychology, neuroscience, philosophy, computer science, and anthropology that seek to understand the mind. ^ robinson, peter ( 2008 ). handbook of cognitive linguistics and second language acquisition. routledge. pp. 3 \u2013 8. isbn 978 - 0 - 805 - 85352 - 0. ^ lakoff, george ( 1999 ). philosophy in the flesh : the embodied mind and its challenge to western philosophy ; appendix : the neural theory of language paradigm. new york basic books. pp. 569 \u2013 583. isbn 978 - 0 - 465 - 05674 - 3. ^ strauss, claudia ( 1999 ). a cognitive theory of cultural meaning. cambridge university press. pp. 156 \u2013 164. isbn",
    "chunk_id": "doc_fixed_002_chunk_36",
    "type": "inferential"
  },
  {
    "id": "q_078",
    "question": "What is the main idea of the passage ?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Large_language_model",
    "ground_truth_context": "main article : prompt engineering in 2020, openai researchers demonstrated that their new model gpt - 3 could understand what format to use given a few rounds of q and a ( or other type of task ) in the input data as example, thanks in part due to the rlhf technique. this technique, called few - shot prompting, allows llms to be adapted to any task without requiring fine - tuning. also in 2022, it was found that the base gpt - 3 model can generate an instruction based on user input. the generated instruction along with user input is then used as input to another instance of the model under a \" instruction : [... ], input : [... ], output : \" format. the other instance is able to complete the output and often produces the correct answer in doing so. the ability to \" self - instruct \" makes llms able to bootstrap themselves toward a correct answer. dialogue processing ( chatbot ) an llm can be turned into a chatbot by specializing it for conversation. user input is prefixed with a marker such as \" q : \" or \" user : \" and the llm is asked to predict the output after a fixed \" a : \" or \" assistant : \". this type of model became commercially available in 2022 with chatgpt, a sibling model of instructgpt fine - tuned to accept and produce dialog - formatted text based on gpt - 3.",
    "chunk_id": "doc_fixed_001_chunk_15",
    "type": "inferential"
  },
  {
    "id": "q_079",
    "question": "What is the bridging relationship between the front door and the front door?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "ground_truth_context": "analysis, coreference ; see natural language understanding below ). semantic role labelling ( see also implicit semantic role labelling below ) given a single sentence, identify and disambiguate semantic predicates ( e. g., verbal frames ), then identify and classify the frame elements ( semantic roles ). discourse ( semantics beyond individual sentences ) coreference resolution given a sentence or larger chunk of text, determine which words ( \" mentions \" ) refer to the same objects ( \" entities \" ). anaphora resolution is a specific example of this task, and is specifically concerned with matching up pronouns with the nouns or names to which they refer. the more general task of coreference resolution also includes identifying so - called \" bridging relationships \" involving referring expressions. for example, in a sentence such as \" he entered john ' s house through the front door \", \" the front door \" is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of john ' s house ( rather than of some other structure that might also be referred to ). discourse analysis this rubric includes several related tasks. one task is discourse parsing, i. e., identifying the discourse structure of a connected text, i. e. the nature of the discourse relationships between sentences ( e. g. elaboration, explanation, contrast ). another possible task is recognizing and classifying the speech acts in a chunk of",
    "chunk_id": "doc_fixed_002_chunk_15",
    "type": "inferential"
  },
  {
    "id": "q_080",
    "question": "What is the difference between infinite impulse and finite impulse networks?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
    "ground_truth_context": "learning for the recognition of sequences can also be implemented by a more biological - based model which uses the silencing mechanism exhibited in neurons with a relatively high frequency spiking activity. additional stored states and the storage under direct control by the network can be added to both infinite - impulse and finite - impulse networks. another network or graph can also replace the storage if that incorporates time delays or has feedback loops. such controlled states are referred to as gated states or gated memory and are part of long short - term memory networks ( lstms ) and gated recurrent units. this is also called feedback neural network ( fnn ). libraries modern libraries provide runtime - optimized implementations of the above functionality or allow to speed up the slow loop by just - in - time compilation. apache singa caffe : created by the berkeley vision and learning center ( bvlc ). it supports both cpu and gpu. developed in c + +, and has python and matlab wrappers. chainer : fully in python, production support for cpu, gpu, distributed training. deeplearning4j : deep learning in java and scala on multi - gpu - enabled spark. flux : includes interfaces for rnns, including grus and lstms, written in julia. keras : high - level api, providing a wrapper to many other deep learning libraries. microsoft cognitive toolkit mxnet : an open - source deep learning framework used to train",
    "chunk_id": "doc_fixed_004_chunk_34",
    "type": "inferential"
  },
  {
    "id": "q_081",
    "question": "What is the heuristic of a llm?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Large_language_model",
    "ground_truth_context": "##out heuristic. when a programmatic world model is not available, an llm can also be prompted with a description of the environment to act as world model. for open - ended exploration, an llm can be used to score observations for their \" interestingness \", which can be used as a reward signal to guide a normal ( non - llm ) reinforcement learning agent. alternatively, it can propose increasingly difficult tasks for curriculum learning. instead of outputting individual actions, an llm planner can also construct \" skills \", or functions for complex action sequences. the skills can be stored and later invoked, allowing increasing levels of abstraction in planning. multiple agents with memory can interact socially. reasoning llms are conventionally trained to generate an output without generating intermediate steps. as a result, their performance tends to be subpar on complex questions requiring ( at least in humans ) intermediate steps of thought. early research demonstrated that inserting intermediate \" scratchpad \" computations could improve performance on such tasks. later methods overcame this deficiency more systematically by breaking tasks into smaller steps for the llm, either manually or automatically. chaining prompt chaining was introduced in 2022. in this method, a user manually breaks a complex problem down into several steps. in each step, the llm receives as input a prompt telling it what to do and some results from preceding steps. the result from one step is then reused in a next step, until a final answer is reached. the ability of an",
    "chunk_id": "doc_fixed_001_chunk_19",
    "type": "factual"
  },
  {
    "id": "q_082",
    "question": "What is the name of the author who wrote the book \" yang , yinfei \" ?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Transformer_(machine_learning)",
    "ground_truth_context": "2102. 12092 ^ yu, jiahui ; xu, yuanzhong ; koh, jing yu ; luong, thang ; baid, gunjan ; wang, zirui ; vasudevan, vijay ; ku, alexander ; yang, yinfei ( 2022 - 06 - 21 ), scaling autoregressive models for content - rich text - to - image generation, arxiv : 2206. 10789 ^ kariampuzha, william ; alyea, gioconda ; qu, sue ; sanjak, jaleal ; mathe, ewy ; sid, eric ; chatelaine, haley ; yadaw, arjun ; xu, yanji ; zhu, qian ( 2023 ). \" precision information extraction for rare disease epidemiology at scale \". journal of translational medicine. 21 ( 1 ) : 157. doi : 10. 1186 / s12967 - 023 - 04011 - y. pmc 9972634. pmid 36855134. further reading alexander rush, the annotated transformer archived 2021 - 09 - 22 at the wayback machine, harvard nlp group, 3 april 2018 phuong, mary ; hutter, marcus ( 2022 ). \" formal algorithms for transformers \". arxiv : 2207. 09238 [ cs. lg ]. ferrando, javier ; sarti,",
    "chunk_id": "doc_fixed_000_chunk_102",
    "type": "inferential"
  },
  {
    "id": "q_083",
    "question": "What is the physical symbol system hypothesis?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "ground_truth_context": "s. ). ^ nilsson ( 1983 ), p. 10. ^ haugeland ( 1985 ), pp. 112 \u2013 117. ^ physical symbol system hypothesis : newell & simon ( 1976, p. 116 ) historical significance : mccorduck ( 2004, p. 153 ), russell & norvig ( 2021, p. 19 ) ^ moravec ' s paradox : moravec ( 1988, pp. 15 \u2013 16 ), minsky ( 1986, p. 29 ), pinker ( 2007, pp. 190 \u2013 191 ) ^ dreyfus ' critique of ai : dreyfus ( 1972 ), dreyfus & dreyfus ( 1986 ) historical significance and philosophical implications : crevier ( 1993, pp. 120 \u2013 132 ), mccorduck ( 2004, pp. 211 \u2013 239 ), russell & norvig ( 2021, pp. 981 \u2013 982 ), fearn ( 2007, chpt. 3 ) ^ crevier ( 1993 ), p. 125. ^ langley ( 2011 ). ^ katz ( 2012 ). ^ neats vs. scruffies, the historic debate : mccorduck ( 2004, pp. 421 \u2013 424, 486 \u2013 489 ), crevier ( 1993, p. 168 ), nilsson ( 1983, pp. 10 \u2013 11 ), russell & norvig ( 2021, p. 24 ) a classic example of the \" scruffy \" approach to",
    "chunk_id": "doc_fixed_003_chunk_141",
    "type": "factual"
  },
  {
    "id": "q_084",
    "question": "Which of the following is true about the model?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Large_language_model",
    "ground_truth_context": "cost of training the model, in flops. n { \\ displaystyle n } is the number of parameters in the model. d { \\ displaystyle d } is the number of tokens in the training set. l { \\ displaystyle l } is the average negative log - likelihood loss per token ( nats / token ), achieved by the trained llm on the test dataset. and the statistical hyper - parameters are c 0 = 6 { \\ displaystyle c _ { 0 } = 6 }, meaning that it costs 6 flops per parameter to train on one token. note that training cost is much higher than inference cost, where it costs 1 to 2 flops per parameter to infer on one token. \u03b1 = 0. 34, \u03b2 = 0. 28, a = 406. 4, b = 410. 7, l 0 = 1. 69 { \\ displaystyle \\ alpha = 0. 34, \\ beta = 0. 28, a = 406. 4, b = 410. 7, l _ { 0 } = 1. 69 } emergent abilities at point ( s ) referred to as breaks, the lines change their slopes, appearing on a linear - log plot as a series of linear segments connected by arcs. performance of bigger models on various tasks, when plotted on a log - log scale, appears as a linear extrapolation of performance achieved by smaller models. however, this linearity may be punctuated by \" break ( s ) \"",
    "chunk_id": "doc_fixed_001_chunk_27",
    "type": "inferential"
  },
  {
    "id": "q_085",
    "question": "What is the earliest known work on the ising model?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
    "ground_truth_context": ". doi : 10. 1103 / revmodphys. 39. 883. ^ glauber, roy j. ( february 1963 ). \" roy j. glauber \" time - dependent statistics of the ising model \" \". journal of mathematical physics. 4 ( 2 ) : 294 \u2013 307. doi : 10. 1063 / 1. 1703954. retrieved 2021 - 03 - 21. ^ sherrington, david ; kirkpatrick, scott ( 1975 - 12 - 29 ). \" solvable model of a spin - glass \". physical review letters. 35 ( 26 ) : 1792 \u2013 1796. bibcode : 1975phrvl.. 35. 1792s. doi : 10. 1103 / physrevlett. 35. 1792. issn 0031 - 9007. ^ hopfield, j. j. ( 1982 ). \" neural networks and physical systems with emergent collective computational abilities \". proceedings of the national academy of sciences. 79 ( 8 ) : 2554 \u2013 2558. bibcode : 1982pnas... 79. 2554h. doi : 10. 1073 / pnas. 79. 8. 2554. pmc 346238. pmid 6953413. ^ hopfield, j. j. ( 1984 ). \" neurons with graded response have collective computational properties like those of two - state neurons \". proceedings of the national academy of sciences. 81",
    "chunk_id": "doc_fixed_004_chunk_43",
    "type": "inferential"
  },
  {
    "id": "q_086",
    "question": "What is the main idea of the text?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Transformer_(machine_learning)",
    "ground_truth_context": "of after ) multihead attention and feedforward layers stabilizes training, not requiring learning rate warmup. this is the \" pre - ln transformer \" and is more commonly used, compared to the original \" post - ln transformer \". pretrain - finetune transformers typically are first pretrained by self - supervised learning on a large generic dataset, followed by supervised fine - tuning on a small task - specific dataset. the pretrain dataset is typically an unlabeled large corpus, such as the pile. tasks for pretraining and fine - tuning commonly include : language modeling next - sentence prediction question answering reading comprehension sentiment analysis paraphrasing the t5 transformer report documents a large number of natural language pretraining tasks. some examples are : restoring or repairing incomplete or corrupted text. for example, the input, \" thank you ~ ~ me to your party ~ ~ week \", might generate the output, \" thank you for inviting me to your party last week \". translation between natural languages ( machine translation ) judging the pragmatic acceptability of natural language. for example, the following sentence might be judged \" not acceptable \", because even though it is syntactically well - formed, it is improbable in ordinary human usage : the course is jumping well. note that while each of these tasks is trivial or obvious for human native speakers of the language ( or languages ), they have typically proved challenging for",
    "chunk_id": "doc_fixed_000_chunk_08",
    "type": "inferential"
  },
  {
    "id": "q_087",
    "question": "What is the name of the book that dick wrote about neural networks?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "ground_truth_context": "280. doi : 10. 1016 / j. techfore. 2016. 08. 019. \" from not working to neural networking \". the economist. 2016. archived from the original on 31 december 2016. retrieved 26 april 2018. galvan, jill ( 1 january 1997 ). \" entering the posthuman collective in philip k. dick ' s \" do androids dream of electric sheep? \" \". science fiction studies. 24 ( 3 ) : 413 \u2013 429. doi : 10. 1525 / sfs. 24. 3. 0413. jstor 4240644. geist, edward moore ( 9 august 2015 ). \" is artificial intelligence really an existential threat to humanity? \". bulletin of the atomic scientists. archived from the original on 30 october 2015. retrieved 30 october 2015. gibbs, samuel ( 27 october 2014 ). \" elon musk : artificial intelligence is our biggest existential threat \". the guardian. archived from the original on 30 october 2015. retrieved 30 october 2015. goffrey, andrew ( 2008 ). \" algorithm \". in fuller, matthew ( ed. ). software studies : a lexicon. cambridge, mass. : mit press. pp. 15 \u2013 20. isbn 978 - 1 - 4356 - 4787 - 9. goldman, sharon ( 14 september 2022 ). \" 10 years later, deep learning ' revolution ' rages on, say ai pioneers hinton, lecu",
    "chunk_id": "doc_fixed_003_chunk_156",
    "type": "factual"
  },
  {
    "id": "q_088",
    "question": "What is the name of the book that focuses on the use of artificial intelligence in the prevention of online child sexual abuse?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "ground_truth_context": "##yesha ; nambiar, vaishnavi ( 2024 ). \" role of artificial intelligence in the prevention of online child sexual abuse : a systematic review of literature \". journal of applied security research. 19 ( 4 ) : 586 \u2013 627. doi : 10. 1080 / 19361610. 2024. 2331885. ^ razi, afsaneh ; kim, seunghyun ; alsoubai, ashwaq ; stringhini, gianluca ; solorio, thamar ; de choudhury, munmun ; wisniewski, pamela j. ( 13 october 2021 ). \" a human - centered systematic literature review of the computational approaches for online sexual risk detection \". proceedings of the acm on human - computer interaction. 5 ( cscw2 ) : 1 \u2013 38. doi : 10. 1145 / 3479609. ^ ransbotham, sam ; kiron, david ; gerbert, philipp ; reeves, martin ( 6 september 2017 ). \" reshaping business with artificial intelligence \". mit sloan management review. archived from the original on 13 february 2024. ^ sun, yuran ; zhao, xilei ; lovreglio, ruggiero ; kuligowski, erica ( 2024 ). \" ai for large - scale evacuation modeling : promises and challenges \". interpretable machine learning for the analysis, design, assessment, and informed decision making",
    "chunk_id": "doc_fixed_003_chunk_112",
    "type": "factual"
  },
  {
    "id": "q_089",
    "question": "What is the name of the first decoder only model?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Large_language_model",
    "ground_truth_context": "quickly became \" ubiquitous \". though the original transformer has both encoder and decoder blocks, bert is an encoder - only model. academic and research usage of bert began to decline in 2023, following rapid improvements in the abilities of decoder - only models ( such as gpt ) to solve tasks via prompting. although decoder - only gpt - 1 was introduced in 2018, it was gpt - 2 in 2019 that caught widespread attention because openai claimed to have initially deemed it too powerful to release publicly, out of fear of malicious use. gpt - 3 in 2020 went a step further and as of 2025 is available only via api with no offering of downloading the model to execute locally. but it was the 2022 consumer - facing chatbot chatgpt that received extensive media coverage and public attention. the 2023 gpt - 4 was praised for its increased accuracy and as a \" holy grail \" for its multimodal capabilities. openai did not reveal the high - level architecture and the number of parameters of gpt - 4. the release of chatgpt led to an uptick in llm usage across several research subfields of computer science, including robotics, software engineering, and societal impact work. in 2024 openai released the reasoning model openai o1, which generates long chains of thought before returning a final answer. many llms with parameter counts comparable to those of openai ' s gpt series have been",
    "chunk_id": "doc_fixed_001_chunk_04",
    "type": "factual"
  },
  {
    "id": "q_090",
    "question": "What is the name of the book that focuses on the neural networks?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
    "ground_truth_context": "; polosukhin, illia ( 2017 ). \" attention is all you need \". advances in neural information processing systems. 30. curran associates, inc. ^ oord, aaron van den ; kalchbrenner, nal ; kavukcuoglu, koray ( 2016 - 06 - 11 ). \" pixel recurrent neural networks \". proceedings of the 33rd international conference on machine learning. pmlr : 1747 \u2013 1756. ^ a b cruse, holk ; neural networks as cybernetic systems, 2nd and revised edition ^ elman, jeffrey l. ( 1990 ). \" finding structure in time \". cognitive science. 14 ( 2 ) : 179 \u2013 211. doi : 10. 1016 / 0364 - 0213 ( 90 ) 90002 - e. ^ jordan, michael i. ( 1997 - 01 - 01 ). \" serial order : a parallel distributed processing approach \". neural - network models of cognition \u2014 biobehavioral foundations. advances in psychology. vol. 121. pp. 471 \u2013 495. doi : 10. 1016 / s0166 - 4115 ( 97 ) 80111 - 2. isbn 978 - 0 - 444 - 81931 - 4. s2cid 15375627. ^ gers, felix a. ; schraudolph, nicol n. ; schmidhuber, jurgen ( 2002 ). \" learning precise timing with lstm rec",
    "chunk_id": "doc_fixed_004_chunk_50",
    "type": "factual"
  },
  {
    "id": "q_091",
    "question": "What is the most recent work by the authors of the book \" 137 emergent abilities of large language models \"?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Large_language_model",
    "ground_truth_context": "optimal large language models \". neurips : 30016 \u2013 30030. isbn 978 - 1 - 7138 - 7108 - 8. ^ a b caballero, ethan ; gupta, kshitij ; rish, irina ; krueger, david ( 2022 ). \" broken neural scaling laws \". arxiv : 2210. 14891 [ cs. lg ]. ^ a b wei, jason ; tay, yi ; bommasani, rishi ; raffel, colin ; zoph, barret ; borgeaud, sebastian ; yogatama, dani ; bosma, maarten ; zhou, denny ; metzler, donald ; chi, ed h. ; hashimoto, tatsunori ; vinyals, oriol ; liang, percy ; dean, jeff ; fedus, william ( 31 august 2022 ). \" emergent abilities of large language models \". transactions on machine learning research. issn 2835 - 8856. archived from the original on 22 march 2023. retrieved 19 march 2023. ^ \" 137 emergent abilities of large language models \". jason wei. retrieved 2023 - 06 - 24. ^ bowman, samuel r. ( 2024 ). \" eight things to know about large language models \". critical ai. 2 ( 2 ). doi : 10. 1215 / 2834703x - 11556011. ^ hahn, michael ; goyal",
    "chunk_id": "doc_fixed_001_chunk_87",
    "type": "inferential"
  },
  {
    "id": "q_092",
    "question": "What is the bibcode for the journal \" ecs. cl ''?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
    "ground_truth_context": "). \" sequence to sequence learning with neural networks \" ( pdf ). electronic proceedings of the neural information processing systems conference. 27 : 5346. arxiv : 1409. 3215. bibcode : 2014arxiv1409. 3215s. ^ jozefowicz, rafal ; vinyals, oriol ; schuster, mike ; shazeer, noam ; wu, yonghui ( 2016 - 02 - 07 ). \" exploring the limits of language modeling \". arxiv : 1602. 02410 [ cs. cl ]. ^ gillick, dan ; brunk, cliff ; vinyals, oriol ; subramanya, amarnag ( 2015 - 11 - 30 ). \" multilingual language processing from bytes \". arxiv : 1512. 00103 [ cs. cl ]. ^ vinyals, oriol ; toshev, alexander ; bengio, samy ; erhan, dumitru ( 2014 - 11 - 17 ). \" show and tell : a neural image caption generator \". arxiv : 1411. 4555 [ cs. cv ]. ^ cho, kyunghyun ; van merrienboer, bart ; gulcehre, caglar ; bahdanau, dzmitry ; bougares, fethi ; schwenk, holger ; bengio, yoshua ( 2014 - 06",
    "chunk_id": "doc_fixed_004_chunk_48",
    "type": "factual"
  },
  {
    "id": "q_093",
    "question": "What is the term for the cognitive process that is a part of the cognitive process?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "ground_truth_context": ": discourse parsing, 2019 : semantic parsing ). increasing interest in multilinguality, and, potentially, multimodality ( english since 1999 ; spanish, dutch since 2002 ; german since 2003 ; bulgarian, danish, japanese, portuguese, slovenian, swedish, turkish since 2006 ; basque, catalan, chinese, greek, hungarian, italian, turkish since 2007 ; czech since 2009 ; arabic since 2012 ; 2017 : 40 + languages ; 2018 : 60 + / 100 + languages ) elimination of symbolic representations ( rule - based over supervised towards weakly supervised methods, representation learning and end - to - end systems ) cognition most higher - level nlp applications involve aspects that emulate intelligent behavior and apparent comprehension of natural language. more broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behavior represents one of the developmental trajectories of nlp ( see trends among conll shared tasks above ). cognition refers to \" the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses. \" cognitive science is the interdisciplinary, scientific study of the mind and its processes. cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. especially during the age of symbolic nlp, the area of computational linguistics maintained strong ties with cognitive studies. as an example, george lakoff offers a methodology to build natural language processing ( nlp ) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, with two defining",
    "chunk_id": "doc_fixed_002_chunk_21",
    "type": "factual"
  },
  {
    "id": "q_094",
    "question": "What is the name of the journal that the guardian is a member of ?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Large_language_model",
    "ground_truth_context": "\". transactions of the association for computational linguistics. 8 : 842 \u2013 866. arxiv : 2002. 12327. doi : 10. 1162 / tacl _ a _ 00349. s2cid 211532403. archived from the original on 2022 - 04 - 03. retrieved 2024 - 01 - 21. ^ a b movva, rajiv ; balachandar, sidhika ; peng, kenny ; agostini, gabriel ; garg, nikhil ; pierson, emma ( 2024 ). \" topics, authors, and institutions in large language model research : trends from 17k arxiv papers \". proceedings of the 2024 conference of the north american chapter of the association for computational linguistics : human language technologies ( volume 1 : long papers ). pp. 1223 \u2013 1243. arxiv : 2307. 10700. doi : 10. 18653 / v1 / 2024. naacl - long. 67. retrieved 2024 - 12 - 08. ^ hern, alex ( 14 february 2019 ). \" new ai fake text generator may be too dangerous to release, say creators \". the guardian. archived from the original on 14 february 2019. retrieved 20 january 2024. ^ \" chatgpt a year on : 3 ways the ai chatbot has completely changed the world in 12 months \". euronews. november 30, 2023. archived from the original",
    "chunk_id": "doc_fixed_001_chunk_61",
    "type": "inferential"
  },
  {
    "id": "q_095",
    "question": "What is the decoder layer in a decoder - only transformer composed of?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Transformer_(machine_learning)",
    "ground_truth_context": "decoder transformer, then taking just the encoder. they are also referred to as \" all - to - all \" or \" bert - like \". a \" decoder - only \" transformer is not literally decoder - only, since without an encoder, the cross - attention mechanism has nothing to attend to. thus, the decoder layers in a decoder - only transformer is composed of just two sublayers : the causally masked self - attention, and the feedforward network. this is usually used for text generation and instruction following. the models in the gpt series and chinchilla series are decoder - only. they are also referred to as \" autoregressive \" or \" causal \". an \" encoder \u2013 decoder \" transformer is generally the same as the original transformer, with 2 sublayers per encoder layer and 3 sublayers per decoder layer, etc. they might have minor architectural improvements, such as alternative activation functions, changing the location of normalization, etc. this is also usually used for text generation and instruction following. the models in the t5 series are encoder \u2013 decoder. a \" prefixlm \" ( prefix language model ) is a decoder - only architecture, but with prefix masking, which is different from causal masking. specifically, it has mask of the form m prefixlm = [ 0 \u2212 \u221e 0 m causal ] { \\ displays",
    "chunk_id": "doc_fixed_000_chunk_46",
    "type": "factual"
  },
  {
    "id": "q_096",
    "question": "What is the difference between a bidirectional rnn and a birnn?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
    "ground_truth_context": "##ctional recurrent neural networks bidirectional rnn a bidirectional rnn ( birnn ) is composed of two rnns, one processing the input sequence in one direction, and another in the opposite direction. abstractly, it is structured as follows : the forward rnn processes in one direction : f \u03b8 ( x 0, h 0 ) = ( y 0, h 1 ), f \u03b8 ( x 1, h 1 ) = ( y 1, h 2 ), \u2026 { \\ displaystyle f _ { \\ theta } ( x _ { 0 }, h _ { 0 } ) = ( y _ { 0 }, h _ { 1 } ), f _ { \\ theta } ( x _ { 1 }, h _ { 1 } ) = ( y _ { 1 }, h _ { 2 } ), \\ dots } the backward rnn processes in the opposite direction : f \u03b8 \u2032 \u2032 ( x n, h n \u2032 ) = ( y n \u2032, h n \u2212 1 \u2032 ), f \u03b8 \u2032 \u2032 ( x n \u2212 1, h n \u2212 1 \u2032 ) = ( y n \u2212 1 \u2032, h n \u2212 2 \u2032 ), \u2026 { \\ displaystyle f ' _ { \\ theta ' } ( x _ { n }, h _ { n } ' ) = ( y ' _ { n }, h _ { n - 1 } ' ), f ' _ { \\ theta ' } ( x _",
    "chunk_id": "doc_fixed_004_chunk_09",
    "type": "factual"
  },
  {
    "id": "q_097",
    "question": "What is the name of the book that edwards wrote about ai poisoning?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Large_language_model",
    "ground_truth_context": "##har ; mishra, shailesh ; endres, christoph ; holz, thorsten ; fritz, mario ( 2023 - 02 - 01 ). \" not what you ' ve signed up for : compromising real - world llm - integrated applications with indirect prompt injection \". proceedings of the 16th acm workshop on artificial intelligence and security. pp. 79 \u2013 90. doi : 10. 1145 / 3605764. 3623985. isbn 979 - 8 - 4007 - 0260 - 0. ^ edwards, benj ( 2024 - 01 - 15 ). \" ai poisoning could turn models into destructive \" sleeper agents, \" says anthropic \". ars technica. retrieved 2025 - 07 - 19. ^ \" u. s. judge approves $ 1. 5 billion anthropic copyright settlement with authors \". reuters. 2025 - 09 - 25. retrieved 2025 - 09 - 26. ^ \" anthropic reaches $ 1. 5b settlement with authors over ai copyright claims \". associated press. 2025 - 09 - 25. retrieved 2025 - 09 - 26. ^ \" meta fends off authors ' u. s. copyright lawsuit over ai \". reuters. 2025 - 06 - 25. retrieved 2025 - 06 - 26. ^ \" meta scores victory in ai copyright case \". wired. 2025 - 06 - 25. retrieved 2025 - 06 - 26. ^ \" openai",
    "chunk_id": "doc_fixed_001_chunk_110",
    "type": "factual"
  },
  {
    "id": "q_098",
    "question": "What is the main idea of the passage?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Large_language_model",
    "ground_truth_context": "##er architecture, introduced in 2017, replaced recurrence with self - attention, allowing efficient parallelization, longer context handling, and scalable training on unprecedented data volumes. this innovation enabled models like gpt, bert, and their successors, which demonstrated emergent behaviors at scale, such as few - shot learning and compositional reasoning. reinforcement learning, particularly policy gradient algorithms, has been adapted to fine - tune llms for desired behaviors beyond raw next - token prediction. reinforcement learning from human feedback ( rlhf ) applies these methods to optimize a policy, the llm ' s output distribution, against reward signals derived from human or automated preference judgments. this has been critical for aligning model outputs with user expectations, improving factuality, reducing harmful responses, and enhancing task performance. benchmark evaluations for llms have evolved from narrow linguistic assessments toward comprehensive, multi - task evaluations measuring reasoning, factual accuracy, alignment, and safety. hill climbing, iteratively optimizing models against benchmarks, has emerged as a dominant strategy, producing rapid incremental performance gains but raising concerns of overfitting to benchmarks rather than achieving genuine generalization or robust capability improvements. history the number of publications about large language models by year grouped by publication types. the training compute of notable large models in flops vs publication date over the period 2010 \u2013 2024. for overall notable models ( top left ), frontier models ( top right ), top language models ( bottom left",
    "chunk_id": "doc_fixed_001_chunk_01",
    "type": "inferential"
  },
  {
    "id": "q_099",
    "question": "What is the process of proving a new statement from other statements that are given and assumed to be true?",
    "ground_truth_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "ground_truth_context": "s that are y s \" ). deductive reasoning in logic is the process of proving a new statement ( conclusion ) from other statements that are given and assumed to be true ( the premises ). proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules. given a problem and a set of premises, problem - solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. in the case of horn clauses, problem - solving search can be performed by reasoning forwards from the premises or backwards from the problem. in the more general case of the clausal form of first - order logic, resolution is a single, axiom - free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved. inference in both horn clause logic and first - order logic is undecidable, and therefore intractable. however, backward reasoning with horn clauses, which underpins computation in the logic programming language prolog, is turing complete. moreover, its efficiency is competitive with computation in other symbolic programming languages. fuzzy logic assigns a \" degree of truth \" between 0 and 1. it can therefore handle propositions that are vague and partially true. non - monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning.",
    "chunk_id": "doc_fixed_003_chunk_11",
    "type": "factual"
  }
]